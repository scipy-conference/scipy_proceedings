{
  "toc": [
    {
      "author_institution_map": {
        "Minesh B. Amin": [
          "MBA Sciences, Inc"
        ]
      },
      "author": [
        "Minesh B. Amin"
      ],
      "author_email": [
        "mamin@mbasciences.com"
      ],
      "abstract": [
        "SPM.Python is a scalable, parallel fault-tolerant version of the\nserial Python language, and can be deployed to create parallel\ncapabilities to solve problems in domains spanning finance, life\nsciences, electronic design, IT, visualization, and\nresearch. Software developers may use SPM.Python to augment new or\nexisting (Python) serial scripts for scalability across parallel\nhardware. Alternatively, SPM.Python may be used to better manage\nthe execution of stand-alone (non-Python x86 and GPU) applications\nacross compute resources in a fault-tolerant manner taking into\naccount hard deadlines."
      ],
      "title": "A Technical Anatomy of SPM.Python, a Scalable, Parallel Version of Python",
      "bibliography": "",
      "authors": "Minesh B. Amin",
      "keywords": "fault tolerance, parallel closures, parallel exceptions, parallel invariants,\nparallel programming, parallel sequence points, scalable vocabulary,\nparallel management patterns",
      "paper_id": "amin",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-000",
      "pages": 9,
      "author_institution": [
        "MBA Sciences, Inc"
      ],
      "copyright_holder": "Minesh B. Amin.",
      "page": {
        "start": 1,
        "stop": 9
      }
    },
    {
      "author_institution_map": {
        "Dan Nguyen": [
          "Smithsonian Astrophysical Observatory"
        ],
        "Brian Refsdal": [
          "Smithsonian Astrophysical Observatory"
        ],
        "Aneta Siemiginowska": [
          "Smithsonian Astrophysical Observatory"
        ],
        "Stephen Doe": [
          "Smithsonian Astrophysical Observatory"
        ]
      },
      "author": [
        "Brian Refsdal",
        "Stephen Doe",
        "Dan Nguyen",
        "Aneta Siemiginowska"
      ],
      "author_email": [
        "brefsdal@cfa.harvard.edu",
        "sdoe@cfa.harvard.edu",
        "dnguyen@cfa.harvard.edu",
        "asiemiginowska@cfa.harvard.edu"
      ],
      "abstract": [
        "Sherpa is a generalized modeling and fitting package.  Primarily developed\nfor the Chandra Interactive Analysis of Observations (CIAO) package by the\nChandra X-ray Center, Sherpa provides an Object-Oriented Programming (OOP)\nAPI for parametric data modeling.  It is designed to use the forward fitting\ntechnique to search for the set of best-fit parameter values in parametrized\nmodel functions.  Sherpa can also estimate the confidence limits on best-fit\nparameters using a new confidence method or using an algorithm based on\nMarkov chain Monte Carlo (MCMC).  Confidence limits on parameter values are\nnecessary for any data analysis result, but can be non-trivial to compute in\na non-linear and multi-parameter space.  This new, robust confidence method\ncan estimate confidence limits of Sherpa parameters using a finite\nconvergence rate.  The Sherpa extension module, pyBLoCXS, implements a\nsophisticated Bayesian MCMC-based algorithm for simple single-component\nspectral models defined in Sherpa.  pyBLoCXS has primarily been developed in\nPython using high-energy X-ray spectral data.  We describe the algorithm\nincluding the features for defining priors and incorporating deviations in\nthe calibration information.  We will demonstrate examples of estimating\nconfidence limits using the confidence method and processing simulations\nusing pyBLoCXS."
      ],
      "title": "Fitting and Estimating Parameter Confidence Limits with Sherpa",
      "bibliography": "",
      "authors": "Brian Refsdal, Stephen Doe, Dan Nguyen, Aneta Siemiginowska",
      "keywords": "modeling, fitting, parameter, confidence, mcmc, bayesian",
      "paper_id": "brefsdal",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-001",
      "pages": 7,
      "author_institution": [
        "Smithsonian Astrophysical Observatory",
        "Smithsonian Astrophysical Observatory",
        "Smithsonian Astrophysical Observatory",
        "Smithsonian Astrophysical Observatory"
      ],
      "copyright_holder": "Brian Refsdal et al.",
      "page": {
        "start": 10,
        "stop": 16
      }
    },
    {
      "author_institution_map": {
        "Ricardo Caspirro": [
          "Muricoca Labs"
        ],
        "Bruno Melo": [
          "Muricoca Labs"
        ],
        "Marcel Caraciolo": [
          "Muricoca Labs"
        ]
      },
      "author": [
        "Marcel Caraciolo",
        "Bruno Melo",
        "Ricardo Caspirro"
      ],
      "author_email": [
        "marcel@muricoca.com",
        "bruno@muricoca.com",
        "ricardo@muricoca.com"
      ],
      "abstract": [
        "Crab is a flexible, fast recommender engine for Python that integrates classic information filtering\nrecommendation algorithms in the world of scientific Python packages (NumPy,SciPy, Matplotlib). The engine\naims to provide a rich set of components from which you can construct a customized recommender system from\na set of algorithms. It is designed for scability, flexibility and performance making use of scientific\noptimized python packages in order to provide simple and efficient solutions that are acessible to everybody\nand reusable in various contexts: science and engineering.\nThe engine takes users' preferences for items and returns estimated preferences for other items. For instance,\na web site that sells movies could easily use Crab to figure out, from past purchase data, which movies a\ncustomer might be interested in watching to. This work presents our inniative in developing this framework\nin Python following the standards of the well-known machine learning toolkit Scikit-Learn to be an alternative\nsolution for Mahout Taste collaborative framework for Java. Finally, we discuss its main features,\nreal scenarios where this framework is already applied and future extensions."
      ],
      "title": "Crab: A Recommendation Engine Framework for Python",
      "bibliography": "",
      "authors": "Marcel Caraciolo, Bruno Melo, Ricardo Caspirro",
      "keywords": "data mining, machine learning, recommendation systems, information filtering, framework, web",
      "paper_id": "caraciolo",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-002",
      "pages": 7,
      "author_institution": [
        "Muricoca Labs",
        "Muricoca Labs",
        "Muricoca Labs"
      ],
      "copyright_holder": "Marcel Caraciolo et al.",
      "page": {
        "start": 17,
        "stop": 23
      }
    },
    {
      "author_institution_map": {
        "Andrew Cron": [
          "Duke University"
        ],
        "Wes McKinney": [
          "Duke University"
        ]
      },
      "author": [
        "Andrew Cron",
        "Wes McKinney"
      ],
      "author_email": [
        "ajc40@stat.duke.edu",
        "wesmckinn@gmail.com"
      ],
      "abstract": [
        "In this work we discuss gpustats, a new Python library for\nassisting in \\textquotedbl{}big data\\textquotedbl{} statistical computing applications,\nparticularly Monte Carlo-based inference algorithms. The library\nprovides a general code generation / metaprogramming framework for\neasily implementing discrete and continuous probability density\nfunctions and random variable samplers. These functions can be\nutilized to achieve more than 100x speedup over their CPU\nequivalents. We demonstrate their use in an Bayesian MCMC\napplication and discuss avenues for future work."
      ],
      "title": "gpustats: GPU Library for Statistical Computing in Python",
      "bibliography": "",
      "authors": "Andrew Cron, Wes McKinney",
      "keywords": "GPU, CUDA, OpenCL, Python, statistical inference, statistics,\nmetaprogramming, sampling, Markov Chain Monte Carlo (MCMC), PyMC,\nbig data",
      "paper_id": "cron_mckinney",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-003",
      "pages": 5,
      "author_institution": [
        "Duke University",
        "Duke University"
      ],
      "copyright_holder": "Andrew Cron et al.",
      "page": {
        "start": 24,
        "stop": 28
      }
    },
    {
      "author_institution_map": {
        "Jeff Daily": [
          "Pacific Northwest National Laboratory"
        ],
        "Robert R. Lewis": [
          "Washington State University"
        ]
      },
      "author": [
        "Jeff Daily",
        "Robert R. Lewis"
      ],
      "author_email": [
        "jeff.daily@pnnl.gov",
        "bobl@tricity.wsu.edu"
      ],
      "abstract": [
        "Global Arrays (GA) is a software system from Pacific Northwest National\nLaboratory that enables an efficient, portable, and parallel shared-memory\nprogramming interface to manipulate distributed dense arrays. Using a\ncombination of GA and NumPy, we have reimplemented NumPy as a\ndistributed drop-in replacement called Global Arrays in NumPy (GAiN).\nScalability studies will be presented showing the utility of developing\nserial NumPy codes which can later run on more capable clusters or\nsupercomputers."
      ],
      "title": "Using the Global Arrays Toolkit to Reimplement NumPy for Distributed Computation",
      "bibliography": "",
      "authors": "Jeff Daily, Robert R. Lewis",
      "keywords": "Global Arrays, Python, NumPy, MPI",
      "paper_id": "daily",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-004",
      "pages": 7,
      "author_institution": [
        "Pacific Northwest National Laboratory",
        "Washington State University"
      ],
      "copyright_holder": "Jeff Daily et al.",
      "page": {
        "start": 29,
        "stop": 35
      }
    },
    {
      "author_institution_map": {
        "Scott Determan": [
          "Vision Spreadsheet"
        ]
      },
      "author": [
        "Scott Determan"
      ],
      "author_email": [
        "scott.determan@gmail.com"
      ],
      "abstract": [
        "Vision Spreadsheet is an environment for computer vision. It combines a\nspreadsheet with computer vision and scientific python. The cells in the\nspreadsheet are images, computations on images, measurements, and plots. There\nare many built in image processing and machine learning algorithms and it\nextensible by writing python functions and importing them into the\nspreadsheet."
      ],
      "title": "Vision Spreadsheet: An Environment for Computer Vision",
      "bibliography": "",
      "authors": "Scott Determan",
      "keywords": "computer vision, spreadsheet, OpenCV",
      "paper_id": "determan",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-005",
      "pages": 4,
      "author_institution": [
        "Vision Spreadsheet"
      ],
      "copyright_holder": "Scott Determan.",
      "page": {
        "start": 36,
        "stop": 39
      }
    },
    {
      "author_institution_map": {
        "Mark Dewing": [
          "Intel"
        ]
      },
      "author": [
        "Mark Dewing"
      ],
      "author_email": [
        "markdewing@gmail.com"
      ],
      "abstract": [
        "We describe a method for constructing scientific programs where SymPy is\nused to model the mathematical steps in the derivation.  With this workflow,\neach step in the process can be checked by machine, from the derivation of\nthe equations to the generation of the source code.  We present an example\nbased on computing the partition function integrals in statistical mechanics."
      ],
      "title": "Constructing scientific programs using SymPy",
      "bibliography": "",
      "authors": "Mark Dewing",
      "keywords": "SymPy, code generation, metaprogramming",
      "paper_id": "dewing",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-006",
      "pages": 4,
      "author_institution": [
        "Intel"
      ],
      "copyright_holder": "Mark Dewing.",
      "page": {
        "start": 40,
        "stop": 43
      }
    },
    {
      "author_institution_map": {
        "Andrew Wilson": [
          "Texas Water Development Board"
        ],
        "Dharhas Pothina": [
          "Texas Water Development Board"
        ]
      },
      "author": [
        "Dharhas Pothina",
        "Andrew Wilson"
      ],
      "author_email": [
        "dharhas.pothina@twdb.state.tx.us",
        "andrew.wilson@twdb.state.tx.us"
      ],
      "abstract": [
        "Obtaining time-series monitoring data in a particular region often requires a significant effort involving visiting multiple websites, contacting multiple organizations and dealing with a variety of data formats. Although there has been a large research effort nationally in techniques to share and disseminate water related time-series monitoring data, development of a usable system has lagged. The pieces have been available for some time now, but a lack of vision,expertise, resources and software licensing requirements have hindered uptake outside of the academic research groups. The Texas Water Development Board is both a data provider and large user of data collected by other entities. As such, using the lessons learned from the last several years of research, we have implemented an expandable infrastructure for sharing water data in Texas. In this paper, we discuss the social, institutional and technological challenges in creating a system that allows discovery, access, and publication of water data from multiple federal, state, local and university sources and how we have used Python to create this system in a resource limited environment."
      ],
      "title": "Using Python, Partnerships, Standards and Web Services to provide Water Data for Texans",
      "bibliography": "",
      "authors": "Dharhas Pothina, Andrew Wilson",
      "keywords": "time-series, web services, waterml, data, wofpy, pyhis, HIS,\nhydrologic information system, cyberinfrastructure",
      "paper_id": "dharhas_pothina",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-007",
      "pages": 4,
      "author_institution": [
        "Texas Water Development Board",
        "Texas Water Development Board"
      ],
      "copyright_holder": "Dharhas Pothina et al.",
      "page": {
        "start": 44,
        "stop": 47
      }
    },
    {
      "author_institution_map": {
        "Jonathan Jacky": [
          "University of Washington"
        ]
      },
      "author": [
        "Jonathan Jacky"
      ],
      "author_email": [
        "jon@uw.edu"
      ],
      "abstract": [
        "In unit testing, the programmer codes the test cases, and also codes\nassertions that check whether each test case passed.  In model-based\ntesting, the programmer codes a \\textquotedbl{}model\\textquotedbl{} that generates as many test\ncases as desired and also acts as the oracle that checks the cases.\nModel-based testing is recommended where so many test cases are needed\nthat it is not feasible to code them all by hand.  This need arises\nwhen testing behaviors that exhibit history-dependence and\nnondeterminism, so that many variations (data values, interleavings,\netc.) should be tested for each scenario (or use case).  Examples\ninclude communication protocols, web applications, control systems,\nand user interfaces.  PyModel is a model-based testing framework in\nPython.  PyModel supports on-the-fly testing, which can generate\nindefinitely long nonrepeating tests as the test run executes.\nPyModel can focus test cases on scenarios of interest by composition,\na versatile technique that combines models by synchronizing shared\nactions and interleaving unshared actions.  PyModel can guide test\ncoverage according to programmable strategies coded by the programmer."
      ],
      "title": "PyModel: Model-based testing in Python",
      "bibliography": "",
      "authors": "Jonathan Jacky",
      "keywords": "testing, model-based testing, automated testing, executable\nspecification, finite state machine, nondeterminism, exploration,\noffline testing, on-the-fly testing, scenario, composition",
      "paper_id": "jacky",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-008",
      "pages": 5,
      "author_institution": [
        "University of Washington"
      ],
      "copyright_holder": "Jonathan Jacky.",
      "page": {
        "start": 48,
        "stop": 52
      }
    },
    {
      "author_institution_map": {
        "Joseph Koning": [
          "Lawrence Livermore National Laboratory"
        ],
        "Matthew Terry": [
          "Lawrence Livermore National Laboratory"
        ]
      },
      "author": [
        "Matthew Terry",
        "Joseph Koning"
      ],
      "author_email": [
        "terry10@llnl.gov",
        "koning1@llnl.gov"
      ],
      "abstract": [
        "The process of tuning an inertial confinement fusion pulse shape to a\nspecific target design is highly iterative process.  When done manually,\neach iteration has large latency and is consequently time consuming.  We\nhave developed several techniques that can be used to automate much of the\npulse tuning process and significantly accelerate the tuning process by\nremoving the human induced latency.  The automated data analysis techniques\nrequire specialized diagnostics to run within the simulation.  To\nfacilitate these techniques, we have embedded a loosely coupled Python\ninterpreter within a pre-existing radiation-hydrodynamics code, Hydra.  To\nautomate the tuning process we use numerical optimization techniques and\nconstruct objective functions to identify tuned parameters."
      ],
      "title": "Automation of Inertial Fusion Target Design with Python",
      "bibliography": "",
      "authors": "Matthew Terry, Joseph Koning",
      "keywords": "inertial confinement fusion, python, automation",
      "paper_id": "koningterry",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-009",
      "pages": 5,
      "author_institution": [
        "Lawrence Livermore National Laboratory",
        "Lawrence Livermore National Laboratory"
      ],
      "copyright_holder": "Matthew Terry et al.",
      "page": {
        "start": 53,
        "stop": 57
      }
    },
    {
      "author_institution_map": {
        "Mark DeMaria": [
          "NOAA/NESDIS/STAR"
        ],
        "Minwoo Lee": [
          "Colorado State University"
        ],
        "Charles W. Anderson": [
          "Colorado State University"
        ]
      },
      "author": [
        "Minwoo Lee",
        "Charles W. Anderson",
        "Mark DeMaria"
      ],
      "author_email": [
        "lemin@cs.colostate.edu",
        "anderson@cs.colostate.edu",
        "Mark.DeMaria@noaa.gov"
      ],
      "abstract": [
        "The National Centers for Environmental Prediction (NCEP) Global Forecast System (GFS) is a global spectral model used for aviation weather forecast. It produces forecasts of wind speed and direction, temperature, humidity and precipitation out to 192 hr every 6 hours over the entire globe. The horizontal resolution in operational version of the GFS is about 25 km. Much longer integration of similar global models are run for climate applications but with much lower horizontal resolution. Although not specifically designed for tropical cyclones, the model solutions contain smoothed representations of these storms. One of the challenges in using global atmospheric model for hurricane applications is objectively determining what is a tropical cyclone, given the three dimensional solutions of atmospheric variables. This is especially difficult in the lower resolution climate models.\nTo address this issue, without manually selecting features of interests, the initial conditions from a low resolution version of the GFS (2 degree latitude-longitude grid) are examined at 6 hour periods and compared with the known positions of tropical cyclones.\nSeveral Python modules are used to build a prototype model quickly, and the prototype model shows fast and accurate prediction with the low resolution GFS data."
      ],
      "title": "Hurricane Prediction with Python",
      "bibliography": "",
      "authors": "Minwoo Lee, Charles W. Anderson, Mark DeMaria",
      "keywords": "hurricane, prediction, GFS, SVM",
      "paper_id": "lee",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-00a",
      "pages": 5,
      "author_institution": [
        "Colorado State University",
        "Colorado State University",
        "NOAA/NESDIS/STAR"
      ],
      "copyright_holder": "Minwoo Lee et al.",
      "page": {
        "start": 58,
        "stop": 62
      }
    },
    {
      "author_institution_map": {
        "Alex D. Young": [
          "University of Edinburgh"
        ],
        "Martin J. Ling": [
          "University of Edinburgh"
        ]
      },
      "author": [
        "Martin J. Ling",
        "Alex D. Young"
      ],
      "author_email": [
        "m.j.ling@ed.ac.uk",
        "ayoung9@inf.ed.ac.uk"
      ],
      "abstract": [
        "IMUSim is a new simulation package developed in Python to model Inertial\nMeasurement Units, i.e. devices which include accelerometers, gyroscopes\nand magnetometers. It was developed in the course of our research into\nalgorithms for IMU-based motion capture, and has now been released under\nthe GPL for the benefit of other researchers and users. The software\ngenerates realistic sensor readings based on trajectory, environment,\nsensor and system models. It includes implementaions of various relevant\nprocessing algorithms and mathematical utilities, some of which may be\nuseful elsewhere. The simulator makes extensive use of NumPy, SciPy, SimPy,\nCython, Matplotlib and Mayavi. The rapid development enabled by these tools\nallowed the project to be completed as a side project by two researchers.\nCareful design of an object-oriented API for the various models involved\nin the simulation allows the software to remain flexible and extensible\nwhile requiring users to write a minimum amount of code to use it."
      ],
      "title": "IMUSim - Simulating inertial and magnetic sensor systems in Python",
      "bibliography": "",
      "authors": "Martin J. Ling, Alex D. Young",
      "keywords": "simulation, IMU, accelerometer, gyroscope, magnetometer",
      "paper_id": "ling",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-00b",
      "pages": 7,
      "author_institution": [
        "University of Edinburgh",
        "University of Edinburgh"
      ],
      "copyright_holder": "Martin J. Ling et al.",
      "page": {
        "start": 63,
        "stop": 69
      }
    },
    {
      "author_institution_map": {
        "Amal Alghamdi": [
          "King Abdullah University of Science and Technology"
        ],
        "William Scullin": [
          "Argonne National Labs"
        ],
        "Aron Ahmadia": [
          "King Abdullah University of Science and Technology"
        ],
        "Kyle T. Mandli": [
          "University of Washington"
        ],
        "David I. Ketcheson": [
          "King Abdullah University of Science and Technology"
        ]
      },
      "author": [
        "Kyle T. Mandli",
        "Amal Alghamdi",
        "Aron Ahmadia",
        "David I. Ketcheson",
        "William Scullin"
      ],
      "author_email": [
        "mandli@amath.washington.edu",
        "amal.ghamdi@kaust.edu.sa",
        "aron.ahmadia@kaust.edu.sa",
        "david.ketcheson@kaust.edu.sa",
        "wscullin@alcf.anl.gov"
      ],
      "abstract": [
        "Computational scientists seek to provide efficient, easy-to-use tools and\nframeworks that enable application scientists within a specific discipline to\nbuild and/or apply numerical models with up-to-date computing technologies\nthat can be executed on all available computing systems. Although many tools\ncould be useful for groups beyond a specific application, it is often\ndifficult and time consuming to combine existing software, or to adapt it for\na more general purpose. Python enables a high-level approach where a general\nframework can be supplemented with tools written for different fields and in\ndifferent languages. This is particularly important when a large number of\ntools are necessary, as is the case for high performance scientific codes.\nThis motivated our development of PetClaw, a scalable distributed-memory\nsolver for time-dependent nonlinear wave propagation, as a case-study for how\nPython can be used as a high-level framework leveraging a multitude of codes,\nefficient both in the reuse of code and programmer productivity. We present\nscaling results for computations on up to four racks of Shaheen, an IBM\nBlueGene/P supercomputer at King Abdullah University of Science and\nTechnology. One particularly important issue that PetClaw has faced is the\noverhead associated with dynamic loading leading to catastrophic scaling. We\nuse the walla library to solve the issue which does so by supplanting\nhigh-cost filesystem calls with MPI operations at a low enough level that\ndevelopers may avoid any changes to their codes."
      ],
      "title": "Using Python to Construct a Scalable Parallel Nonlinear Wave Solver",
      "bibliography": "",
      "authors": "Kyle T. Mandli, Amal Alghamdi, Aron Ahmadia, David I. Ketcheson, William Scullin",
      "keywords": "parallel, scaling, finite volume, nonlinear waves, PyClaw, PetClaw, Walla",
      "paper_id": "mandli_etal",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-00c",
      "pages": 6,
      "author_institution": [
        "University of Washington",
        "King Abdullah University of Science and Technology",
        "King Abdullah University of Science and Technology",
        "King Abdullah University of Science and Technology",
        "Argonne National Labs"
      ],
      "copyright_holder": "Kyle T. Mandli et al.",
      "page": {
        "start": 70,
        "stop": 75
      }
    },
    {
      "author_institution_map": {
        "Leif Strand": [
          "California Institute of Technology"
        ],
        "Tim Sullivan": [
          "California Institute of Technology"
        ],
        "Michael A.G. Aivazis": [
          "California Institute of Technology"
        ],
        "Alta Fang": [
          "California Institute of Technology"
        ],
        "Michael M. McKerns": [
          "California Institute of Technology"
        ]
      },
      "author": [
        "Michael M. McKerns",
        "Leif Strand",
        "Tim Sullivan",
        "Alta Fang",
        "Michael A.G. Aivazis"
      ],
      "author_email": [
        "mmckerns@caltech.edu",
        "leif@cacr.caltech.edu",
        "tjs@caltech.edu",
        "altafang@caltech.edu",
        "aivazis@caltech.edu"
      ],
      "abstract": [
        "Key questions that scientists and engineers typically want to address can be\nformulated in terms of predictive science. Questions such as: \\textquotedbl{}How well does my\ncomputational model represent reality?\\textquotedbl{}, \\textquotedbl{}What are the most important\nparameters in the problem?\\textquotedbl{}, and \\textquotedbl{}What is the best next experiment to perform?\\textquotedbl{}\nare fundamental in solving scientific problems. mystic is a framework for\nmassively-parallel optimization and rigorous sensitivity analysis that enables\nthese motivating questions to be addressed quantitatively as global\noptimization problems. Often realistic physics, engineering, and materials\nmodels may have hundreds of input parameters, hundreds of constraints, and may\nrequire execution times of seconds or longer. In more extreme cases, realistic\nmodels may be multi-scale, and require the use of high-performance computing\nclusters for their evaluation. Predictive calculations, formulated as a global\noptimization over a potential surface in design parameter space, may require an\nalready prohibitively large simulation to be performed hundreds, if not\nthousands, of times. The need to prepare, schedule, and monitor thousands of\nmodel evaluations, and dynamically explore and analyze results, is a\nchallenging problem that requires a software infrastructure capable of\ndistributing and managing computations on large-scale heterogeneous resources.\nIn this paper, we present the design behind an optimization framework, and also\na framework for heterogeneous computing, that when utilized together, can make\ncomputationally intractable sensitivity and optimization problems much more\ntractable. The optimization framework provides global search algorithms that\nhave been extended to parallel, where evaluations of the model can be\ndistributed to appropriate large-scale resources, while the optimizer centrally\nmanages their interactions and navigates the objective function.  New methods\nhave been developed for imposing and solving constraints that aid in reducing\nthe size and complexity of the optimization problem. Additionally, new\nalgorithms have been developed that launch multiple optimizers in parallel,\nthus allowing highly efficient local search algorithms to provide fast global\noptimization. In this way, parallelism in optimization also can allow us to not\nonly find global minima, but to simultaneously find all local minima and\ntransition points -{}- thus providing a much more efficient means of mapping out\na potential energy surface."
      ],
      "title": "Building a Framework for Predictive Science",
      "bibliography": "",
      "authors": "Michael M. McKerns, Leif Strand, Tim Sullivan, Alta Fang, Michael A.G. Aivazis",
      "keywords": "predictive science, optimization, uncertainty quantification,\nverification, validation, sensitivity analysis,\nparallel computing, distributed computing, heterogeneous computing",
      "paper_id": "mckerns",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-00d",
      "pages": 11,
      "author_institution": [
        "California Institute of Technology",
        "California Institute of Technology",
        "California Institute of Technology",
        "California Institute of Technology",
        "California Institute of Technology"
      ],
      "copyright_holder": "Michael M. McKerns et al.",
      "page": {
        "start": 76,
        "stop": 86
      }
    },
    {
      "author_institution_map": {
        "Nick Bray": [
          "Google"
        ]
      },
      "author": [
        "Nick Bray"
      ],
      "author_email": [
        "ncbray@google.com"
      ],
      "abstract": [
        "PyStream is a static compiler that can radically transform Python code and run it on a Graphics Processing Unit (GPU).  Python compiled to run on the GPU is \\textasciitilde{}100,000x faster than when interpreted on the CPU.  The PyStream compiler is specially designed to simplify the development of real-time rendering systems by allowing the entire rendering system to be written in a single, highly productive language.  Without PyStream, GPU-accelerated real-time rendering systems must contain two separate code bases written in two separate languages: one for the CPU and one for the GPU.  Functions and data structures are not shared between the code bases, and any common functionality must be redundantly written in both languages.  PyStream unifies a rendering system into a single, Python code base, allowing functions and data structures to be transparently shared between the CPU and the GPU.  A single, unified code base makes it easy to create, maintain, and evolve a high-performance GPU-accelerated application."
      ],
      "title": "PyStream: Compiling Python onto the GPU",
      "bibliography": "",
      "authors": "Nick Bray",
      "keywords": "pystream, compiling python, gpu",
      "paper_id": "nick_bray",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-00e",
      "pages": 4,
      "author_institution": [
        "Google"
      ],
      "copyright_holder": "Nick Bray.",
      "page": {
        "start": 87,
        "stop": 90
      }
    },
    {
      "author_institution_map": {
        "Derrick Coetzee": [
          "University of California, Berkeley"
        ],
        "Armando Fox": [
          "University of California, Berkeley"
        ],
        "Shoaib Kamil": [
          "University of California, Berkeley"
        ]
      },
      "author": [
        "Shoaib Kamil",
        "Derrick Coetzee",
        "Armando Fox"
      ],
      "author_email": [
        "skamil@cs.berkeley.edu",
        "dcoetzee@cs.berkeley.edu",
        "fox@cs.berkeley.edu"
      ],
      "abstract": [
        "Today's productivity programmers, such as scientists who need to\nwrite code to do science, are typically forced to\nchoose between productive and maintainable code with modest\nperformance (e.g. Python plus native libraries such as SciPy\nSciPy) or complex, brittle, hardware-specific code that\nentangles application logic with performance concerns but runs two\nto three orders of magnitude faster (e.g. C++ with OpenMP, CUDA,\netc.).  The dynamic features of modern productivity languages like\nPython enable an alternative approach that bridges the gap between\nproductivity and performance.  SEJITS (Selective, Embedded,\nJust-in-Time Specialization) embeds domain-specific languages\n(DSLs) in high-level languages like Python for popular computational kernels such as\nstencils, matrix algebra, and others.  At runtime, the DSLs are\n\\textquotedbl{}compiled\\textquotedbl{} by combining expert-provided source code templates\nspecific to each problem type, plus a strategy for optimizing an\nabstract syntax tree representing a domain-specific but\nlanguage-independent representation of the problem instance.  The\nresult is efficiency-level (e.g. C, C++) code callable from Python\nwhose performance equals or exceeds that of handcrafted code, plus\nperformance portability by allowing multiple code generation\nstrategies within the same specializer to target\ndifferent hardware present at runtime, e.g. multicore CPUs vs. GPUs.\nApplication writers never leave the Python world, and we\ndo not assume any modification or support for parallelism in\nPython itself.",
        "We present Asp (\\textquotedbl{}Asp is SEJITS for Python\\textquotedbl{}) and initial results from\nseveral domains. We demonstrate that domain-specific specializers\nallow highly-productive Python code to obtain performance meeting\nor exceeding expert-crafted low-level code on parallel hardware,\nwithout sacrificing maintainability or portability."
      ],
      "title": "Bringing Parallel Performance to Python  with Domain-Specific Selective Embedded Just-in-Time Specialization",
      "bibliography": "",
      "authors": "Shoaib Kamil, Derrick Coetzee, Armando Fox",
      "keywords": "parallel programming, specialization",
      "paper_id": "shoaib_kamil",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-00f",
      "pages": 7,
      "author_institution": [
        "University of California, Berkeley",
        "University of California, Berkeley",
        "University of California, Berkeley"
      ],
      "copyright_holder": "Shoaib Kamil et al.",
      "page": {
        "start": 91,
        "stop": 97
      }
    },
    {
      "author_institution_map": {
        "Steven E. Gorrell": [
          "Brigham Young University"
        ],
        "Stephen M. McQuay": [
          "Brigham Young University"
        ]
      },
      "author": [
        "Stephen M. McQuay",
        "Steven E. Gorrell"
      ],
      "author_email": [
        "stephen@mcquay.me",
        "sgorrell@byu.edu"
      ],
      "abstract": [
        "The research contained herein yielded an open source interpolation library\nimplemented in and designed for use with the Python programming language.\nThis library, named smbinterp, yields an interpolation to an arbitrary\ndegree of accuracy. The smbinterp module was designed to be mesh agnostic.\nA plugin system was implemented that allows end users to conveniently and\nconsistently present their numerical results to the library for rapid\nprototyping and integration. The library includes modules that allow for\nits use in high-performance parallel computing environments. These modules\nwere implemented using built-in Python modules to simplify deployment. This\nimplementation was found to scale linearly to approximately 180\nparticipating compute processes."
      ],
      "title": "N-th-order Accurate, Distributed Interpolation Library",
      "bibliography": "",
      "authors": "Stephen M. McQuay, Steven E. Gorrell",
      "keywords": "n-th-order accurate general interpolation, distributed calculation schemes,\nmultiphysics simulation",
      "paper_id": "smcquay",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-010",
      "pages": 6,
      "author_institution": [
        "Brigham Young University",
        "Brigham Young University"
      ],
      "copyright_holder": "Stephen M. McQuay et al.",
      "page": {
        "start": 98,
        "stop": 103
      }
    },
    {
      "author_institution_map": {
        "Douglas A. Starnes": [
          "University of Memphis - Institute For Intelligent Systems"
        ]
      },
      "author": [
        "Douglas A. Starnes"
      ],
      "author_email": [
        "douglas@poweredbyalt.net"
      ],
      "abstract": [
        "In recent years, one of the fastest growing trends in information technology has been the move towards cloud computing.  The scalable concept of computing resources on demand allows applications to dynamically react to increased usage instead of having to keep resources in reserve that are often not in use but are still paid for.  There are several popular entrants into this market including Google App Engine.  Modeled after Google's own architecture for building applications, Google App Engine (GAE) provides a scalable solution for web-based applications and services including data storage, communications, application deployment and monitoring, and management tools.  With GAE, developers have the option of writing applications using an API exposed to Python.  The same benefits of using Python in other applications are available in the cloud."
      ],
      "title": "Google App Engine Python",
      "bibliography": "",
      "authors": "Douglas A. Starnes",
      "keywords": "cloud computing, web, google, application development",
      "paper_id": "starnes",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-011",
      "pages": 3,
      "author_institution": [
        "University of Memphis - Institute For Intelligent Systems"
      ],
      "copyright_holder": "Douglas A. Starnes.",
      "page": {
        "start": 104,
        "stop": 106
      }
    },
    {
      "author_institution_map": {
        "Josef Perktold": [
          "University of North Carolina, Chapel Hill"
        ],
        "Skipper Seabold": [
          "American University"
        ],
        "Wes McKinney": [
          "Duke University"
        ]
      },
      "author": [
        "Wes McKinney",
        "Josef Perktold",
        "Skipper Seabold"
      ],
      "author_email": [
        "wesmckinn@gmail.com",
        "josef.pktd@gmail.com",
        "js2796a@american.edu"
      ],
      "abstract": [
        "We introduce the new time series analysis features of scikits.statsmodels.\nThis includes descriptive statistics, statistical tests and several linear\nmodel classes, autoregressive, AR, autoregressive moving-average, ARMA, and\nvector autoregressive models VAR."
      ],
      "title": "Time Series Analysis in Python with statsmodels",
      "bibliography": "",
      "authors": "Wes McKinney, Josef Perktold, Skipper Seabold",
      "keywords": "time series analysis, statistics, econometrics, AR, ARMA, VAR, GLSAR,\nfiltering, benchmarking",
      "paper_id": "statsmodels",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-012",
      "pages": 7,
      "author_institution": [
        "Duke University",
        "University of North Carolina, Chapel Hill",
        "American University"
      ],
      "copyright_holder": "Wes McKinney et al.",
      "page": {
        "start": 107,
        "stop": 113
      }
    },
    {
      "author_institution_map": {
        "Solomon Negusse": [
          "Texas Water Development Board"
        ],
        "Tyler McEwen": [
          "Texas Water Development Board"
        ],
        "Dharhas Pothina": [
          "Texas Water Development Board"
        ]
      },
      "author": [
        "Tyler McEwen",
        "Dharhas Pothina",
        "Solomon Negusse"
      ],
      "author_email": [
        "tyler.mcewen@twdb.state.tx.us",
        "dharhas.pothina@twdb.state.tx.us",
        "solomon.negusse@twdb.state.tx.us"
      ],
      "abstract": [
        "With increasing population and water use demands in Texas, accurate estimates of lake volumes is a critical part of planning for future water supply needs. Lakes are large and surveying them is expensive in terms of labor, time and cost. High spatial resolution surveys are prohibitive to conduct, hence lake are usually surveyed along widely spaced survey lines. While this choice reduces the time spent in field data collection, it increases the time required for post processing significantly. Standard spatial interpolation techniques available in commercial software are not well suited to this problem and a custom procedure was developed using in-house Fortran software. This procedure involved difficult to repeat manual manipulation of data in graphical user interfaces, visual interpretation of data and a laborious  manually guided interpolation process. Repeatibility is important since volume differences derived from multiple surveys of individual reservoirs provides estimates of capacity loss over time due to sedimentation. Through python scripts that make use of spatial algorithms and GIS routines available within various Python scientific modules, we first streamlined our original procedure and then replaced it completely with a new pure python implementation. In this paper, we compare the original procedure, the streamlined procedure and our new pure python implementation with regard to automation, efficiency and repeatability of our lake volumetric estimates. Applying these techniques to Lake Texana in Texas, we show that the new pure python implementation reduces data post processing time from approximately 90 man hours to 8 man hours while improving repeatability and maintaining accuracy."
      ],
      "title": "Improving efficiency and repeatability of lake volume estimates using Python",
      "bibliography": "",
      "authors": "Tyler McEwen, Dharhas Pothina, Solomon Negusse",
      "keywords": "gis, spatial interpolation, hydrographic surveying, bathymetry, lake volume,\nreservoir volume, anisotropic, inverse distance wieghted, sedimentation",
      "paper_id": "tyler_mcewen",
      "video": "",
      "doi": "10.25080/Majora-ebaa42b7-013",
      "pages": 5,
      "author_institution": [
        "Texas Water Development Board",
        "Texas Water Development Board",
        "Texas Water Development Board"
      ],
      "copyright_holder": "Tyler McEwen et al.",
      "page": {
        "start": 114,
        "stop": 118
      }
    }
  ]
}
