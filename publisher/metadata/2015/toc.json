{
  "toc": [
    {
      "paper_id": "allen_downey",
      "page": {
        "stop": 5,
        "start": 1
      },
      "abstract": [
        "Using data from the National Survey of Family Growth (NSFG),\nwe investigate marriage patterns among women in the United States\nWe describe and predict age at\nfirst marriage for successive generations based on decade of\nbirth. The fraction of women married by age 22 has dropped by 11\npercentage points per decade, from 69\\% for women born in the 1940s\nto 13\\% for women born in the 90s.  The fraction of women married by\nage 42 fell more slowly, from 93\\% for women born in the 40s to 82\\%\nfor women born in the 70s.  Projections suggest that this\nfraction will be substantially lower for later generations, between\n68\\% and 72\\%.  Along with these results, this paper presents an\nintroduction to survival analysis methods and an implementation\nin Python."
      ],
      "title": "Will Millennials Ever Get Married?",
      "author": [
        "Allen B. Downey"
      ],
      "authors": "Allen B. Downey",
      "author_institution_map": {
        "Allen B. Downey": [
          "Olin College of Engineering"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-000",
      "keywords": "Survival analysis, marriage patterns, Python.",
      "author_institution": [
        "Olin College of Engineering"
      ],
      "video": "https://www.youtube.com/watch?v=XHYFNraQEEo",
      "copyright_holder": "Allen B. Downey.",
      "author_email": [
        "allen.downey@olin.edu"
      ],
      "pages": 5
    },
    {
      "paper_id": "ankur_ankan",
      "page": {
        "stop": 11,
        "start": 6
      },
      "abstract": [
        "Probabilistic Graphical Models (PGM) is a technique of compactly representing\na joint distribution by exploiting dependencies between the random variables.\nIt also allows us to do inference on joint distributions in a computationally\ncheaper way than the traditional methods. PGMs are widely used in the field\nof speech recognition, information extraction, image segmentation, modelling\ngene regulatory networks.",
        "pgmpy pgmpy is a python library for working with graphical models. It allows the\nuser to create their own graphical models and answer inference or map queries over\nthem. pgmpy has implementation of many inference algorithms like\nVariableElimination, Belief Propagation etc.",
        "This paper first gives a short introduction to PGMs and various other python\npackages available for working with PGMs. Then we discuss about creating and\ndoing inference over Bayesian Networks and Markov Networks using pgmpy."
      ],
      "title": "pgmpy: Probabilistic Graphical Models using Python",
      "author": [
        "Ankur Ankan",
        "Abinash Panda"
      ],
      "authors": "Ankur Ankan, Abinash Panda",
      "author_institution_map": {
        "Abinash Panda": [],
        "Ankur Ankan": []
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-001",
      "keywords": "Graphical Models, Bayesian Networks, Markov Networks, Variable Elimination",
      "author_institution": [],
      "video": "https://www.youtube.com/watch?v=Vcmjqx7lht0",
      "copyright_holder": "Ankur Ankan et al.",
      "author_email": [
        "ankurankan@gmail.com",
        "mailme.abinashpanda@gmail.com"
      ],
      "pages": 6
    },
    {
      "paper_id": "brian_chapman",
      "page": {
        "stop": 17,
        "start": 12
      },
      "abstract": [
        "We have been involved with teaching Python to biomedical scientists since 2005. In all, seven courses have been taught: 5 at the University of Pittsburgh, as a required course for biomedical informatics graduate students. Students have primarily been biomedical informatics graduate students with other students coming from human genetics, molecular biology, statistics, and similar fields. The range of prior computing experience has been wide: the majority of students had little or no prior programming experiences while a few students were experienced in other languages such as C/C++ and wanted to learn a scripting language for increased productivity. The semester-long courses have followed a procedural first approach then an introduction to object-oriented programming. By the end of the course students produce an independent programming project on a topic of their own choosing."
      ],
      "title": "Python as a First Programming Language for Biomedical Scientists",
      "author": [
        "Brian E. Chapman, Ph.D.",
        "Jeannie Irwin, Ph.D."
      ],
      "authors": "Brian E. Chapman, Ph.D., Jeannie Irwin, Ph.D.",
      "author_institution_map": {
        "Brian E. Chapman, Ph.D.": [
          "Department of Radiology, University of Utah"
        ],
        "Jeannie Irwin, Ph.D.": [
          "Unaffiliated"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-002",
      "keywords": "education, biomedical informatics, biomedical sciences",
      "author_institution": [
        "Department of Radiology, University of Utah",
        "Unaffiliated"
      ],
      "video": "https://www.youtube.com/watch?v=kP_glnbesJ4",
      "copyright_holder": "Brian E. Chapman, Ph.D. et al.",
      "author_email": [
        "brian.chapman@utah.edu",
        "jeannieirwin@gmail.com"
      ],
      "pages": 6
    },
    {
      "paper_id": "brian_mcfee",
      "page": {
        "stop": 24,
        "start": 18
      },
      "abstract": [
        "This document describes version 0.4.0 of librosa: a Python\npackage for audio and music signal processing.\nAt a high level, librosa provides implementations of a variety\nof common functions used throughout the field of music information retrieval.\nIn this document, a brief overview of the library's functionality is provided,\nalong with explanations of the design goals, software development\npractices, and notational conventions."
      ],
      "title": "librosa: Audio and Music Signal Analysis in Python",
      "author": [
        "Brian McFee",
        "Colin Raffel",
        "Dawen Liang",
        "Daniel P.W. Ellis",
        "Matt McVicar",
        "Eric Battenberg",
        "Oriol Nieto"
      ],
      "authors": "Brian McFee, Colin Raffel, Dawen Liang, Daniel P.W. Ellis, Matt McVicar, Eric Battenberg, Oriol Nieto",
      "author_institution_map": {
        "Daniel P.W. Ellis": [
          "LabROSA, Columbia University"
        ],
        "Brian McFee": [
          "Center for Data Science, New York University",
          "Music and Audio Research Laboratory, New York University"
        ],
        "Colin Raffel": [
          "LabROSA, Columbia University"
        ],
        "Dawen Liang": [
          "LabROSA, Columbia University"
        ],
        "Eric Battenberg": [
          "Silicon Valley AI Lab, Baidu, Inc."
        ],
        "Oriol Nieto": [
          "Music and Audio Research Laboratory, New York University"
        ],
        "Matt McVicar": [
          "Department of Engineering Mathematics, University of Bristol"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-003",
      "keywords": "audio, music, signal processing",
      "author_institution": [
        "Center for Data Science, New York University",
        "Music and Audio Research Laboratory, New York University",
        "LabROSA, Columbia University",
        "LabROSA, Columbia University",
        "LabROSA, Columbia University",
        "Department of Engineering Mathematics, University of Bristol",
        "Silicon Valley AI Lab, Baidu, Inc.",
        "Music and Audio Research Laboratory, New York University"
      ],
      "video": "https://www.youtube.com/watch?v=MhOdbtPhbLU",
      "copyright_holder": "Brian McFee et al.",
      "author_email": [
        "brian.mcfee@nyu.edu",
        "craffel@gmail.com",
        "dliang@ee.columbia.edu",
        "dpwe@ee.columbia.edu",
        "mattjamesmcvicar@gmail.com",
        "ericbattenberg@baidu.com",
        "oriol@nyu.edu"
      ],
      "pages": 7
    },
    {
      "paper_id": "chris_drake",
      "page": {
        "stop": 30,
        "start": 25
      },
      "abstract": [
        "This paper introduces PyEDA,\na Python library for electronic design automation (EDA).\nPyEDA provides both a high level interface to the representation of\nBoolean functions,\nand blazingly-fast C extensions for fundamental algorithms where\nperformance is essential.\nPyEDA is a hobby project which has the simple but audacious goal of\nimproving the state of digital design by using Python."
      ],
      "title": "PyEDA: Data Structures and Algorithms for Electronic Design Automation",
      "author": [
        "Chris Drake"
      ],
      "authors": "Chris Drake",
      "author_institution_map": {
        "Chris Drake": [
          "Drake Enterprises"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-004",
      "keywords": "",
      "author_institution": [
        "Drake Enterprises"
      ],
      "video": "https://www.youtube.com/watch?v=cljDuK0ouRs",
      "copyright_holder": "Chris Drake.",
      "author_email": [
        "cjdrake@gmail.com"
      ],
      "pages": 6
    },
    {
      "paper_id": "cory_quammen",
      "page": {
        "stop": 38,
        "start": 31
      },
      "abstract": [
        "VTK and ParaView are leading software packages for data analysis\nand visualization. Since their early years, Python has played an\nimportant role in each package. In many use cases, VTK and ParaView\nserve as modules used by Python applications. In other use cases,\nPython modules are used to generate visualization components within\nVTK. In this paper, we provide an overview of Python integration in\nVTK and ParaView and give some concrete examples of usage. We also\nprovide a roadmap for additional Python integration in VTK and\nParaView in the future."
      ],
      "title": "Scientific Data Analysis and Visualization with Python, VTK, and ParaView",
      "author": [
        "Cory Quammen"
      ],
      "authors": "Cory Quammen",
      "author_institution_map": {
        "Cory Quammen": [
          "Kitware, Inc."
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-005",
      "keywords": "data analysis, scientific visualization, VTK, ParaView",
      "author_institution": [
        "Kitware, Inc."
      ],
      "video": "https://www.youtube.com/watch?v=8ugmkKaYKxM",
      "copyright_holder": "Cory Quammen.",
      "author_email": [
        "cory.quammen@kitware.com"
      ],
      "pages": 8
    },
    {
      "paper_id": "david_lippa",
      "page": {
        "stop": 42,
        "start": 39
      },
      "abstract": [
        "Built on Google App Engine (GAE), RealMassive encountered challenges while attempting to scale its recommendation engine to match its nationwide, multi-market expansion. To address this problem, we borrowed a conceptual model from spectral data processing to transform our domain-specific problem into one that the GAE's search engine could solve. Rather than using a more traditional heuristics-based relevancy ranking, we filtered and scored results using a modified version of a spectral angle. While this approach seems to have little in common with providing a recommendation based on similarity, there are important parallels: filtering to reduce the search space; independent variables that can be resampled into a signature; a signature library to identify meaningful similarities; and an algorithm that lends itself to an accurate but flexible definition of similarity. We implemented this as a web service that provides recommendations in sub-second time. The RealMassive platform currently covers over 4.5 billion square feet of commercial real estate inventory and is expanding quickly."
      ],
      "title": "Creating a Real-Time Recommendation Engine using Modified K-Means Clustering and Remote Sensing Signature Matching Algorithms",
      "author": [
        "David Lippa",
        "Jason Vertrees"
      ],
      "authors": "David Lippa, Jason Vertrees",
      "author_institution_map": {
        "David Lippa": [
          "RealMassive, Inc."
        ],
        "Jason Vertrees": [
          "RealMassive, Inc."
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-006",
      "keywords": "algorithms, clustering, recommendation engine, remote sensing",
      "author_institution": [
        "RealMassive, Inc.",
        "RealMassive, Inc."
      ],
      "video": "",
      "copyright_holder": "RealMassive, Inc.",
      "author_email": [
        "david.lippa@realmassive.com",
        "jason.vertrees@realmassive.com"
      ],
      "pages": 4
    },
    {
      "paper_id": "howard_bushouse",
      "page": {
        "stop": 47,
        "start": 43
      },
      "abstract": [
        "The James Webb Space Telescope (JWST) is the successor to the Hubble Space\nTelescope (HST) and is currently expected to be launched in late 2018.\nThe Space Telescope Science Institute (STScI) is developing the software\nsystems that will be used to provide routine calibration of the science\ndata received from JWST. The calibration operations use a processing\nenvironment provided by a Python module called stpipe that provides\nmany common services to each calibration step, relieving step developers\nfrom having to implement such functionality. The stpipe module provides\ncommon configuration handling, parameter validation and persistence, and\nI/O management.",
        "Individual steps are written as Python classes that can be invoked\nindividually from within Python or from the stpipe command line. Any\nset of step classes can be configured into a pipeline,\nwith stpipe handling the flow of\ndata between steps. The stpipe environment includes the use of standard\ndata models. The data models, defined using json schema, provide a means of\nvalidating the correct format of the data files presented to the pipeline,\nas well as presenting an abstract interface to isolate the calibration\nsteps from details of how the data are stored on disk."
      ],
      "title": "The James Webb Space Telescope Data Calibration Pipeline",
      "author": [
        "Howard Bushouse",
        "Michael Droettboom",
        "Perry Greenfield"
      ],
      "authors": "Howard Bushouse, Michael Droettboom, Perry Greenfield",
      "author_institution_map": {
        "Howard Bushouse": [
          "Space Telescope Science Institute"
        ],
        "Perry Greenfield": [
          "Space Telescope Science Institute"
        ],
        "Michael Droettboom": [
          "Space Telescope Science Institute"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-007",
      "keywords": "pipelines, astronomy",
      "author_institution": [
        "Space Telescope Science Institute",
        "Space Telescope Science Institute",
        "Space Telescope Science Institute"
      ],
      "video": "https://www.youtube.com/watch?v=o-D4TpRFza4",
      "copyright_holder": "Howard Bushouse et al.",
      "author_email": [
        "bushouse@stsci.edu",
        "mdroe@stsci.edu",
        "perry@stsci.edu"
      ],
      "pages": 5
    },
    {
      "paper_id": "ian_henriksen",
      "page": {
        "stop": 50,
        "start": 48
      },
      "abstract": [
        "BLAS, LAPACK, and other libraries like them have formed the underpinnings of much of the scientific stack in Python.\nUntil now, the standard practice in many packages for using BLAS and LAPACK has been to link each Python extension directly against the libraries needed.\nEach module that calls these low-level libraries directly has had to link against them independently.\nThe task of finding and linking properly against the correct libraries has, in the past, been a substantial obstacle in the development and distribution of Python extension modules.",
        "Cython has existing machinery that allows C-level declarations to be shared between Cython-compiled extension modules without linking against the original libraries.\nThe Cython BLAS and LAPACK API in SciPy uses this functionality to make it so that the same BLAS and LAPACK libraries that were used to compile SciPy can be used in Python extension modules via Cython.\nThis paper will demonstrate how to create and use these APIs for both Fortran and C libraries in a platform-independent manner."
      ],
      "title": "Circumventing The Linker: Using SciPy's BLAS and LAPACK Within Cython",
      "author": [
        "Ian Henriksen"
      ],
      "authors": "Ian Henriksen",
      "author_institution_map": {
        "Ian Henriksen": [
          "Brigham Young University Math Department"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-008",
      "keywords": "Cython, BLAS, LAPACK, SciPy",
      "author_institution": [
        "Brigham Young University Math Department"
      ],
      "video": "https://www.youtube.com/watch?v=R4yB-8tB0J0",
      "copyright_holder": "Ian Henriksen.",
      "author_email": [
        "iandh@byu.edu"
      ],
      "pages": 3
    },
    {
      "paper_id": "jacqueline_kazil",
      "page": {
        "stop": 58,
        "start": 51
      },
      "abstract": [
        "Agent-based modeling is a computational methodology used in social science, biology, and other fields, which involves simulating the behavior and interaction of many autonomous entities, or agents, over time. There is currently a hole in this area in  Python’s robust and growing scientific ecosystem. Mesa is a new open-source, Apache 2.0 licensed package meant to fill that gap. It allows users to quickly create agent-based models using built-in core components (such as agent schedulers and spatial grids) or customized implementations; visualize them using a browser-based interface; and analyze their results using Python’s data analysis tools. Its goal is to be a Python 3-based alternative to other popular frameworks based in other languages such as NetLogo, Repast, or MASON. Since the framework is being built from scratch it is able to incorporate lessons from other tools. In this paper, we present Mesa's core features and demonstrate them with a simple example model.1"
      ],
      "title": "Mesa: An Agent-Based Modeling Framework",
      "author": [
        "David Masad",
        "Jacqueline Kazil"
      ],
      "authors": "David Masad, Jacqueline Kazil",
      "author_institution_map": {
        "David Masad": [
          "Department of Computational Social Science, George Mason University"
        ],
        "Jacqueline Kazil": [
          "Department of Computational Social Science, George Mason University"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-009",
      "keywords": "agent-based modeling, multi-agent systems, cellular automata, complexity, modeling, simulation",
      "author_institution": [
        "Department of Computational Social Science, George Mason University",
        "Department of Computational Social Science, George Mason University"
      ],
      "video": "https://www.youtube.com/watch?v=lcySLoprPMc",
      "copyright_holder": "David Masad et al.",
      "author_email": [
        "david.masad@gmail.com, jackiekazil@gmail.com",
        "jackiekazil@gmail.com"
      ],
      "pages": 8
    },
    {
      "paper_id": "jean-luc_stevens",
      "page": {
        "stop": 66,
        "start": 59
      },
      "abstract": [
        "Scientific visualization typically requires large amounts of custom\ncoding that obscures the underlying principles of the work and\nmakes it difficult to reproduce the results.  Here we describe how\nthe new HoloViews Python package, when combined with the IPython\nNotebook and a plotting library, provides a rich, interactive\ninterface for flexible and nearly code-free visualization of your\nresults while storing a full record of the process for later\nreproduction.",
        "HoloViews provides a set of general-purpose data structures that\nallow you to pair your data with a small amount of metadata.  These\ndata structures are then used by a separate plotting system to\nrender your data interactively, e.g. within the IPython Notebook\nenvironment, revealing even complex data in publication-quality\nform without requiring custom plotting code for each figure.",
        "HoloViews also provides powerful containers that allow you to\norganize this data for analysis, embedding it whatever\nmultidimensional continuous or discrete space best characterizes\nit. The resulting workflow allows you to focus on exploring,\nanalyzing, and understanding your data and results, while leading\ndirectly to an exportable recipe for reproducible research."
      ],
      "title": "HoloViews: Building Complex Visualizations Easily for Reproducible Science",
      "author": [
        "Jean-Luc R. Stevens",
        "Philipp Rudiger",
        "James A. Bednar"
      ],
      "authors": "Jean-Luc R. Stevens, Philipp Rudiger, James A. Bednar",
      "author_institution_map": {
        "James A. Bednar": [
          "Institute for Adaptive and Neural Computation, University of Edinburgh"
        ],
        "Philipp Rudiger": [
          "Institute for Adaptive and Neural Computation, University of Edinburgh"
        ],
        "Jean-Luc R. Stevens": [
          "Institute for Adaptive and Neural Computation, University of Edinburgh"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-00a",
      "keywords": "reproducible, interactive, visualization, notebook",
      "author_institution": [
        "Institute for Adaptive and Neural Computation, University of Edinburgh",
        "Institute for Adaptive and Neural Computation, University of Edinburgh",
        "Institute for Adaptive and Neural Computation, University of Edinburgh"
      ],
      "video": "https://www.youtube.com/watch?v=hNsR2H7Lrg0",
      "copyright_holder": "Jean-Luc R. Stevens et al.",
      "author_email": [
        "jlstevens@ed.ac.uk",
        "p.rudiger@ed.ac.uk",
        "jbednar@inf.ed.ac.uk"
      ],
      "pages": 8
    },
    {
      "paper_id": "jordi_torrents",
      "page": {
        "stop": 76,
        "start": 67
      },
      "abstract": [
        "The structural cohesion model is a powerful sociological conception of cohesion in social groups, but its diffusion in empirical literature has been hampered by computational problems. We present useful heuristics for computing structural cohesion that allow a speed-up of one order of magnitude over the algorithms currently available. Both the heuristics and the exact algorithm have been implemented on NetworkX by the first author. Using as examples three large collaboration networks (co-maintenance of Debian packages, co-authorship in Nuclear Theory, and co-authorship in High-Energy Theory) we illustrate our approach to measure structural cohesion in relatively large networks. We also introduce a novel graphical representation of the structural cohesion analysis to quickly spot differences across networks. It is implemented using matplotlib."
      ],
      "title": "Structural Cohesion: Visualization and Heuristics for Fast Computation with NetworkX and matplotlib",
      "author": [
        "Jordi Torrents",
        "Fabrizio Ferraro"
      ],
      "authors": "Jordi Torrents, Fabrizio Ferraro",
      "author_institution_map": {
        "Jordi Torrents": [
          "University of Barcelona"
        ],
        "Fabrizio Ferraro": [
          "IESE Business School"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-00b",
      "keywords": "Network Analysis, Sociology, Structural Cohesion, NetworkX, matplotlib",
      "author_institution": [
        "University of Barcelona",
        "IESE Business School"
      ],
      "video": "https://www.youtube.com/watch?v=K8RFIdG3g9Y",
      "copyright_holder": "Jordi Torrents et al.",
      "author_email": [
        "jordi.t21@gmail.com",
        "fferraro@iese.edu"
      ],
      "pages": 10
    },
    {
      "paper_id": "josh_walawender",
      "page": {
        "stop": 83,
        "start": 77
      },
      "abstract": [
        "Automated telescopes are capable of generating images more quickly than they can be inspected by a human, but detailed information on the performance of the telescope is valuable for monitoring and tuning of their operation.  The IQMon (Image Quality Monitor) package1 was developed to provide basic image quality metrics of automated telescopes in near real time."
      ],
      "title": "Automated Image Quality Monitoring with IQMon",
      "author": [
        "Josh Walawender"
      ],
      "authors": "Josh Walawender",
      "author_institution_map": {
        "Josh Walawender": [
          "Subaru Telescope, National Astronomical Observatory of Japan"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-00c",
      "keywords": "astronomy, automated telescopes, image quality",
      "author_institution": [
        "Subaru Telescope, National Astronomical Observatory of Japan"
      ],
      "video": "https://www.youtube.com/watch?v=dGLkDOvYOHA",
      "copyright_holder": "Josh Walawender.",
      "author_email": [
        "joshwalawender@me.com"
      ],
      "pages": 7
    },
    {
      "paper_id": "kathryn_huff",
      "page": {
        "stop": 90,
        "start": 84
      },
      "abstract": [
        "In this work, a new python package, PyRK (Python for Reactor Kinetics), is\nintroduced.  PyRK has been designed to simulate, in zero\ndimensions, the transient, coupled, thermal-hydraulics and neutronics of\ntime-dependent behavior in nuclear reactors. PyRK is intended for analysis\nof many commonly studied transient scenarios including normal reactor\nstartup and shutdown as well as abnormal scenarios including Beyond Design\nBasis Events (BDBEs) such as Accident Transients Without Scram (ATWS). For\nrobustness, this package employs various tools within the scientific python\necosystem. For additional ease of use, it employs a reactor-agnostic,\nobject-oriented data model, allowing nuclear engineers to rapidly prototype\nnuclear reactor control and safety systems in the context of their novel\nnuclear reactor designs."
      ],
      "title": "PyRK: A Python Package For Nuclear Reactor Kinetics",
      "author": [
        "Kathryn Huff"
      ],
      "authors": "Kathryn Huff",
      "author_institution_map": {
        "Kathryn Huff": [
          "University of California, Berkeley"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-00d",
      "keywords": "engineering, nuclear reactor, package",
      "author_institution": [
        "University of California, Berkeley"
      ],
      "video": "https://www.youtube.com/watch?v=2HToG61wMWI",
      "copyright_holder": "Kathryn Huff.",
      "author_email": [
        "katyhuff@gmail.com"
      ],
      "pages": 7
    },
    {
      "paper_id": "luke_campagnola-vispy",
      "page": {
        "stop": 96,
        "start": 91
      },
      "abstract": [
        "The growing availability of large, multidimensional data sets has created\ndemand for high-performance, interactive visualization tools. VisPy\nleverages the GPU to provide fast, interactive, and beautiful visualizations\nin a high-level API. Here we introduce the main features,\narchitecture, and techniques used in VisPy."
      ],
      "title": "VisPy: Harnessing The GPU For Fast, High-Level Visualization",
      "author": [
        "Luke Campagnola",
        "Almar Klein",
        "Eric Larson",
        "Cyrille Rossant",
        "Nicolas Rougier"
      ],
      "authors": "Luke Campagnola, Almar Klein, Eric Larson, Cyrille Rossant, Nicolas Rougier",
      "author_institution_map": {
        "Almar Klein": [
          "Continuum Analytics"
        ],
        "Nicolas Rougier": [
          "French National Institute for Research in Computer Science and Control"
        ],
        "Luke Campagnola": [
          "University of North Carolina at Chapel Hill"
        ],
        "Cyrille Rossant": [
          "University College London"
        ],
        "Eric Larson": [
          "University of Washington"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-00e",
      "keywords": "graphics, visualization, plotting, performance, interactive, opengl",
      "author_institution": [
        "University of North Carolina at Chapel Hill",
        "Continuum Analytics",
        "University of Washington",
        "University College London",
        "French National Institute for Research in Computer Science and Control"
      ],
      "video": "https://www.youtube.com/watch?v=_3YoaeoiIFI",
      "copyright_holder": "Luke Campagnola et al.",
      "author_email": [
        "luke.campagnola@gmail.com",
        "almar.klein@gmail.com",
        "eric.larson.d@gmail.com",
        "cyrille.rossant@gmail.com",
        "Nicolas.Rougier@inria.fr"
      ],
      "pages": 6
    },
    {
      "paper_id": "margaret_mahan",
      "page": {
        "stop": 104,
        "start": 97
      },
      "abstract": [
        "Time series analysis has been a dominant technique for assessing relations within datasets collected over time and is becoming increasingly prevalent in the scientific community; for example, assessing brain networks by calculating pairwise correlations of time series generated from different areas of the brain. The assessment of these relations relies, in turn, on the proper calculation of interactions between time series, which is achieved by rendering each individual series stationary and nonautocorrelated (i.e., white noise, or to “prewhiten” the series). This ensures that the relations computed subsequently are due to the interactions between the series and do not reflect internal dependencies of the series themselves. An established method for prewhitening time series is to apply an Autoregressive (AR, p) Integrative (I, d) Moving Average (MA, q) model (ARIMA) and retain the residuals. To diagnostically check whether the model orders (p, d, q) are sufficient, both visualization and statistical tests (e.g., Ljung-Box test) of the residuals are performed. However, these tests are not robust for high-order models in long time series. Additionally, as dataset size increases (i.e., number of time series to model) it is not feasible to visually inspect each series independently. As a result, there is a need for robust alternatives to diagnostic evaluations of ARIMA modeling. Here, we demonstrate how to perform ARIMA modeling of long time series using Statsmodels, a library for statistical analysis in Python. Then, we present a comprehensive procedure (White Noise Test) to detect autocorrelation and nonstationarities in prewhitened time series, thereby establishing that the series does not differ significantly from white noise. This test was validated using time series collected from magnetoencephalography recordings. Overall, our White Noise Test provides a robust alternative to diagnostic checks of ARIMA modeling for long time series."
      ],
      "title": "White Noise Test: detecting autocorrelation and nonstationarities in long time series after ARIMA modeling",
      "author": [
        "Margaret Y Mahan",
        "Chelley R Chorn",
        "Apostolos P Georgopoulos"
      ],
      "authors": "Margaret Y Mahan, Chelley R Chorn, Apostolos P Georgopoulos",
      "author_institution_map": {
        "Apostolos P Georgopoulos": [
          "Brain Sciences Center, Minneapolis VA Health Care System \\& University of Minnesota"
        ],
        "Margaret Y Mahan": [
          "Brain Sciences Center, Minneapolis VA Health Care System \\& University of Minnesota"
        ],
        "Chelley R Chorn": [
          "Brain Sciences Center, Minneapolis VA Health Care System \\& University of Minnesota"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-00f",
      "keywords": "Time series, Statsmodels, ARIMA, statistics",
      "author_institution": [
        "Brain Sciences Center, Minneapolis VA Health Care System \\& University of Minnesota",
        "Brain Sciences Center, Minneapolis VA Health Care System \\& University of Minnesota",
        "Brain Sciences Center, Minneapolis VA Health Care System \\& University of Minnesota"
      ],
      "video": "",
      "copyright_holder": "Margaret Y Mahan et al.",
      "author_email": [
        "mahan027@umn.edu"
      ],
      "pages": 8
    },
    {
      "paper_id": "mark_wickert",
      "page": {
        "stop": 112,
        "start": 105
      },
      "abstract": [
        "This paper will take the audience through the story of how an electrical and computer\nengineering faculty member has come to embrace Python, in particular IPython Notebook\n(IPython kernel for Jupyter),\nas an analysis and simulation tool for both teaching and research in signal processing\nand communications. Legacy tools such as MATLAB are well established (entrenched) in\nthis discipline, but engineers need to be aware of alternatives, especially in the case\nof Python where there is such a vibrant community of developers.\nIn this paper case studies will also be used to describe domain\nspecific code modules that are being developed to support both lecture and lab oriented\ncourses going through the conversion from MATLAB to Python. These modules in particular\naugment scipy.signal in a very positive way and enable rapid prototyping of\ncommunications and signal processing algorithms. Both student and industry team\nmembers in subcontract work, have responded favorably to the use of Python as an\nengineering problem solving platform. In teaching, IPython notebooks are used to augment\nlecture material with live calculations and simulations. These same notebooks are then\nplaced on the course Web Site so students can download and tinker on their own. This\nactivity also encourages learning more about the language core and Numpy, relative to\nMATLAB. The students quickly mature and are able to turn in homework solutions and\ncomplete computer simulation projects, all in the notebook. Rendering notebooks to\nPDF via LaTeX is also quite popular. The next step is to get other signals and systems faculty\ninvolved."
      ],
      "title": "Signal Processing and Communications: Teaching and Research Using IPython Notebook",
      "author": [
        "Mark Wickert"
      ],
      "authors": "Mark Wickert",
      "author_institution_map": {
        "Mark Wickert": [
          "University of Colorado Colorado Springs"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-010",
      "keywords": "numerical computing, signal processing, communications systems, system modeling",
      "author_institution": [
        "University of Colorado Colorado Springs"
      ],
      "video": "https://www.youtube.com/watch?v=xWREmn7EajM",
      "copyright_holder": "Mark Wickert.",
      "author_email": [
        "mwickert@uccs.edu"
      ],
      "pages": 8
    },
    {
      "paper_id": "mattheus_ueckermann",
      "page": {
        "stop": 120,
        "start": 113
      },
      "abstract": [
        "Hydrological terrain analysis is important for applications such as environmental resource, agriculture, and flood risk management. It is based on processing of high-resolution, tiled digital elevation model (DEM) data for geographic regions of interest.  A major challenge in global hydrological terrain analysis is addressing cross-tile dependencies that arise from the tiled nature of the underlying DEM data, which is too large to hold in memory as a single array. We are not aware of existing tools that can accurately and efficiently perform global terrain analysis within current memory and computational constraints. We solved this problem by implementing a new algorithm in Python, which uses a simple but robust file-based locking mechanism to coordinate the work flow between an arbitrary number of independent processes operating on separate DEM tiles.",
        "We used this system to analyze the conterminous US’s terrain at 1 arc-second resolution in under 3 days on a single compute node, and global terrain at 3 arc-second resolution in under 4 days. Our solution is implemented and made available as pyDEM, an open source Python/Cython library that enables global geospatial terrain analysis. We will describe our algorithm for calculating various terrain analysis parameters of interest, our file-based locking mechanism to coordinate the work between processors, and optimization using Cython. We will demonstrate pyDEM on a few example test cases, as well as real DEM data."
      ],
      "title": "pyDEM: Global Digital Elevation Model Analysis",
      "author": [
        "Mattheus P. Ueckermann",
        "Robert D. Chambers",
        "Christopher A. Brooks",
        "William E. Audette III",
        "Jerry Bieszczad"
      ],
      "authors": "Mattheus P. Ueckermann, Robert D. Chambers, Christopher A. Brooks, William E. Audette III, Jerry Bieszczad",
      "author_institution_map": {
        "William E. Audette III": [
          "Creare LLC, Hanover, NH"
        ],
        "Christopher A. Brooks": [
          "Creare LLC, Hanover, NH"
        ],
        "Robert D. Chambers": [
          "Creare LLC, Hanover, NH"
        ],
        "Jerry Bieszczad": [
          "Creare LLC, Hanover, NH"
        ],
        "Mattheus P. Ueckermann": [
          "Creare LLC, Hanover, NH"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-011",
      "keywords": "digital elevation model, hydrology, terrain analysis, topographic wetness index",
      "author_institution": [
        "Creare LLC, Hanover, NH",
        "Creare LLC, Hanover, NH",
        "Creare LLC, Hanover, NH",
        "Creare LLC, Hanover, NH",
        "Creare LLC, Hanover, NH"
      ],
      "video": "https://www.youtube.com/watch?v=bGulPZh_-Mo",
      "copyright_holder": "Mattheus P. Ueckermann et al.",
      "author_email": [
        "mpu@creare.com",
        "rxc@creare.com",
        "cab@creare.com",
        "wea@creare.com",
        "jyb@creare.com"
      ],
      "pages": 8
    },
    {
      "paper_id": "matthew_craig",
      "page": {
        "stop": 125,
        "start": 121
      },
      "abstract": [
        "This paper describes a tool for astronomical research implemented as an\nIPython notebook with a widget interface. The notebook uses Astropy, a\ncommunity-developed package of fundamental tools for astronomy, and\nAstropy affiliated packages, as the back end. The widget interface makes\nAstropy a much more useful tool to undergraduates or other non-experts\ndoing research in astronomy, filling a niche for software that connects\nbeginners to research-grade code."
      ],
      "title": "Widgets and Astropy: Accomplishing Productive Research with Undergraduates",
      "author": [
        "Matthew Craig"
      ],
      "authors": "Matthew Craig",
      "author_institution_map": {
        "Matthew Craig": [
          "Department of Physics and Astronomy, Minnesota State University Moorhead"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-012",
      "keywords": "astronomy",
      "author_institution": [
        "Department of Physics and Astronomy, Minnesota State University Moorhead"
      ],
      "video": "https://www.youtube.com/watch?v=hyxCDdBH1Mg",
      "copyright_holder": "Matthew Craig.",
      "author_email": [
        "mcraig@mnstate.edu"
      ],
      "pages": 5
    },
    {
      "paper_id": "matthew_rocklin",
      "page": {
        "stop": 132,
        "start": 126
      },
      "abstract": [
        "Dask enables parallel and out-of-core computation.  We couple blocked\nalgorithms with dynamic and memory aware task scheduling to achieve a\nparallel and out-of-core NumPy clone.  We show how this extends the\neffective scale of modern hardware to larger datasets and discuss how these\nideas can be more broadly applied to other parallel collections."
      ],
      "title": "Dask: Parallel Computation with Blocked algorithms and Task Scheduling",
      "author": [
        "Matthew Rocklin"
      ],
      "authors": "Matthew Rocklin",
      "author_institution_map": {
        "Matthew Rocklin": [
          "Continuum Analytics"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-013",
      "keywords": "parallelism, NumPy, scheduling",
      "author_institution": [
        "Continuum Analytics"
      ],
      "video": "https://www.youtube.com/watch?v=1kkFZ4P-XHg",
      "copyright_holder": "Matthew Rocklin.",
      "author_email": [
        "mrocklin@gmail.com"
      ],
      "pages": 7
    },
    {
      "paper_id": "mellissa_cross_p",
      "page": {
        "stop": 137,
        "start": 133
      },
      "abstract": [
        "The National Oceanic and Atmospheric Administration (NOAA) Air Resources Laboratory's HYSPLIT (HYbrid Single Particle Lagrangian Transport) model Drax98, Drax97 uses a hybrid Langrangian and Eulerian calculation method to compute air parcel trajectories and particle dispersion and deposition simulations.  Air parcels are hypothetical small volumes of air with uniform characteristics.  The HYSPLIT model outputs air parcel paths projected forwards or backwards in time (trajectories) and is used in a variety of scientific contexts.  Here we present the first package in the mainstream scientific Python ecosystem designed to facilitate HYSPLIT trajectory analysis workflow by providing an intuitive API for generating, inspecting, and plotting trajectory paths and data."
      ],
      "title": "PySPLIT: a Package for the Generation, Analysis, and Visualization of HYSPLIT Air Parcel Trajectories",
      "author": [
        "Mellissa Cross"
      ],
      "authors": "Mellissa Cross",
      "author_institution_map": {
        "Mellissa Cross": [
          "Department of Earth Sciences, University of Minnesota"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-014",
      "keywords": "HYSPLIT, trajectory analysis, matplotlib Basemap",
      "author_institution": [
        "Department of Earth Sciences, University of Minnesota"
      ],
      "video": "https://www.youtube.com/watch?v=2mzhTC4Kp-Y",
      "copyright_holder": "Mellissa Cross.",
      "author_email": [
        "cros0324@umn.edu, mellissa.cross@gmail.com"
      ],
      "pages": 5
    },
    {
      "paper_id": "mellissa_cross_t",
      "page": {
        "stop": 143,
        "start": 138
      },
      "abstract": [
        "TrendVis is a plotting package that uses matplotlib to create information-dense, sparkline-like, quantitative visualizations of multiple disparate data sets in a common plot area against a common variable.  This plot type is particularly well-suited for time-series data.  We discuss the rationale behind and the challenges associated with adapting matplotlib to this particular plot style, the TrendVis API and architecture, and various features available for users to customize and enhance the readability of their figures while walking through a sample workflow."
      ],
      "title": "TrendVis: an Elegant Interface for dense, sparkline-like, quantitative visualizations of multiple series using matplotlib",
      "author": [
        "Mellissa Cross"
      ],
      "authors": "Mellissa Cross",
      "author_institution_map": {
        "Mellissa Cross": [
          "Department of Earth Sciences, University of Minnesota"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-015",
      "keywords": "time series visualization, matplotlib, plotting",
      "author_institution": [
        "Department of Earth Sciences, University of Minnesota"
      ],
      "video": "https://www.youtube.com/watch?v=tklAFsce7eg",
      "copyright_holder": "Mellissa Cross.",
      "author_email": [
        "cros0324@umn.edu, mellissa.cross@gmail.com"
      ],
      "pages": 6
    },
    {
      "paper_id": "mike_pacer",
      "page": {
        "stop": 151,
        "start": 144
      },
      "abstract": [
        "Probabilistic graphical models are useful tools for modeling systems governed by probabilistic structure. Bayesian networks are one class of probabilistic graphical model that have proven useful for characterizing both formal systems and for reasoning with those systems. Probabilistic dependencies in Bayesian networks are graphically expressed in terms of directed links from parents to their children. Casual Bayesian networks are a generalization of Bayesian networks that allow one to \\textquotedbl{}intervene\\textquotedbl{} and perform \\textquotedbl{}graph surgery\\textquotedbl{} — cutting nodes off from their parents. Causal theories are a formal framework for generating causal Bayesian networks.",
        "This report provides a brief introduction to the formal tools needed to comprehend Bayesian networks, including probability theory and graph theory. Then, it describes Bayesian networks and causal Bayesian networks. It introduces some of the most basic functionality of the extensive NetworkX python package for working with complex graphs and networks networkx. I introduce some utilities I have build on top of NetworkX including conditional graph enumeration and sampling from discrete valued Bayesian networks encoded in NetworkX graphs pacer2015cbnx. I call this Causal Bayesian NetworkX, or cbnx. I conclude by introducing a formal framework for generating causal Bayesian networks called theory based causal induction griffithst09, out of which these utilities emerged. I discuss the background motivations for frameworks of this sort, their use in computational cognitive science, and the use of computational cognitive science for the machine learning community at large."
      ],
      "title": "Causal Bayesian NetworkX",
      "author": [
        "Michael D. Pacer"
      ],
      "authors": "Michael D. Pacer",
      "author_institution_map": {
        "Michael D. Pacer": [
          "University of California at Berkeley"
        ]
      },
      "bibliography": "myBibliography",
      "doi": "10.25080/Majora-7b98e3ed-016",
      "keywords": "probabilistic graphical models, causal theories, Bayesian networks, computational cognitive science, networkx",
      "author_institution": [
        "University of California at Berkeley"
      ],
      "video": "https://www.youtube.com/watch?v=qWAQgWOD_nA",
      "copyright_holder": "Michael D. Pacer.",
      "author_email": [
        "mpacer@berkeley.edu"
      ],
      "pages": 8
    },
    {
      "paper_id": "nicola_creati",
      "page": {
        "stop": 157,
        "start": 152
      },
      "abstract": [
        "The deformation of the Earth surface reflects the action of several forces that act inside the planet. To understand how the Earth surface evolves complex models must be built to reconcile observations with theoretical numerical simulations. Starting from a well known numerical methodology already used among the geodynamic scientific community, PyGmod has been developed from scratch in the last year. The application simulates 2D large scale geodynamic processes by solving the conservation equations of mass, momentum, and energy by a finite difference method with a marker-in-cell technique.\nUnlike common simulation code written in Fortran or C this code is written in Python. The code implements a new approach that takes advantage of the hybrid architecture of the latest HPC machines. In PyGmod the standard MPI is coupled with a threading architecture to speed up some critical computations. Since the OpenMP API cannot be used with Python, threading is implemented in Cython. In addition a realtime visualization library has been developed to inspect the evolution of the model during the computation."
      ],
      "title": "Geodynamic simulations in HPC with Python",
      "author": [
        "Nicola Creati",
        "Roberto Vidmar",
        "Paolo Sterzai"
      ],
      "authors": "Nicola Creati, Roberto Vidmar, Paolo Sterzai",
      "author_institution_map": {
        "Roberto Vidmar": [
          "Istituto Nazionale di Oceanografia e di Geofisica Sperimentale, OGS"
        ],
        "Paolo Sterzai": [
          "Istituto Nazionale di Oceanografia e di Geofisica Sperimentale, OGS"
        ],
        "Nicola Creati": [
          "Istituto Nazionale di Oceanografia e di Geofisica Sperimentale, OGS"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-017",
      "keywords": "HPC, numerical modelling, geodynamics",
      "author_institution": [
        "Istituto Nazionale di Oceanografia e di Geofisica Sperimentale, OGS",
        "Istituto Nazionale di Oceanografia e di Geofisica Sperimentale, OGS",
        "Istituto Nazionale di Oceanografia e di Geofisica Sperimentale, OGS"
      ],
      "video": "https://www.youtube.com/watch?v=PTEgs7salEc",
      "copyright_holder": "Nicola Creati et al.",
      "author_email": [
        "ncreati@inogs.it",
        "rvidmar@inogs.it",
        "psterzai@inogs.it"
      ],
      "pages": 6
    },
    {
      "paper_id": "qiita_development_team",
      "page": {
        "stop": 163,
        "start": 158
      },
      "abstract": [
        "Advances in sequencing, proteomics, transcriptomics and metabolomics are\ngiving us new insights into the microbial world and dramatically improving\nour ability to understand microbial community composition and function at\nhigh resolution. These new technologies are generating vast amounts of data,\neven from a single study or sample, leading to challenges in storage,\nrepresentation, analysis, and integration of the disparate data types.",
        "Qiita (https://github.com/biocore/qiita) aims to be the leading platform to\nstore, analyze, and share multi-omics data. Qiita is BSD-licensed,\nunit-tested, and adherent to PEP8 style guidelines. New code additions are\nreviewed by multiple developers and tested using Travis CI. This approach\nopens development to the largest possible number of experts in \\textquotedbl{}-omics\\textquotedbl{}\nfields. The heterogeneous data generated by these disciplines led us to use\na combination of Redis, PostgreSQL, BIOM (Atr10), and HDF5 for relational\nand hierarchical storage. The compute backend is provided by IPython’s\nparallel framework (http://ipython.org/). In addition, the project depends\non mature Python packages such as Tornado\n(http://www.tornadoweb.org/en/stable/), click (http://click.pocoo.org/4/),\nscipy (http://www.scipy.org), numpy (http://www.numpy.org), and scikit-bio\n(http://scikit-bio.org) among others. Most notably, the analysis pipeline is\nprovided by QIIME (http://qiime.org), with EMPeror\n(http://emperor.microbio.me) serving as the visualization platform for\nhigh-dimensional ordination plots, which can be recolored interactively and\nmanipulated using the sample metadata.",
        "By providing the database and compute resources at http://qiita.microbio.me\nto the global community of microbiome researchers, Qiita alleviates the\ntechnical burdens, such as familiarity with the command line or access to\ncompute power, that are typically limiting for researchers studying\nmicrobial ecology, while at the same time promoting an open access culture.\nBecause Qiita is entirely open source and highly scalable, developers can\ninspect, customize, and extend it to suit their needs regardless of whether\nit is deployed as a desktop application or as a shared resource."
      ],
      "title": "Qiita: report of progress towards an open access microbiome data analysis and visualization platform",
      "author": [
        "The Qiita Development Team"
      ],
      "authors": "The Qiita Development Team",
      "author_institution_map": {
        "The Qiita Development Team": [
          "University of California, San Diego"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-018",
      "keywords": "Microbiome, multi-omics, open science, metagenomics, metatranscriptomics,\nmetaproteomics, metabolomics",
      "author_institution": [
        "University of California, San Diego"
      ],
      "video": "https://www.youtube.com/watch?v=TQzXwQ9Vx08",
      "copyright_holder": "The Qiita Development Team.",
      "author_email": [
        "robknight@ucsd.edu"
      ],
      "pages": 6
    },
    {
      "paper_id": "randy_paffenroth",
      "page": {
        "stop": 170,
        "start": 164
      },
      "abstract": [
        "In this paper we demonstrate how Python can be used throughout the\nentire life cycle of a graduate program in Data Science.  In\ninterdisciplinary fields, such as Data Science, the students often\ncome from a variety of different backgrounds where, for example,\nsome students may have strong mathematical training but less\nexperience in programming.  Python’s ease of use, open source\nlicense, and access to a vast array of libraries make it\nparticularly suited for such students.  In particular, we will\ndiscuss how Python, IPython notebooks, scikit-learn, NumPy, SciPy,\nand pandas can be used in several phases of graduate Data Science\neducation, starting from introductory classes (covering topics such\nas data gathering, data cleaning, statistics, regression,\nclassification, machine learning, etc.) and culminating in degree\ncapstone research projects using more advanced ideas such as convex\noptimization, non-linear dimension reduction, and compressed\nsensing.  One particular item of note is the scikit-learn library,\nwhich provides numerous routines for machine learning.  Having\naccess to such a library allows interesting problems to be addressed\nearly in the educational process and the experience gained with such\n“black box” routines provides a firm foundation for the students own\nsoftware development, analysis, and research later in their academic\nexperience."
      ],
      "title": "Python in Data Science Research and Education",
      "author": [
        "Randy Paffenroth",
        "Xiangnan Kong"
      ],
      "authors": "Randy Paffenroth, Xiangnan Kong",
      "author_institution_map": {
        "Randy Paffenroth": [
          "Worcester Polytechnic Institute, Mathematical Sciences Department and Data Science Program"
        ],
        "Xiangnan Kong": [
          "Worcester Polytechnic Institute, Computer Science Department and Data Science Program"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-019",
      "keywords": "data science, education, machine learning",
      "author_institution": [
        "Worcester Polytechnic Institute, Mathematical Sciences Department and Data Science Program",
        "Worcester Polytechnic Institute, Computer Science Department and Data Science Program"
      ],
      "video": "https://www.youtube.com/watch?v=EUEHOYl0mRg",
      "copyright_holder": "Randy Paffenroth et al.",
      "author_email": [
        "rcpaffenroth@wpi.edu",
        "xkong@wpi.edu"
      ],
      "pages": 7
    },
    {
      "paper_id": "scott_james",
      "page": {
        "stop": 174,
        "start": 171
      },
      "abstract": [],
      "title": "Relation: The Missing Container",
      "author": [
        "Scott James",
        "James Larkin"
      ],
      "authors": "Scott James, James Larkin",
      "author_institution_map": {
        "Scott James": [
          "Noblis"
        ],
        "James Larkin": [
          "Noblis"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-01a",
      "keywords": "",
      "author_institution": [
        "Noblis",
        "Noblis"
      ],
      "video": "",
      "copyright_holder": "Scott James et al.",
      "author_email": [
        "scott.james@noblis.org",
        "james.larkin@noblis.org"
      ],
      "pages": 4
    },
    {
      "paper_id": "sebastian_benthall",
      "page": {
        "stop": 181,
        "start": 175
      },
      "abstract": [
        "We introduce BigBang, a new Python toolkit for analyzing\nonline collaborative communities such as those that\nbuild open source software.\nMailing lists serve as critical communications infrastructure for\nmany communities, including several of the open source software\ndevelopment communities that build scientific Python packages.\nBigBang provides tools for analyzing mailing lists.\nAs a demonstration, in this paper  we test a generative\nmodel of network growth on collaborative communities.\nWe derive social networks from archival mailing list history\nand test the Barabási-Alpert model against this data.\nWe find the model does not fit the data, but that mailing list\nsocial networks share statistical regularities.\nThis suggests room for a new generative model of network formation\nin the open collaborative setting."
      ],
      "title": "Testing Generative Models of Online Collaboration with BigBang",
      "author": [
        "Sebastian Benthall"
      ],
      "authors": "Sebastian Benthall",
      "author_institution_map": {
        "Sebastian Benthall": [
          "UC Berkeley School of Information"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-01b",
      "keywords": "mailing lists, network analysis, assortativity, power law distributions,\ncollaboration",
      "author_institution": [
        "UC Berkeley School of Information"
      ],
      "video": "https://www.youtube.com/watch?v=AQFS_ES7rT0",
      "copyright_holder": "Sebastian Benthall.",
      "author_email": [
        "sb@ischool.berkeley.edu"
      ],
      "pages": 7
    },
    {
      "paper_id": "sebastian_sepulveda",
      "page": {
        "stop": 186,
        "start": 182
      },
      "abstract": [
        "This article presents an open-source Python software package, dubbed RTGraph, to visualize, process and record physiological signals (electrocardiography, electromyography, etc.) in real-time. RTGraph has a multiprocess architecture. This allows RTGraph to take advantage of multiple cores and to be able to handle data rates typically encountered during the acquisition and processing of biomedical signals. It also allows RTGraph to have a clean separation between the communication and visualization code. The paper presents the architecture and some programming details of RTGraph. It also includes three examples where RTGraph was adapted to work with (i) signals from a Inertial Measurement Unit (IMU) in the context of a biomechanical experiment; (ii) electromyography signals to estimate muscle fatigue; and (iii) pressure signals from a device used to monitor nutrition disorders in premature infants."
      ],
      "title": "Visualizing physiological signals in real-time",
      "author": [
        "Sebastián Sepúlveda",
        "Pablo Reyes",
        "Alejandro Weinstein"
      ],
      "authors": "Sebastián Sepúlveda, Pablo Reyes, Alejandro Weinstein",
      "author_institution_map": {
        "Alejandro Weinstein": [
          "Escuela de Ingeniería Civil Biomédica, Facultad de Ingeniería, Universidad de Valparaíso"
        ],
        "Sebastián Sepúlveda": [
          "Escuela de Ingeniería Civil Biomédica, Facultad de Ingeniería, Universidad de Valparaíso"
        ],
        "Pablo Reyes": [
          "Escuela de Ingeniería Civil Biomédica, Facultad de Ingeniería, Universidad de Valparaíso"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-01c",
      "keywords": "real-time processing, visualization, signal processing",
      "author_institution": [
        "Escuela de Ingeniería Civil Biomédica, Facultad de Ingeniería, Universidad de Valparaíso",
        "Escuela de Ingeniería Civil Biomédica, Facultad de Ingeniería, Universidad de Valparaíso",
        "Escuela de Ingeniería Civil Biomédica, Facultad de Ingeniería, Universidad de Valparaíso"
      ],
      "video": "https://www.youtube.com/watch?v=6WxkOeTuX7w",
      "copyright_holder": "Sebastián Sepúlveda et al.",
      "author_email": [
        "ssepulveda.sm@gmail.com",
        "pablo.reyes@uv.cl",
        "alejandro.weinstein@uv.cl"
      ],
      "pages": 5
    },
    {
      "paper_id": "yannick_congo",
      "page": {
        "stop": 193,
        "start": 187
      },
      "abstract": [
        "The notion of capturing each execution of a script and workflow and its\nassociated metadata is enormously appealing and should be at the heart of\nany attempt to make scientific simulations repeatable and reproducible.",
        "Most of the work in the literature focus in the terminology and the\napproaches to acquire those metadata. Those are critical but not enough.\nSince one of the purposes of capturing an execution is to be able to\nrecreate the same execution environment as in the original run, there is a\ngreat need to investigate ways to recreate a similar environment from those\nmetadata and also to be able to make them accessible to the community for\ncollaboration. The so popular social collaborative pull request mechanism\nin Github is a great example of how cloud infrastructures can bring another\nlayer of public collaboration. We think reproducibility could benefit from\na cloud social collaborative presence because capturing the metadata about\na simulation is far from being the end game of making it reproducible,\nrepeatable or of any use to another scientist that has difficulties to\neasily get them.",
        "In this paper we define a reproducibility record atom and the cloud\ninfrastructure to support it. We also provide a use case example with the\nevent based simulation management tool Sumatra and the container system\nDocker."
      ],
      "title": "Building a Cloud Service for Reproducible Simulation Management",
      "author": [
        "Faical Yannick Palingwende Congo"
      ],
      "authors": "Faical Yannick Palingwende Congo",
      "author_institution_map": {
        "Faical Yannick Palingwende Congo": [
          "LIMOS - UMR CNRS 6158, Blaise Pascal Univerity, Campus Universitaire des Cezeaux, 2 Rue de la Chebarde, TSA 60125 - CS, 60026, 63178 Aubière CEDEX FRANCE"
        ]
      },
      "bibliography": "",
      "doi": "10.25080/Majora-7b98e3ed-01d",
      "keywords": "metadata, simulations, repeatable, reproducible, Sumatra, cloud, Docker.",
      "author_institution": [
        "LIMOS - UMR CNRS 6158, Blaise Pascal Univerity, Campus Universitaire des Cezeaux, 2 Rue de la Chebarde, TSA 60125 - CS, 60026, 63178 Aubière CEDEX FRANCE"
      ],
      "video": "https://www.youtube.com/watch?v=euMLYw7SNdk",
      "copyright_holder": "Faical Yannick Palingwende Congo.",
      "author_email": [
        "yannick.congo@gmail.com"
      ],
      "pages": 7
    }
  ]
}
