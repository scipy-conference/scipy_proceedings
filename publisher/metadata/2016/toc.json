{
  "toc": [
    {
      "author": [
        "Alejandro Weinstein",
        "Wael El-Deredy",
        "Stéren Chabert",
        "Myriam Fuentes"
      ],
      "author_institution": [
        "Universidad de Valparaiso, Chile",
        "Advanced Center for Electrical and Electronic Engineering",
        "Universidad de Valparaiso, Chile",
        "Advanced Center for Electrical and Electronic Engineering",
        "Universidad de Valparaiso, Chile",
        "Universidad de Valparaiso, Chile"
      ],
      "pages": 6,
      "author_institution_map": {
        "Wael El-Deredy": [
          "Universidad de Valparaiso, Chile",
          "Advanced Center for Electrical and Electronic Engineering"
        ],
        "Myriam Fuentes": ["Universidad de Valparaiso, Chile"],
        "Alejandro Weinstein": [
          "Universidad de Valparaiso, Chile",
          "Advanced Center for Electrical and Electronic Engineering"
        ],
        "Stéren Chabert": ["Universidad de Valparaiso, Chile"]
      },
      "paper_id": "alejandro_weinstein",
      "bibliography": "",
      "title": "Fitting Human Decision Making Models using Python",
      "keywords": "decision making modeling, reinforcement learning",
      "video": "",
      "abstract": [
        "A topic of interest in experimental psychology and cognitive neuroscience is to understand how humans make decisions. A common approach involves using computational models to represent the decision making process, and use the model parameters to analyze brain imaging data. These computational models are based on the Reinforcement Learning (RL) paradigm, where an agent learns to make decisions based on the difference between what it expects and what it gets each time it interacts with the environment. In the typical experimental setup, subjects are presented with a set of options, each one associated to different numerical rewards. The task for each subject is to learn, by taking a series of sequential actions, which option maximizes their total reward. The sequence of actions made by the subject and the obtained rewards are used to fit a parametric RL model. The model is fit by maximizing the likelihood of the parameters given the experiment data. In this work we present a Python implementation of this model fitting procedure. We extend the implementation to fit a model of the experimental setup known as the \\textquotedbl{}contextual bandit\\textquotedbl{}, where the probabilities of the outcome change from trial to trial depending on a predictive cue. We also developed an artificial agent that can simulate the behavior of a human making decisions under the RL paradigm. We use this artificial agent to validate the model fitting by comparing the parameters estimated from the data with the known agent parameters. We also present the results of a model fitted with experimental data. We use the standard scientific Python stack (NumPy/SciPy) to compute the likelihood function and to find its maximum. The code organization allows to easily change the RL model. We also use the Seaborn library to create a visualization with the behavior of all the subjects. The simulation results validate the correctness of the implementation. The experimental results shows the usefulness and simplicity of the program when working with experimental data. The source code of the program is available at https://github.com/aweinstein/FHDMM."
      ],
      "copyright_holder": "Alejandro Weinstein et al.",
      "doi": "10.25080/Majora-629e541a-000",
      "authors": "Alejandro Weinstein, Wael El-Deredy, Stéren Chabert, Myriam Fuentes",
      "page": {
        "start": 1,
        "stop": 6
      },
      "author_email": [
        "alejandro.weinstein@uv.cl",
        "wael.el-deredy@uv.cl",
        "steren.chabert@uv.cl",
        "mfuentes018@gmail.com"
      ]
    },
    {
      "author": ["Andrew M. Fraser", "Stephen A. Andrews"],
      "author_institution": [
        "XCP-8, Los Alamos National Laboratory",
        "XCP-8, Los Alamos National Laboratory"
      ],
      "pages": 8,
      "author_institution_map": {
        "Andrew M. Fraser": ["XCP-8, Los Alamos National Laboratory"],
        "Stephen A. Andrews": ["XCP-8, Los Alamos National Laboratory"]
      },
      "paper_id": "andrew_fraser",
      "bibliography": "",
      "title": "Functional Uncertainty Constrained by Law and Experiment",
      "keywords": "python, uncertainty quantification, Bayesian inference, convex\noptimization, reproducible research, function estimation, equation\nof state, inverse problems",
      "video": "",
      "abstract": [
        "Many physical processes are modeled by unspecified functions.\nHere, we introduce the F\\_UNCLE project which uses the Python\necosystem of scientific software to develop and explore techniques\nfor estimating such unknown functions and our uncertainty about\nthem.  The work provides ideas for quantifying uncertainty about\nfunctions given the constraints of both laws governing the\nfunction's behavior and experimental data.  We present an analysis\nof pressure as a function of volume for the gases produced by\ndetonating an imaginary explosive, estimating a best pressure\nfunction and using estimates of Fisher information to quantify\nhow well a collection of experiments constrains uncertainty about\nthe function.  A need to model particular physical processes has\ndriven our work on the project, and we conclude with a plot from\nsuch a process."
      ],
      "copyright_holder": "Andrew M. Fraser et al.",
      "doi": "10.25080/Majora-629e541a-001",
      "authors": "Andrew M. Fraser, Stephen A. Andrews",
      "page": {
        "start": 7,
        "stop": 14
      },
      "author_email": ["afraser@lanl.gov", "saandrews@lanl.gov"]
    },
    {
      "author": ["Anton Malakhov"],
      "author_institution": ["Intel Corporation"],
      "pages": 5,
      "author_institution_map": {
        "Anton Malakhov": ["Intel Corporation"]
      },
      "paper_id": "anton_malakhov",
      "bibliography": "",
      "title": "Composable Multi-Threading for Python Libraries",
      "keywords": "Multi-threading, Over-subscription, Parallel Computations, Nested Parallelism, Multi-core, Python, GIL, Dask, Joblib, NumPy, SciPy, Numba, TBB",
      "video": "https://youtu.be/kfQcWez2URE",
      "abstract": [
        "Python is popular among numeric communities that value it for easy to use number crunching modules like NumPy, SciPy, Dask, Numba, and many others.\nThese modules often use multi-threading for efficient multi-core parallelism in order to utilize all the available CPU cores.\nNevertheless, their threads can interfere with each other leading to overhead and inefficiency if used together in one application.\nThe loss of performance can be prevented if all the multi-threaded parties are coordinated.\nThis paper describes usage of Intel® Threading Building Blocks (Intel® TBB), an open-source cross-platform library for multi-core parallelism TBB, as the composability layer for Python modules.\nIt helps to unlock additional performance for numeric applications on multi-core systems."
      ],
      "copyright_holder": "Anton Malakhov.",
      "doi": "10.25080/Majora-629e541a-002",
      "authors": "Anton Malakhov",
      "page": {
        "start": 15,
        "stop": 19
      },
      "author_email": ["Anton.Malakhov@intel.com"]
    },
    {
      "author": ["Ben Lasscock"],
      "author_institution": ["Geotrace Technologies"],
      "pages": 7,
      "author_institution_map": {
        "Ben Lasscock": ["Geotrace Technologies"]
      },
      "paper_id": "ben_lasscock",
      "bibliography": "",
      "title": "Generalized earthquake classification",
      "keywords": "machine learning, earthquake, hazard, classification.",
      "video": "https://youtu.be/uT0Nkf0BA7o",
      "abstract": [
        "We characterize the source of an earthquake based on identifying\nthe nodal lines of the radiation pattern it produces. These\ncharacteristics are the mode of failure of the rock (shear or\ntensile), the orientation of the fault plane and direction of\nslip. We will also derive a correlation coefficient comparing the\nsource mechanisms of different earthquakes.  The problem is\nformulated in terms of a simple binary classification on the\nsurface of the sphere. Our design goal was to derive an algorithm\nthat would be both robust to misclassification of the observed data\nand suitable for online processing. We will then go on to derive a\nmapping which translates the learned solution for the separating\nhyper-plane back to the physics of the problem, that is, the\nprobable source type and orientation. For reproducibility, we will\ndemonstrate our algorithm using the example data provided with the\nHASH earthquake classification software, which is available online."
      ],
      "copyright_holder": "Ben Lasscock.",
      "doi": "10.25080/Majora-629e541a-003",
      "authors": "Ben Lasscock",
      "page": {
        "start": 20,
        "stop": 26
      },
      "author_email": ["blasscoc@gmail.com"]
    },
    {
      "author": [
        "Brett Naul",
        "Stéfan van der Walt",
        "Arien Crellin-Quick",
        "Joshua S. Bloom",
        "Fernando Pérez"
      ],
      "author_institution": [
        "University of California, Berkeley",
        "University of California, Berkeley",
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley"
      ],
      "pages": 9,
      "author_institution_map": {
        "Stéfan van der Walt": ["University of California, Berkeley"],
        "Brett Naul": ["University of California, Berkeley"],
        "Arien Crellin-Quick": ["University of California, Berkeley"],
        "Joshua S. Bloom": [
          "Lawrence Berkeley National Laboratory",
          "University of California, Berkeley"
        ],
        "Fernando Pérez": [
          "Lawrence Berkeley National Laboratory",
          "University of California, Berkeley"
        ]
      },
      "paper_id": "brett_naul",
      "bibliography": "mybib",
      "title": "cesium: Open-Source Platform for Time-Series Inference",
      "keywords": "time series, machine learning, reproducible science",
      "video": "https://youtu.be/ZgHGCfwExw0",
      "abstract": [
        "Inference on time series data is a common requirement in many scientific\ndisciplines and internet of things (IoT) applications, yet there are few\nresources available to domain scientists to easily, robustly, and repeatably\nbuild such complex inference workflows: traditional statistical\nmodels of time series are often too rigid to explain complex time domain\nbehavior, while popular machine learning packages require already-featurized\ndataset inputs. Moreover, the software engineering tasks required to\ninstantiate the computational platform are daunting. cesium is an\nend-to-end time series analysis framework, consisting of a Python library as\nwell as a web front-end interface, that allows researchers to featurize raw\ndata and apply modern machine learning techniques in a simple, reproducible,\nand extensible way. Users can apply out-of-the-box feature engineering\nworkflows as well as save and replay their own analyses. Any steps taken in\nthe front end can also be exported to a Jupyter notebook, so users can\niterate between possible models within the front end and then fine-tune their\nanalysis using the additional capabilities of the back-end library. The\nopen-source packages make us of many use modern Python toolkits, including\nxarray, dask, Celery, Flask, and scikit-learn."
      ],
      "copyright_holder": "Brett Naul et al.",
      "doi": "10.25080/Majora-629e541a-004",
      "authors": "Brett Naul, Stéfan van der Walt, Arien Crellin-Quick, Joshua S. Bloom, Fernando Pérez",
      "page": {
        "start": 27,
        "stop": 35
      },
      "author_email": [
        "bnaul@berkeley.edu",
        "stefanv@berkeley.edu",
        "arien@berkeley.edu",
        "joshbloom@berkeley.edu",
        "fperez@lbl.gov"
      ]
    },
    {
      "author": ["Bryan W. Weber", "Chih-Jen Sung"],
      "author_institution": [
        "Mechanical Engineering Department, University of Connecticut, Storrs, CT 06269",
        "Mechanical Engineering Department, University of Connecticut, Storrs, CT 06269"
      ],
      "pages": 9,
      "author_institution_map": {
        "Chih-Jen Sung": [
          "Mechanical Engineering Department, University of Connecticut, Storrs, CT 06269"
        ],
        "Bryan W. Weber": [
          "Mechanical Engineering Department, University of Connecticut, Storrs, CT 06269"
        ]
      },
      "paper_id": "bryan_weber",
      "bibliography": "SciPy-2016",
      "title": "UConnRCMPy: Python-based data analysis for Rapid Compression Machines",
      "keywords": "rapid compression machine, engineering, kinetic models",
      "video": "https://youtu.be/tsjqkIAh8cw",
      "abstract": [
        "The ignition delay of a fuel/air mixture is an important quantity in designing combustion\ndevices, and these data are also used to validate computational kinetic models for combustion.\nOne of the typical experimental devices used to measure the ignition delay is called a Rapid\nCompression Machine (RCM). This paper presents UConnRCMPy, an open-source Python package to\nprocess experimental data from the RCM at the University of Connecticut. Given an experimental\nmeasurement, UConnRCMPy computes the thermodynamic conditions in the reaction chamber of the RCM\nduring an experiment along with the ignition delay. UConnRCMPy relies on several packages from\nthe SciPy stack and the broader scientific Python community. UConnRCMPy implements an extensible\nframework, so that alternative experimental data formats can be incorporated easily. In this\nway, UConnRCMPy improves the consistency of RCM data processing and enables reproducible\nanalysis of the data."
      ],
      "copyright_holder": "Bryan W. Weber et al.",
      "doi": "10.25080/Majora-629e541a-005",
      "authors": "Bryan W. Weber, Chih-Jen Sung",
      "page": {
        "start": 36,
        "stop": 44
      },
      "author_email": ["bryan.w.weber@gmail.com", "chih-jen.sung@uconn.edu"]
    },
    {
      "author": ["Christian Schou Oxvig", "Thomas Arildsen", "Torben Larsen"],
      "author_institution": [
        "Faculty of Engineering and Science, Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark",
        "Faculty of Engineering and Science, Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark",
        "Faculty of Engineering and Science, Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark"
      ],
      "pages": 6,
      "author_institution_map": {
        "Thomas Arildsen": [
          "Faculty of Engineering and Science, Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark"
        ],
        "Torben Larsen": [
          "Faculty of Engineering and Science, Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark"
        ],
        "Christian Schou Oxvig": [
          "Faculty of Engineering and Science, Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark"
        ]
      },
      "paper_id": "christian_oxvig",
      "bibliography": "references",
      "title": "Storing Reproducible Results from Computational Experiments using Scientific Python Packages",
      "keywords": "Reproducibility, Computational Science, HDF5",
      "video": "",
      "abstract": [
        "Computational methods have become a prime branch of modern science. Unfortunately, retractions of papers in high-ranked journals due to erroneous computations as well as a general lack of reproducibility of results have led to a so-called credibility crisis. The answer from the scientific community has been an increased focus on implementing reproducible research in the computational sciences. Researchers and scientists have addressed this increasingly important problem by proposing best practices as well as making available tools for aiding in implementing them. We discuss and give an example of how to implement such best practices using scientific Python packages. Our focus is on how to store the relevant metadata along with the results of a computational experiment. We propose the use of JSON and the HDF5 database and detail a reference implementation in the Magni Python package. Further, we discuss the focuses and purposes of the broad range of available tools for making scientific computations reproducible. We pinpoint the particular use cases that we believe are better solved by storing metadata along with results the same HDF5 database. Storing metadata along with results is important in implementing reproducible research and it is readily achievable using scientific Python packages."
      ],
      "copyright_holder": "Christian Schou Oxvig et al.",
      "doi": "10.25080/Majora-629e541a-006",
      "authors": "Christian Schou Oxvig, Thomas Arildsen, Torben Larsen",
      "page": {
        "start": 45,
        "stop": 50
      },
      "author_email": ["cso@es.aau.dk", "tha@es.aau.dk", "tl@es.aau.dk"]
    },
    {
      "author": [
        "David L. Dotson",
        "Sean L. Seyler",
        "Max Linke",
        "Richard J. Gowers",
        "Oliver Beckstein"
      ],
      "author_institution": [
        "Arizona State University, Tempe, Arizona, USA",
        "Arizona State University, Tempe, Arizona, USA",
        "Max Planck Institut für Biophysik, Frankfurt, Germany",
        "University of Manchester, Manchester, UK",
        "University of Edinburgh, Edinburgh, UK",
        "Arizona State University, Tempe, Arizona, USA"
      ],
      "pages": 6,
      "author_institution_map": {
        "David L. Dotson": ["Arizona State University, Tempe, Arizona, USA"],
        "Oliver Beckstein": ["Arizona State University, Tempe, Arizona, USA"],
        "Richard J. Gowers": [
          "University of Manchester, Manchester, UK",
          "University of Edinburgh, Edinburgh, UK"
        ],
        "Max Linke": ["Max Planck Institut für Biophysik, Frankfurt, Germany"],
        "Sean L. Seyler": ["Arizona State University, Tempe, Arizona, USA"]
      },
      "paper_id": "david_dotson",
      "bibliography": "",
      "title": "datreant: persistent, Pythonic trees for heterogeneous data",
      "keywords": "data management, science, filesystems",
      "video": "https://youtu.be/enLHDZoch0U",
      "abstract": [
        "In science the filesystem often serves as a de facto database, with directory trees being the zeroth-order scientific data structure.\nBut it can be tedious and error prone to work directly with the filesystem to retrieve and store heterogeneous datasets.\ndatreant makes working with directory structures and files Pythonic with Treants: specially marked directories with distinguishing characteristics that can be discovered, queried, and filtered.\nTreants can be manipulated individually and in aggregate, with mechanisms for granular access to the directories and files in their trees.\nDisparate datasets stored in any format (CSV, HDF5, NetCDF, Feather, etc.) scattered throughout a filesystem can thus be manipulated as meta-datasets of Treants.\ndatreant is modular and extensible by design to allow specialized applications to be built on top of it, with MDSynthesis as an example for working with molecular dynamics simulation data. http://datreant.org/"
      ],
      "copyright_holder": "David L. Dotson et al.",
      "doi": "10.25080/Majora-629e541a-007",
      "authors": "David L. Dotson, Sean L. Seyler, Max Linke, Richard J. Gowers, Oliver Beckstein",
      "page": {
        "start": 51,
        "stop": 56
      },
      "author_email": [
        "dldotson@asu.edu",
        "slseyler@asu.edu",
        "max.linke@biophys.mpg.de",
        "richardjgowers@gmail.com",
        "oliver.beckstein@asu.edu"
      ]
    },
    {
      "author": ["David Nicholson"],
      "author_institution": [
        "Emory University, graduate program in Neuroscience, Biology department"
      ],
      "pages": 5,
      "author_institution_map": {
        "David Nicholson": [
          "Emory University, graduate program in Neuroscience, Biology department"
        ]
      },
      "paper_id": "david_nicholson",
      "bibliography": "",
      "title": "Comparison of machine learning methods applied to birdsong element classification",
      "keywords": "machine learning,birdsong,scikit-learn",
      "video": "",
      "abstract": [
        "Songbirds provide neuroscience with a model system for understanding how the brain learns and produces\na motor skill similar to speech. Much like humans, songbirds learn their vocalizations from social\ninteractions during a critical period in development. Each bird’s song consists of repeated elements\nreferred to as “syllables”. To analyze song, scientists label syllables by hand, but a bird can\nproduce hundreds of songs a day, many more than can be labeled. Several groups have applied machine\nlearning algorithms to automate labeling of syllables, but little work has been done comparing these\nvarious algorithms. For example, there are articles that propose using support vector machines (SVM), K-nearest\nneighbors (k-NN), and even deep learning to automate labeling song of the Bengalese Finch (a\nspecies whose behavior has made it the subject of an increasing number of neuroscience studies). This paper\ncompares algorithms for classifying Bengalese Finch syllables (building on previous work\n{[}https://youtu.be/ghgniK4X\\_Js{]}). Using a standard cross-validation approach, classifiers were trained on\nsyllables from a given bird, and then classifier accuracy was measured with large hand-labeled testing datasets for\nthat bird. The results suggest that both k-NN and SVM with a non-linear kernel achieve higher accuracy than\na previously published linear SVM method. Experiments also demonstrate that the accuracy of linear SVM\nis impaired by \\textquotedbl{}intro syllables\\textquotedbl{}, a low-amplitude high-noise syllable found in all Bengalese Finch songs.\nTesting of machine learning algorithms was carried out using Scikit-learn and Numpy/Scipy via Anaconda.\nFigures from this paper in Jupyter notebook form, as well as code and links to data, are here:\nhttps://github.com/NickleDave/ML-comparison-birdsong"
      ],
      "copyright_holder": "David Nicholson.",
      "doi": "10.25080/Majora-629e541a-008",
      "authors": "David Nicholson",
      "page": {
        "start": 57,
        "stop": 61
      },
      "author_email": ["dnicho4@emory.edu"]
    },
    {
      "author": [
        "Jonathon Smith, William Taber, Theodore Drain, Scott Evans,\nJames Evans, Michelle Guevara, William Schulze,\nRichard Sunseri, Hsi-Cheng Wu"
      ],
      "author_institution": [
        "Jet Propulsion Laboratory,\nCalifornia Institute of Technology / NASA"
      ],
      "pages": 7,
      "author_institution_map": {
        "Jonathon Smith, William Taber, Theodore Drain, Scott Evans,\nJames Evans, Michelle Guevara, William Schulze,\nRichard Sunseri, Hsi-Cheng Wu": [
          "Jet Propulsion Laboratory,\nCalifornia Institute of Technology / NASA"
        ]
      },
      "paper_id": "jonathon_smith",
      "bibliography": "",
      "title": "MONTE Python for Deep Space Navigation",
      "keywords": "astrodynamics, aerospace, orbit, trajectory, JPL, NASA",
      "video": "https://youtu.be/E3RhKKpm4TM",
      "abstract": [
        "The Mission Analysis, Operations, and Navigation Toolkit Environment\n(MONTE) is the Jet Propulsion Laboratory's (JPL) signature astrodynamic\ncomputing platform. It was built to support JPL's deep space exploration\nprogram, and has been used to fly robotic spacecraft to Mars, Jupiter,\nSaturn, Ceres, and many solar system small bodies. At its core, MONTE\nconsists of low-level astrodynamic libraries that are written in C++\nand presented to the end user as an importable Python language module.\nThese libraries form the basis on which Python-language applications\nare built for specific astrodynamic applications, such as trajectory\ndesign and optimization, orbit determination, flight path control, and\nmore. The first half of this paper gives context to the MONTE project\nby outlining its history, the field of deep space navigation and where\nMONTE fits into the current Python landscape. The second half gives\nan overview of the main MONTE libraries and provides a narrative\nexample of how it can be used for astrodynamic analysis. For\ninformation on licensing MONTE and getting a copy visit\nmontepy.jpl.nasa.gov or\nemail mdn\\_software@jpl.nasa.gov."
      ],
      "copyright_holder": "California Institute of Technology. Government sponsorship acknowledged.",
      "doi": "10.25080/Majora-629e541a-009",
      "authors": "Jonathon Smith, William Taber, Theodore Drain, Scott Evans,\nJames Evans, Michelle Guevara, William Schulze,\nRichard Sunseri, Hsi-Cheng Wu",
      "page": {
        "start": 62,
        "stop": 68
      },
      "author_email": ["jonathon.j.smith@jpl.nasa.gov"]
    },
    {
      "author": ["Joy Merwin Monteiro", "Rodrigo Caballero"],
      "author_institution": [
        "MISU, Stockholm University",
        "MISU, Stockholm University"
      ],
      "pages": 6,
      "author_institution_map": {
        "Joy Merwin Monteiro": ["MISU, Stockholm University"],
        "Rodrigo Caballero": ["MISU, Stockholm University"]
      },
      "paper_id": "joy_monteiro",
      "bibliography": "climt",
      "title": "The Climate Modelling Toolkit",
      "keywords": "Climate Modelling, Hierarchical Models",
      "video": "",
      "abstract": [
        "The Climate Modelling Toolkit (CliMT) is a Python-based software component toolkit providing a\nflexible problem-solving environment for climate science problems. It aims to simplify the\ndevelopment of models of complexity 'appropriate' to the scientific question at hand. This aim\nis achieved by providing Python-level access to components commonly used in climate models (such\nas radiative transfer models and dynamical cores) and using the expressive data structures\navailable in Python to access and combine these components. This paper describes the motivation behind\ndeveloping CliMT, and serves as an introduction to interested users and developers."
      ],
      "copyright_holder": "Joy Merwin Monteiro et al.",
      "doi": "10.25080/Majora-629e541a-00a",
      "authors": "Joy Merwin Monteiro, Rodrigo Caballero",
      "page": {
        "start": 69,
        "stop": 74
      },
      "author_email": ["joy.merwin@gmail.com", "rodrigo@misu.su.se"]
    },
    {
      "author": ["Juan Shishido", "Jaya Narasimhan", "Matar Haller"],
      "author_institution": [
        "School of Information, University of California, Berkeley",
        "Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "Helen Wills Neuroscience Institute, University of California, Berkeley"
      ],
      "pages": 7,
      "author_institution_map": {
        "Jaya Narasimhan": [
          "Department of Electrical Engineering and Computer Science, University of California, Berkeley"
        ],
        "Juan Shishido": [
          "School of Information, University of California, Berkeley"
        ],
        "Matar Haller": [
          "Helen Wills Neuroscience Institute, University of California, Berkeley"
        ]
      },
      "paper_id": "juan_shishido",
      "bibliography": "",
      "title": "Tell Me Something I Don't Know: Analyzing OkCupid Profiles",
      "keywords": "natural language processing, machine learning, supervised learning,\nunsupervised learning, topic modeling, okcupid, online dating",
      "video": "https://youtu.be/dtgmMj8W298",
      "abstract": [
        "In this paper, we present an analysis of 59,000 OkCupid user profiles that\nexamines online self-presentation by combining natural language processing\n(NLP) with machine learning. We analyze word usage patterns by self-reported\nsex and drug usage status. In doing so, we review standard NLP techniques,\ncover several ways to represent text data, and explain topic modeling. We find\nthat individuals in particular demographic groups self-present in consistent\nways. Our results also suggest that users may unintentionally reveal\ndemographic attributes in their online profiles."
      ],
      "copyright_holder": "Juan Shishido et al.",
      "doi": "10.25080/Majora-629e541a-00b",
      "authors": "Juan Shishido, Jaya Narasimhan, Matar Haller",
      "page": {
        "start": 75,
        "stop": 81
      },
      "author_email": [
        "juanshishido@berkeley.edu",
        "jnaras@berkeley.edu",
        "matar@berkeley.edu"
      ]
    },
    {
      "author": ["Kyle E. Niemeyer"],
      "author_institution": [
        "School of Mechanical, Industrial, and Manufacturing Engineering, Oregon State University"
      ],
      "pages": 8,
      "author_institution_map": {
        "Kyle E. Niemeyer": [
          "School of Mechanical, Industrial, and Manufacturing Engineering, Oregon State University"
        ]
      },
      "paper_id": "kyle_niemeyer",
      "bibliography": "",
      "title": "PyTeCK: a Python-based automatic testing package for chemical kinetic models",
      "keywords": "combustion, chemical kinetics, model validation",
      "video": "https://youtu.be/Ke3Ip25C4pY",
      "abstract": [
        "Combustion simulations require detailed chemical kinetic models to predict\nfuel oxidation, heat release, and pollutant emissions. These models are\ntypically validated using qualitative rather than quantitative comparisons\nwith limited sets of experimental data. This work introduces PyTeCK, an\nopen-source Python-based package for automatic testing of chemical kinetic\nmodels. Given a model of interest, PyTeCK automatically parses experimental\ndatasets encoded in a YAML format, validates the self-consistency of each\ndataset, and performs simulations for each experimental data point. It then\nreports a quantitative metric of the model's performance, based on the\ndiscrepancy between experimental and simulated values and weighted by\nexperimental variance. The initial version of PyTeCK supports shock tube\nand rapid compression machine experiments that measure autoignition delay."
      ],
      "copyright_holder": "Kyle E. Niemeyer.",
      "doi": "10.25080/Majora-629e541a-00c",
      "authors": "Kyle E. Niemeyer",
      "page": {
        "start": 82,
        "stop": 89
      },
      "author_email": ["Kyle.Niemeyer@oregonstate.edu"]
    },
    {
      "author": ["Michael D. Pacer", "Jordan W. Suchow"],
      "author_institution": [
        "University of California, Berkeley",
        "University of California, Berkeley"
      ],
      "pages": 8,
      "author_institution_map": {
        "Michael D. Pacer": ["University of California, Berkeley"],
        "Jordan W. Suchow": ["University of California, Berkeley"]
      },
      "paper_id": "mikejordan_pacersuchow",
      "bibliography": "mybib",
      "title": "Linting science prose and the science of prose linting",
      "keywords": "linters, writing tools, copyediting",
      "video": "https://youtu.be/S55EFUOu4O0",
      "abstract": [
        "The craft of writing is hard despite the abundance of thoughtful advice available in usage guides and other sources. This is partly a problem of medium: amassing advice is not enough to improve writing. Writing would thus benefit if our collective knowledge about best practices in writing were extracted and transformed into a medium that makes the knowledge more accessible to authors.",
        "We built Proselint, a Python-based linter for English prose that identifies violations of style and usage guidelines. Proselint is open-source software released under the BSD license and is compatible with Pythons 2 and 3. It runs as a command-line utility or as a text-editor plugin. Proselint's modules address redundancy, jargon, illogic, clichés, unidiomatic vocabulary, sexism, inconsistency, misuse of symbols, malapropisms, oxymorons, security gaffes, hedging, apologizing, and pretension. Furthermore, Proselint is extensible, enabling creation of domain-specific modules and implementation of house style guides.",
        "Proselint can be seen as both a language tool for scientists and a tool for language science. On the one hand, Proselint can help scientists communicate their ideas to each other and to the public by improving their writing. On the other hand, scientists can use Proselint to measure language usage, to provide style- and usage-based features for tasks such as authorship identification, and to explore the factors that make a linter useful (e.g., a low false discovery rate)."
      ],
      "copyright_holder": "Michael D. Pacer et al.",
      "doi": "10.25080/Majora-629e541a-00d",
      "authors": "Michael D. Pacer, Jordan W. Suchow",
      "page": {
        "start": 90,
        "stop": 97
      },
      "author_email": ["mpacer@berkeley.edu"]
    },
    {
      "author": [
        "Richard J. Gowers",
        "Max Linke",
        "Jonathan Barnoud",
        "Tyler J. E. Reddy",
        "Manuel N. Melo",
        "Sean L. Seyler",
        "Jan Domański",
        "David L. Dotson",
        "Sébastien Buchoux",
        "Ian M. Kenney",
        "Oliver Beckstein"
      ],
      "author_institution": [
        "University of Manchester, Manchester, UK",
        "University of Edinburgh, Edinburgh, UK",
        "Max Planck Institut für Biophysik, Frankfurt, Germany",
        "University of Groningen, Groningen, The Netherlands",
        "University of Oxford, Oxford, UK",
        "University of Groningen, Groningen, The Netherlands",
        "Arizona State University, Tempe, Arizona, USA",
        "University of Oxford, Oxford, UK",
        "Arizona State University, Tempe, Arizona, USA",
        "Université de Picardie Jules Verne, Amiens, France",
        "Arizona State University, Tempe, Arizona, USA",
        "Arizona State University, Tempe, Arizona, USA"
      ],
      "pages": 8,
      "author_institution_map": {
        "Jan Domański": ["University of Oxford, Oxford, UK"],
        "Sébastien Buchoux": [
          "Université de Picardie Jules Verne, Amiens, France"
        ],
        "Jonathan Barnoud": [
          "University of Groningen, Groningen, The Netherlands"
        ],
        "Richard J. Gowers": [
          "University of Manchester, Manchester, UK",
          "University of Edinburgh, Edinburgh, UK"
        ],
        "Ian M. Kenney": ["Arizona State University, Tempe, Arizona, USA"],
        "Tyler J. E. Reddy": ["University of Oxford, Oxford, UK"],
        "David L. Dotson": ["Arizona State University, Tempe, Arizona, USA"],
        "Oliver Beckstein": ["Arizona State University, Tempe, Arizona, USA"],
        "Manuel N. Melo": [
          "University of Groningen, Groningen, The Netherlands"
        ],
        "Max Linke": ["Max Planck Institut für Biophysik, Frankfurt, Germany"],
        "Sean L. Seyler": ["Arizona State University, Tempe, Arizona, USA"]
      },
      "paper_id": "oliver_beckstein",
      "bibliography": "mdanalysis",
      "title": "MDAnalysis: A Python Package for the Rapid Analysis of Molecular Dynamics Simulations",
      "keywords": "molecular dynamics simulations, science, chemistry, physics, biology",
      "video": "https://youtu.be/zVQGFysYDew",
      "abstract": [
        "MDAnalysis (http://mdanalysis.org) is a library for structural and temporal analysis of molecular dynamics (MD) simulation trajectories and individual protein structures.\nMD simulations of biological molecules have become an important tool to elucidate the relationship between molecular structure and physiological function.\nSimulations are performed with highly optimized software packages on HPC resources but most codes generate output trajectories in their own formats so that the development of new trajectory analysis algorithms is confined to specific user communities and widespread adoption and further development is delayed.\nMDAnalysis addresses this problem by abstracting access to the raw simulation data and presenting a uniform object-oriented Python interface to the user.\nIt thus enables users to rapidly write code that is portable and immediately usable in virtually all biomolecular simulation communities.\nThe user interface and modular design work equally well in complex scripted work flows, as foundations for other packages, and for interactive and rapid prototyping work in IPython  / Jupyter notebooks, especially together with molecular visualization provided by nglview and time series analysis with pandas.\nMDAnalysis is written in Python and Cython and uses NumPy arrays for easy interoperability with the wider scientific Python ecosystem.\nIt is widely used and forms the foundation for more specialized biomolecular simulation tools.\nMDAnalysis is available under the GNU General Public License v2."
      ],
      "copyright_holder": "Richard J. Gowers et al.",
      "doi": "10.25080/Majora-629e541a-00e",
      "authors": "Richard J. Gowers, Max Linke, Jonathan Barnoud, Tyler J. E. Reddy, Manuel N. Melo, Sean L. Seyler, Jan Domański, David L. Dotson, Sébastien Buchoux, Ian M. Kenney, Oliver Beckstein",
      "page": {
        "start": 98,
        "stop": 105
      },
      "author_email": [
        "richardjgowers@gmail.com",
        "max.linke@biophys.mpg.de",
        "j.barnoud@rug.nl",
        "tyler.reddy@bioch.ox.ac.uk",
        "m.n.melo@rug.nl",
        "slseyler@asu.edu",
        "jan.domanski@bioch.ox.ac.uk",
        "dldotson@asu.edu",
        "sebastien.buchoux@u-picardie.fr",
        "Ian.Kenney@asu.edu",
        "oliver.beckstein@asu.edu"
      ]
    },
    {
      "author": [
        "Patrick Steffen Pedersen",
        "Christian Schou Oxvig",
        "Jan Østergaard",
        "Torben Larsen"
      ],
      "author_institution": [
        "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark",
        "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark",
        "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark",
        "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark"
      ],
      "pages": 8,
      "author_institution_map": {
        "Patrick Steffen Pedersen": [
          "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark"
        ],
        "Torben Larsen": [
          "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark"
        ],
        "Jan Østergaard": [
          "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark"
        ],
        "Christian Schou Oxvig": [
          "Faculty of Engineering and Science, Department of Electronic\nSystems, Section of Signal and Information Processing, Aalborg\nUniversity, 9220 Aalborg, Denmark"
        ]
      },
      "paper_id": "patrick_pedersen",
      "bibliography": "references",
      "title": "Validating Function Arguments in Python Signal Processing Applications",
      "keywords": "Function Argument Validation, Application-driven Data Types, Signal\nProcessing, Computational Science",
      "video": "",
      "abstract": [
        "Python does not have a built-in mechanism to validate the value of function\narguments. This can lead to nonsensical exceptions, unexpected behaviour,\nerroneous results and the like. In the present paper, we define the concept\nof so-called application-driven data types which place a layer of\nabstraction on top of Python data types. With this concept in mind, we\ndiscuss the current argument validation solutions of PyDBC, Traitlets and\nNumtraits, MyPy, PyValid, and PyContracts. We find that they share the issue\nof expressing the validation scheme in terms of Python objects rather than\nin terms of the data they hold. Consequently, we lay out a suggestion for a\nvalidation strategy including what qualifies as a validation scheme, how to\ncreate an interface which promotes both usability and readability, and which\nPython constructs to encourage using for validation encapsulation. A\nreference implementation of the suggested validation strategy is part of the\nopen-source Python package, Magni which is thus presented along with a\nnumber of examples of the usages of this package."
      ],
      "copyright_holder": "Patrick Steffen Pedersen et al.",
      "doi": "10.25080/Majora-629e541a-00f",
      "authors": "Patrick Steffen Pedersen, Christian Schou Oxvig, Jan Østergaard, Torben Larsen",
      "page": {
        "start": 106,
        "stop": 113
      },
      "author_email": [
        "psp@es.aau.dk",
        "cso@es.aau.dk",
        "jo@es.aau.dk",
        "tola@adm.aau.dk"
      ]
    },
    {
      "author": ["Prabhu Ramachandran"],
      "author_institution": [
        "Department of Aerospace Engineering",
        "IIT Bombay, Mumbai, India"
      ],
      "pages": 8,
      "author_institution_map": {
        "Prabhu Ramachandran": [
          "Department of Aerospace Engineering",
          "IIT Bombay, Mumbai, India"
        ]
      },
      "paper_id": "prabhu_ramachandran_fossee",
      "bibliography": "",
      "title": "Spreading the Adoption of Python in India: the FOSSEE Python Project",
      "keywords": "",
      "video": "https://youtu.be/6UnuPhTPdnM",
      "abstract": [
        "The FOSSEE (Free Open Source Software for Science and Engineering\nEducation) project (http://fossee.in) is funded by the Ministry of Human\nResources and Development, MHRD, (http://mhrd.gov.in) of the Government of\nIndia.  The FOSSEE project is based out of IIT Bombay and the goal of the\nproject is to eliminate the use of proprietary tools in the college\ncurriculum.  FOSSEE promotes various open source packages.  Python is one\nof them.",
        "In this paper, the Python-related activities and initiatives of FOSSEE are\ndiscussed.  The group focuses on promoting the use of Python in the\ncollege curriculum.  The important activities of this group include the\ncreation of spoken-tutorials on Python, the creation of 400+ IPython-based\ntextbook companions, an online testing tool for a variety of programming\nlanguages, a course akin to software carpentry at IIT Bombay, the\norganization of the SciPy India conference, and finally spreading the\nadoption of Python in schools and colleges.  The paper discusses how these\ntools may be used to teach Python in the context of collegiate education\nand computational science."
      ],
      "copyright_holder": "Prabhu Ramachandran.",
      "doi": "10.25080/Majora-629e541a-010",
      "authors": "Prabhu Ramachandran",
      "page": {
        "start": 114,
        "stop": 121
      },
      "author_email": ["prabhu@aero.iitb.ac.in"]
    },
    {
      "author": ["Prabhu Ramachandran"],
      "author_institution": [
        "Department of Aerospace Engineering",
        "IIT Bombay, Mumbai, India"
      ],
      "pages": 8,
      "author_institution_map": {
        "Prabhu Ramachandran": [
          "Department of Aerospace Engineering",
          "IIT Bombay, Mumbai, India"
        ]
      },
      "paper_id": "prabhu_ramachandran_pysph",
      "bibliography": "references",
      "title": "PySPH: a reproducible and high-performance framework for smoothed particle hydrodynamics",
      "keywords": "",
      "video": "https://youtu.be/6UnuPhTPdnM",
      "abstract": [
        "Smoothed Particle Hydrodynamics (SPH) is a general purpose technique to\nnumerically compute the solutions to partial differential equations such\nas those used to simulate fluid and solid mechanics.  The method is\ngrid-free and uses particles to discretize the various properties of\ninterest (such as density, fluid velocity, pressure etc.).  The method is\nLagrangian and particles are moved with the local velocity.",
        "PySPH is an open source framework for Smoothed Particle Hydrodynamics.  It\nis implemented in a mix of Python and Cython.  It is designed to be easy\nto use on multiple platforms, high-performance and support parallel\nexecution.  Users write pure-Python code and HPC code is generated on the\nfly, compiled, and executed.  PySPH supports OpenMP and MPI for\ndistributed computing, in a way that is transparent to the user.  PySPH is\nalso designed to make it easy to perform reproducible research.  In this\npaper we discuss the design and implementation of PySPH."
      ],
      "copyright_holder": "Prabhu Ramachandran.",
      "doi": "10.25080/Majora-629e541a-011",
      "authors": "Prabhu Ramachandran",
      "page": {
        "start": 122,
        "stop": 129
      },
      "author_email": ["prabhu@aero.iitb.ac.in"]
    },
    {
      "author": [
        "Sebastian Benthall",
        "Travis Pinney",
        "JC Herz",
        "Kit Plummer"
      ],
      "author_institution": [
        "Ion Channel, ionchannel.io",
        "UC Berkeley School of Information",
        "Ion Channel, ionchannel.io",
        "travis.pinney@ionchannel.io",
        "jc.herz@ionchannel.io",
        "Ion Channel, ionchannel.io",
        "kit.plummer@ionchannel.io",
        "Ion Channel, ionchannel.io"
      ],
      "pages": 7,
      "author_institution_map": {
        "Sebastian Benthall": [
          "Ion Channel, ionchannel.io",
          "UC Berkeley School of Information"
        ],
        "Kit Plummer": [
          "kit.plummer@ionchannel.io",
          "Ion Channel, ionchannel.io"
        ],
        "JC Herz": ["jc.herz@ionchannel.io", "Ion Channel, ionchannel.io"],
        "Travis Pinney": [
          "Ion Channel, ionchannel.io",
          "travis.pinney@ionchannel.io"
        ]
      },
      "paper_id": "sebastian_benthall",
      "bibliography": "",
      "title": "An Ecological Approach to Software Supply Chain Risk Management",
      "keywords": "risk management, software dependencies, complex networks, software vulnerabilities, software security",
      "video": "https://youtu.be/6UnuPhTPdnM",
      "abstract": [
        "We approach the problem of software assurance in a novel way inspired\nby an analytic framework used in natural hazard risk mitigation.\nExisting approaches to software assurance focus on\nevaluating individual software projects in isolation.\nWe demonstrate a technique that evaluates an entire ecosystem of software\nprojects, taking into account the dependencey structure between packages.\nOur model analytically separates vulnerability and exposure as elements of\nsoftware risk, then makes minimal assumptions about the propagation of these values\nthrough a software supply chain.\nCombined with data collected from package management systems, our model\nindicates \\textquotedbl{}hot spots\\textquotedbl{} in the ecosystem of higher expected risk.\nWe demonstrate this model using data collected from the Python Package Index (PyPI).\nOur results suggest that Zope and Plone related projects carry the highest risk of\nall PyPI packages because they are widely used and their core libraries\nare no longer maintained."
      ],
      "copyright_holder": "Sebastian Benthall et al.",
      "doi": "10.25080/Majora-629e541a-012",
      "authors": "Sebastian Benthall, Travis Pinney, JC Herz, Kit Plummer",
      "page": {
        "start": 130,
        "stop": 136
      },
      "author_email": [
        "sb@ischool.berkeley.edu",
        "travis.pinney@ionchannel.io",
        "jc.herz@ionchannel.io",
        "kit.plummer@ionchannel.io"
      ]
    },
    {
      "author": ["Yu Feng", "Nick Hand"],
      "author_institution": [
        "Berkeley Center for Cosmological Physics, University of California, Berkeley CA 94720",
        "Berkeley Institute for Data Science, University of California, Berkeley CA 94720",
        "Berkeley Center for Cosmological Physics, University of California, Berkeley CA 94720"
      ],
      "pages": 7,
      "author_institution_map": {
        "Yu Feng": [
          "Berkeley Center for Cosmological Physics, University of California, Berkeley CA 94720",
          "Berkeley Institute for Data Science, University of California, Berkeley CA 94720"
        ],
        "Nick Hand": [
          "Berkeley Center for Cosmological Physics, University of California, Berkeley CA 94720"
        ]
      },
      "paper_id": "yu_feng",
      "bibliography": "mybib",
      "title": "Launching Python Applications on Peta-scale Massively Parallel Systems",
      "keywords": "Python, high performance computing, development environment, application",
      "video": "https://youtu.be/CfrRDI71vTc",
      "abstract": [
        "We introduce a method to launch Python applications at near native speed on\nlarge high performance computing systems.  The Python run-time and other\ndependencies are bundled and delivered to computing nodes via a broadcast\noperation. The interpreter is instructed to use the local version of the files\non the computing node, removing the shared file system as a bottleneck during\nthe application start-up.  Our method can be added as a preamble to the\ntraditional job script, improving the performance of user applications in a\nnon-invasive way. Furthermore, we find it useful to implement a three-tier\nsystem for the supporting components of an application, reducing the overhead\nof runs during the development phase of an application. The method launches\napplications on Cray XC30 and Cray XT systems up to full machine capacity\nwith an overhead of typically less than 2 minutes. We expect the method to be\nportable to similar applications in Julia or R. We also hope the three-tier\nsystem for the supporting components provides some insight for the container\nbased solutions for launching applications in a development environment. We\nprovide the full source code of an implementation of the method at\nhttps://github.com/rainwoodman/python-mpi-bcast. Now that large scale\nPython applications can launch extremely efficiently on state-of-the-art\nsuper-computing systems, it is time for the high performance computing\ncommunity to seriously consider building complicated computational applications\nat large scale with Python."
      ],
      "copyright_holder": "Yu Feng et al.",
      "doi": "10.25080/Majora-629e541a-013",
      "authors": "Yu Feng, Nick Hand",
      "page": {
        "start": 137,
        "stop": 143
      },
      "author_email": ["yfeng1@berkeley.edu", "nhand@berkeley.edu"]
    }
  ]
}
