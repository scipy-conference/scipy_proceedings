<doi_batch xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.2" xmlns="http://www.crossref.org/schema/4.4.2" xsi:schemaLocation="http://www.crossref.org/schema/4.4.2 http://www.crossref.org/schemas/crossref4.4.2.xsd"><head><doi_batch_id>majora.1b6fd038-17b0328487e</doi_batch_id><timestamp>1627867176</timestamp><depositor><depositor_name>Dillon Niederhut</depositor_name><email_address>dillon.niederhut@gmail.com</email_address></depositor><registrant>Crossref</registrant></head><body><database><database_metadata language="en"><contributors><person_name contributor_role="editor" sequence="first"><given_name>Dillon</given_name><surname>Niederhut</surname></person_name></contributors><titles><title>Proceedings of the Python in Science Conference</title></titles></database_metadata><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Jyotika</given_name><surname>Singh</surname><ORCID>https://orcid.org/0000-0002-5442-3004</ORCID></person_name></contributors><titles><title>Social Media Analysis using Natural Language Processing Techniques</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Demonstration of social media noise and cleaning methods, followed by trend analysis on YouTube with NLP and statistics using pyYouTubeAnalysis.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-015</doi><resource>https://zenodo.org/record/5151865</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Sarthak</given_name><surname>Jariwala</surname><ORCID>https://orcid.org/0000-0002-7020-9008</ORCID></person_name></contributors><titles><title>seaborn-image : image data visualization in Python</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>High level API for attractive and descriptive image visualization in Python built on top of matplotlib</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-016</doi><resource>https://zenodo.org/record/5151891</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Samantha</given_name><surname>Walkow</surname><ORCID>https://orcid.org/0000-0001-7329-1863</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>Chris</given_name><surname>Havlin</surname><ORCID>https://orcid.org/0000-0003-0585-8236</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>Matthew</given_name><surname>Turk</surname><ORCID>https://orcid.org/0000-0002-5294-0198</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>Corentin</given_name><surname>Cadiou</surname><ORCID>https://orcid.org/0000-0003-2285-0332</ORCID></person_name></contributors><titles><title>Towards a Scientific Workflow Description: a yt Project Prototype for Interdisciplinary Analysis</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Scientific workflow description provides an alternative to the cognitive overhead of learning a new software package and use of imperative programming paradigms often used with python. This description is encoded in a JSON schema, accessed by the user through a configuration file, and run using python modules that attach the configuration file to the code which produces output. We use yt, an computational astrophyics tool, to demonstrate how a domain specific software can operate within a descriptive framework.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-017</doi><resource>https://zenodo.org/record/5151905</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Mark</given_name><surname>Thoren</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Cristina</given_name><surname>Suteu</surname></person_name></contributors><titles><title>Using Python for Analysis and Verification of Mixed-mode Signal Chains for Analog Signal Acquisition</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Accurate, precise, and low-noise sensor measurements are essential before any machine can learn about (or artificial-intelligently make decisions about) the physical world. Modern, highly integrated signal acquisition devices can perform analog signal conditioning, digitization, and digital filtering on a single silicon device, greatly simplifying system electronics. However, a complete understanding of the signal chain properties is still required to correctly utilize and debug these modern devices.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-018</doi><resource>https://zenodo.org/record/5151918</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Edis</given_name><surname>Jakupovic</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Oliver</given_name><surname>Beckstein</surname></person_name></contributors><titles><title>Speeding Up Molecular Dynamics Trajectory Analysis with MPI Parallelization</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description></description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-019</doi><resource>https://zenodo.org/record/5151924</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Jyotika</given_name><surname>Singh</surname><ORCID>https://orcid.org/0000-0002-5442-3004</ORCID></person_name></contributors><titles><title>Social Media Analysis using Natural Language Processing Techniques</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Social media is very popularly used every day with daily content viewing and/or posting that in turn influences people around this world in a variety of ways. Social media platforms, such as YouTube, have a lot of activity that goes on every day in terms of video posting, watching and commenting. While we can open the YouTube app on our phones and look at videos and what people are commenting, it only gives us a limited view as to kind of things others around us care about and what is trending amongst other consumers of our favorite topics or videos. Crawling some of this raw data and performing analysis on it using Natural Language Processing (NLP) can be tricky given the different styles of language usage by people in today&#8217;s world. This effort highlights the YouTube&#8217;s open Data API and how to use it in python to get the raw data, data cleaning using NLP tricks and Machine Learning in python for social media interactions, and extraction of trends and key influential factors from this data in an automated fashion using pyYouTubeAnalysis.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-01a</doi><resource>https://zenodo.org/record/5151932</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Amanda</given_name><surname>Kraft</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Matthew</given_name><surname>Widjaja</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Trevor</given_name><surname>Sands</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Brad</given_name><surname>Galego</surname></person_name></contributors><titles><title>Programmatically Identifying Cognitive Biases Present in Software Development</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Mitigating bias in AI-enabled systems is a topic of great concern within the research community. We began developing an approach to identify a subset of cognitive biases that may be present in development artifacts (e.g., version control commit messages): anchoring bias, availability bias, confirmation bias, and hyperbolic discounting. We developed multiple natural language processing (NLP) models to identify and classify the presence of bias in text originating from software development artifacts.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-01b</doi><resource>https://zenodo.org/record/5151947</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Tetsuo</given_name><surname>Koyama</surname></person_name></contributors><titles><title>Visualize 3D scientific data in a Pythonic way like matplotlib</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Do you want to visualize 3D scientific data in a Pythonic way like matplotlib? If you want, this poster is for you. This poster is the introduction of PyVista.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-01c</doi><resource>https://zenodo.org/record/5151953</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Roni</given_name><surname>Kobrosly</surname></person_name></contributors><titles><title>causal-curve: tools to perform causal inference given a continuous treatment</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>There are a multitude of scenarios in both research and industry where this would be useful to evaluate the impact of a continuous &#8220;treatment&#8221; on an outcome of interest in a causal inference framework. Unfortunately, we are not aware of an established python package that is able to perform this. The `causal-curve` package attempts to fill that gap, providing users with tools to generate causal dose-response curves (AKA causal curves).</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-01d</doi><resource>https://zenodo.org/record/5151967</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Samuel</given_name><surname>Wallan</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Dominic</given_name><surname>Chmiel</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Matt</given_name><surname>Haberland</surname><ORCID>https://orcid.org/0000-0003-4806-3601</ORCID></person_name></contributors><titles><title>SciPy 2021: An Accurate Implementation of the Studentized Range Distribution for Python</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>As data becomes more and more accessible, it can be tempting to misuse data analysis techniques to find statistically significant results, a practice known as 'p-hacking'. Tukey's HSD (Honestly Significant Difference) test is one of several tests that guards against this practice by using the studentized range distribution to compute p-values that account for the number of comparisons performed. Implementations of Tukey's HSD already exist within the scientific Python ecosystem, but they rely on approximations of the studentized range distribution that may not behave well outside of their intended range and, even within the intended range, are only accurate to a few digits. In this document, we present a fast, highly accurate, and direct implementation of the studentized range distribution for SciPy, and we demonstrate its speed and accuracy.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-01e</doi><resource>https://zenodo.org/record/5151976</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Varun</given_name><surname>Kapoor</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Claudia</given_name><surname>Carabana</surname></person_name></contributors><titles><title>Cell Tracking in 3D using Deep Learning Segmentations</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Live-cell imaging is highly used technique to study cell migration and dynamics over tile. Automated analysis of florescently membrane-labelled cells can be highly challenging due to their irregular shape, variability in size and dynamic movement across Z planes making it difficult to detect and track them. Ze introduce a detailed analysis pipeline to perform segmentation with accurate shape information, combined with BTrackmate, a customized codebase of popular ImageJ/Fiji software Trackmate, to perform cell tracking inside the tissue of interest. We also created an interface in Napari to visualize the tracks along a chosen view making it possible to follow a cell along the plane of motion. We provide a detailed protocaol to implement this pipeline in a new dataset, together with the required Jupyter notebooks.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-01f</doi><resource>https://zenodo.org/record/5151980</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Adam</given_name><surname>Theisen</surname><ORCID>https://orcid.org/0000-0002-7602-1057</ORCID></person_name></contributors><titles><title>It's Time for the Atmospheric Science Community to ACT Together</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>The Atmospheric data Community Toolkit (ACT) is an open-source Python library for working with n-dimensional atmospheric time-series datasets.  ACT contains functions for every aspect of the research lifecycle.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-020</doi><resource>https://zenodo.org/record/5151986</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Predrag</given_name><surname>Gruevski</surname><ORCID>https://orcid.org/0000-0003-2713-6621</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>Colin</given_name><surname>Carroll</surname><ORCID>https://orcid.org/0000-0001-6977-0861</ORCID></person_name></contributors><titles><title>Adopting static typing in scientific projects</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Are you interested in adding typing to your existing codebase, but are not sure how to get started? Are you worried about managing the typing process without pausing your project's development?

In this talk, we'll embrace the fact that a large project's transition toward typing will likely happen over the course of many months, concurrently with ongoing development. However, that doesn't mean that getting started with typing has to be difficult! We'll share with you two examples of adopting typing in existing open-source codebases (100k and 40k lines of Python). We'll particularly focus on the typing experience from the perspective of project maintainers, contributors, and users of these Python libraries.

We will discuss useful tools and strategies, surprising difficulties, the types of bugs and errors we found, and how the addition of typing changes the overall development experience. By the end of this talk, you'll be able to confidently manage the migration toward typing in your own codebase.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-021</doi><resource>https://zenodo.org/record/5151994</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Gregory</given_name><surname>Lee</surname><ORCID>https://orcid.org/0000-0001-8895-2740</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>Gigon</given_name><surname>Bae</surname><ORCID>https://orcid.org/0000-0003-0648-9735</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>Benjamin</given_name><surname>Zaitlen</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>John</given_name><surname>Kirkham</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Rahul</given_name><surname>Choudhury</surname></person_name></contributors><titles><title>cuCIM - A GPU image I/O and processing library</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>A presentation introducing RAPIDS cuCIM, a library for image I/O and processing on GPUs</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-022</doi><resource>https://zenodo.org/record/5151998</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Matthew</given_name><surname>Feickert</surname><ORCID>https://orcid.org/0000-0003-4124-7862</ORCID></person_name></contributors><titles><title>Distributed statistical inference with pyhf powered by funcX</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>In high energy physics (HEP) a core component of analysis of data collected at the Large Hadron Collider is performing statistical inference for binned models to extract physics information. The statistical fitting tools used in HEP have traditionally been implemented in C++, but in recent years pyhf, a pure-Python library with automatic differentiation and hardware acceleration, has grown in use for analysis related statistical inference problems. The fitting of multiple different hypotheses for new physics signatures (signals) is a computational problem that lends itself easily to parallelization, but is hampered on HPC environments by the additional tooling overhead required, which can be very difficult to master. Through use of funcX, a pure-Python high performance function serving system designed to orchestrate scientific workloads across heterogeneous computing resources, pyhf can be used as a highly scalable (fitting) function as a service (FaaS) on HPCs.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-023</doi><resource>https://zenodo.org/record/5152006</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Jim</given_name><surname>Pivarski</surname><ORCID>https://orcid.org/0000-0002-6649-343X</ORCID></person_name></contributors><titles><title>Awkward Array</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Tools update on Awkward Array.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-024</doi><resource>https://zenodo.org/record/5152018</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Elliott</given_name><surname>de Andrade</surname><ORCID>https://orcid.org/0000-0001-7310-8942</ORCID></person_name></contributors><titles><title>SciPy Tools Plenary on Matplotlib</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. This presentation summarizes changes over the past year, new features, and future plans.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-025</doi><resource>https://zenodo.org/record/5152026</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Inessa</given_name><surname>Pawson</surname><ORCID>https://orcid.org/0000-0001-7294-122X</ORCID></person_name></contributors><titles><title>NumPy &#8211; Annual Update</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Presentation about the highlights and milestones of the NumPy project in 2020-2021</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-026</doi><resource>https://zenodo.org/record/5152034</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Isabela</given_name><surname>Presedo-Floyd</surname></person_name><person_name contributor_role="author" sequence="additional"><given_name>Matthias</given_name><surname>Bussonnier</surname><ORCID>https://orcid.org/0000-0002-7636-8632</ORCID></person_name></contributors><titles><title>SciPy Tools Plenary: Jupyter Updates</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Project Jupyter creates open source software, standards, and services for interactive computing. This presentation covers recent milestones and ideas for people to contribute across the Jupyter ecosystem.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-027</doi><resource>https://zenodo.org/record/5152036</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>K.</given_name><surname>Millman</surname><ORCID>https://orcid.org/0000-0002-5263-5070</ORCID></person_name><person_name contributor_role="author" sequence="additional"><given_name>St&#233;fan</given_name><surname>van der Walt</surname><ORCID>https://orcid.org/0000-0001-9276-1891</ORCID></person_name></contributors><titles><title>Scientific Python Ecosystem Coordination</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>Planning for the Next Decade of Scientific Python: outline of first phase</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-028</doi><resource>https://zenodo.org/record/5152046</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Pamphile</given_name><surname>Roy</surname><ORCID>https://orcid.org/0000-0001-9816-1416</ORCID></person_name></contributors><titles><title>SciPy: SciPy 2021 Tools Track</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>2021 updates and outlooks in SciPy</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-029</doi><resource>https://zenodo.org/record/5152050</resource></doi_data></dataset><dataset dataset_type="other"><contributors><person_name contributor_role="author" sequence="first"><given_name>Gregory</given_name><surname>Lee</surname><ORCID>https://orcid.org/0000-0001-8895-2740</ORCID></person_name></contributors><titles><title>SciPy Tools Plenary: scikit-image annual update</title></titles><database_date><publication_date><year>2021</year></publication_date></database_date><description>A brief update on recent improvements and future plans for scikit-image.</description><program name="relations" xmlns="http://www.crossref.org/relations.xsd"><related_item><inter_work_relation relationship-type="isPartOf" identifier-type="doi">10.25080/issn.2575-9752</inter_work_relation></related_item></program><doi_data><doi>10.25080/majora-1b6fd038-02a</doi><resource>https://zenodo.org/record/5152054</resource></doi_data></dataset></database></body></doi_batch>
