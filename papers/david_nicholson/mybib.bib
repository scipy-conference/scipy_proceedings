@techreport{arnaudImprovingWorkflowCrack2022,
  type = {Preprint},
  title = {Improving the Workflow to Crack {{Small}}, {{Unbalanced}}, {{Noisy}}, but {{Genuine}} ({{SUNG}}) Datasets in Bioacoustics: The Case of Bonobo Calls},
  shorttitle = {Improving the Workflow to Crack {{Small}}, {{Unbalanced}}, {{Noisy}}, but {{Genuine}} ({{SUNG}}) Datasets in Bioacoustics},
  author = {Arnaud, Vincent and Pellegrino, Fran{\c c}ois and Keenan, Sumir and {St-Gelais}, Xavier and Mathevon, Nicolas and Levr{\'e}ro, Florence and Coup{\'e}, Christophe},
  year = {2022},
  month = jun,
  institution = {{Animal Behavior and Cognition}},
	note = {\url{https://doi.org/10.1101/2022.06.26.497684}},
  urldate = {2022-07-04},
  abstract = {Abstract           Despite the accumulation of data and studies, deciphering animal vocal communication remains highly challenging. While progress has been made with some species for which we now understand the information exchanged through vocal signals, researchers are still left struggling with sparse recordings composing Small, Unbalanced, Noisy, but Genuine (SUNG) datasets. SUNG datasets offer a valuable but distorted vision of communication systems. Adopting the best practices in their analysis is therefore essential to effectively extract the available information and draw reliable conclusions. Here we show that the most recent advances in machine learning applied to a SUNG dataset succeed in unraveling the complex vocal repertoire of the bonobo, and we propose a workflow that can be effective with other animal species. We implement acoustic parameterization in three feature spaces along with three classification algorithms (Support Vector Machine, xgboost, neural networks) and their combination to explore the structure and variability of bonobo calls, as well as the robustness of the individual signature they encode. We underscore how classification performance is affected by the feature set and identify the most informative features. We highlight the need to address data leakage in the evaluation of classification performance to avoid misleading interpretations. Finally, using a Uniform Manifold Approximation and Projection (UMAP), we show that classifiers generate parsimonious data descriptions which help to understand the clustering of the bonobo acoustic space. Our results lead to identifying several practical approaches that are generalizable to any other animal communication system. To improve the reliability and replicability of vocal communication studies with SUNG datasets, we thus recommend: i) comparing several acoustic parameterizations; ii) adopting Support Vector Machines as the baseline classification approach; iii) explicitly evaluating data leakage and possibly implementing a mitigation strategy; iv) visualizing the dataset with UMAPs applied to classifier predictions rather than to raw acoustic features.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/53LHP6CN/Arnaud et al. - 2022 - Improving the workflow to crack Small, Unbalanced,.pdf}
}

@article{barkerCulturalTransmissionVocal2021,
  title = {Cultural Transmission of Vocal Dialect in the Naked Mole-Rat},
  author = {Barker, Alison J. and Veviurko, Grigorii and Bennett, Nigel C. and Hart, Daniel W. and Mograby, Lina and Lewin, Gary R.},
  year = {2021},
  month = jan,
  journal = {Science},
  volume = {371},
  number = {6528},
  pages = {503--507},
  issn = {0036-8075, 1095-9203},
	note = {\url{https://doi.org/10.1126/science.abc6588}},
  urldate = {2022-02-17},
  abstract = {The queen's chirp rules                            Naked mole-rats are known for their eusocial lifestyle, living in colonies that consist of many workers and a single breeding queen. Little is known about how individuals within these colonies navigate the many interactions that must occur in such a complex cooperative group. Barker               et al.               show that calls emitted by individuals, in particular the common ``chirp'' call, convey information specific to the animal's group (see the Perspective by Buffenstein). Group differences are cultural, rather than genetic, and are related to the queen: Cross-fostered pups adopt their rearing colony's dialects, and dialects change with queen replacement.                                         Science               , this issue p.               503               ; see also p.               461                        ,              Distinct colonies of naked mole-rats develop their own specific call dialects.           ,                             Naked mole-rats (               Heterocephalus glaber               ) form some of the most cooperative groups in the animal kingdom, living in multigenerational colonies under the control of a single breeding queen. Yet how they maintain this highly organized social structure is unknown. Here we show that the most common naked mole-rat vocalization, the soft chirp, is used to transmit information about group membership, creating distinctive colony dialects. Audio playback experiments demonstrate that individuals make preferential vocal responses to home colony dialects. Pups fostered in foreign colonies in early postnatal life learn the vocal dialect of their adoptive colonies, which suggests vertical transmission and flexibility of vocal signatures. Dialect integrity is partly controlled by the queen: Dialect cohesiveness decreases with queen loss and remerges only with the ascendance of a new queen.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/IFW6Q2P9/Barker et al. - 2021 - Cultural transmission of vocal dialect in the nake.pdf}
}

@article{coffeyDeepSqueakDeepLearningbased2019,
  title = {{{DeepSqueak}}: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations},
  shorttitle = {{{DeepSqueak}}},
  author = {Coffey, Kevin R. and Marx, Ruby E. and Neumaier, John F.},
  year = {2019},
  month = apr,
  journal = {Neuropsychopharmacology},
  volume = {44},
  number = {5},
  pages = {859--868},
  issn = {0893-133X, 1740-634X},
	note = {\url{https://doi.org/10.1038/s41386-018-0303-6}},
  urldate = {2023-03-09},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/E9TXEBJK/Coffey et al. - 2019 - DeepSqueak a deep learning-based system for detec.pdf}
}

@article{cohenAutomatedAnnotationBirdsong2022,
  title = {Automated Annotation of Birdsong with a Neural Network That Segments Spectrograms},
  author = {Cohen, Yarden and Nicholson, David Aaron and Sanchioni, Alexa and Mallaber, Emily K and Skidanova, Viktoriya and Gardner, Timothy J},
  year = {2022},
  journal = {Elife},
  volume = {11},
  pages = {e63853},
  publisher = {{eLife Sciences Publications Limited}}
}

@misc{cohenTweetynet2023,
  title = {Tweetynet},
  author = {Cohen, Yarden and Nicholson, David},
  year = {2023},
  month = feb,
	note = {\url{https://doi.org/10.5281/zenodo.7627197}},
  howpublished = {Zenodo}
}

@book{dask_development_team_dask_2016,
  title = {Dask: {{Library}} for Dynamic Task Scheduling},
  author = {{Dask Development Team}},
  year = {2016}
}

@article{doupeBIRDSONGHUMANSPEECH1999,
  title = {{{BIRDSONG AND HUMAN SPEECH}}: {{Common Themes}} and {{Mechanisms}}},
  shorttitle = {{{BIRDSONG AND HUMAN SPEECH}}},
  author = {Doupe, Allison J. and Kuhl, Patricia K.},
  year = {1999},
  month = mar,
  journal = {Annual Review of Neuroscience},
  volume = {22},
  number = {1},
  pages = {567--631},
  issn = {0147-006X, 1545-4126},
	note = {\url{https://doi.org/10.1146/annurev.neuro.22.1.567}},
  urldate = {2023-05-10},
  abstract = {Human speech and birdsong have numerous parallels. Both humans and songbirds learn their complex vocalizations early in life, exhibiting a strong dependence on hearing the adults they will imitate, as well as themselves as they practice, and a waning of this dependence as they mature. Innate predispositions for perceiving and learning the correct sounds exist in both groups, although more evidence of innate descriptions of species-specific signals exists in songbirds, where numerous species of vocal learners have been compared. Humans also share with songbirds an early phase of learning that is primarily perceptual, which then serves to guide later vocal production. Both humans and songbirds have evolved a complex hierarchy of specialized forebrain areas in which motor and auditory centers interact closely, and which control the lower vocal motor areas also found in nonlearners. In both these vocal learners, however, how auditory feedback of self is processed in these brain areas is surprisingly unclear. Finally, humans and songbirds have similar critical periods for vocal learning, with a much greater ability to learn early in life. In both groups, the capacity for late vocal learning may be decreased by the act of learning itself, as well as by biological factors such as the hormones of puberty. Although some features of birdsong and speech are clearly not analogous, such as the capacity of language for meaning, abstraction, and flexible associations, there are striking similarities in how sensory experience is internalized and used to shape vocal outputs, and how learning is enhanced during a critical period of development. Similar neural mechanisms may therefore be involved.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/L528VXUH/Doupe and Kuhl - 1999 - BIRDSONG AND HUMAN SPEECH Common Themes and Mecha.pdf}
}

@article{elieVocalRepertoireDomesticated2016,
  title = {The Vocal Repertoire of the Domesticated Zebra Finch: A Data-Driven Approach to Decipher the Information-Bearing Acoustic Features of Communication Signals},
  shorttitle = {The Vocal Repertoire of the Domesticated Zebra Finch},
  author = {Elie, Julie E. and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2016},
  month = mar,
  journal = {Animal Cognition},
  volume = {19},
  number = {2},
  pages = {285--315},
  issn = {1435-9448, 1435-9456},
	note = {\url{https://doi.org/10.1007/s10071-015-0933-6}},
  urldate = {2023-02-07},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/FPM49GI4/Elie and Theunissen - 2016 - The vocal repertoire of the domesticated zebra fin.pdf}
}

@misc{falconPyTorchLightning2023,
  title = {{{PyTorch Lightning}}},
  author = {Falcon, William and {team}, The PyTorch Lightning},
  year = {2023},
  month = apr,
	note = {\url{https://doi.org/10.5281/zenodo.7859091}},
  urldate = {2023-06-03},
  abstract = {The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.},
  howpublished = {Zenodo},
  keywords = {artificial intelligence,deep learning,machine learning},
  file = {/Users/davidnicholson/Zotero/storage/BX77DJKP/7859091.html}
}

@phdthesis{fukuzawaComputationalMethodsGeneralised2022,
  title = {Computational Methods for a Generalised Acoustics Analysis Workflow: A Thesis Presented in Partial Fulfilment of the Requirements for the Degree of {{Master}} of {{Science}} in {{Computer Science}} at {{Massey University}}, {{Auckland}}, {{New Zealand}}},
  shorttitle = {Computational Methods for a Generalised Acoustics Analysis Workflow},
  author = {Fukuzawa, Yukio},
  year = {2022},
  school = {Massey University},
  file = {/Users/davidnicholson/Zotero/storage/8PCXKJU5/FukuzawaMPhilThesis.pdf;/Users/davidnicholson/Zotero/storage/C6L86Y4D/Fukuzawa - 2022 - Computational methods for a generalised acoustics .pdf;/Users/davidnicholson/Zotero/storage/TX268GES/17296.html}
}

@article{goffinetLowdimensionalLearnedFeature2021,
  title = {Low-Dimensional Learned Feature Spaces Quantify Individual and Group Differences in Vocal Repertoires},
  author = {Goffinet, Jack and Brudner, Samuel and Mooney, Richard and Pearson, John},
  year = {2021},
  month = may,
  journal = {eLife},
  volume = {10},
  pages = {e67855},
  issn = {2050-084X},
	note = {\url{https://doi.org/10.7554/eLife.67855}},
  urldate = {2022-08-09},
  abstract = {Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-\-specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-\-by-m\- oment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-\-dimensional vocal behavior.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/8KFUVGLW/Goffinet et al. - 2021 - Low-dimensional learned feature spaces quantify in.pdf}
}

@article{graves_framewise_2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  journal = {Neural networks},
  volume = {18},
  number = {5-6},
  pages = {602--610},
	note = {\url{https://doi.org/10.1016/j.neunet.2005.06.042}},
}

@incollection{graves_supervised_2012,
  title = {Supervised Sequence Labelling},
  booktitle = {Supervised Sequence Labelling with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2012},
  pages = {5--13},
  publisher = {{Springer}},
	note = {\url{https://doi.org/10.1007/978-3-642-24797-2_2}},
}

@article{harris2020array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R and Millman, K Jarrod and {van der Walt}, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  year = {2020},
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}}
}

@article{hauserFacultyLanguageWhat2002,
  title = {The {{Faculty}} of {{Language}}: {{What Is It}}, {{Who Has It}}, and {{How Did It Evolve}}?},
  shorttitle = {The {{Faculty}} of {{Language}}},
  author = {Hauser, Marc D. and Chomsky, Noam and Fitch, W. Tecumseh},
  year = {2002},
  month = nov,
  journal = {Science},
  volume = {298},
  number = {5598},
  pages = {1569--1579},
  issn = {0036-8075, 1095-9203},
	note = {\url{https://doi.org/10.1126/science.298.5598.1569}},
  urldate = {2022-07-06},
  abstract = {We argue that an understanding of the faculty of language requires substantial interdisciplinary cooperation. We suggest how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience. We submit that a distinction should be made between the faculty of language in the broad sense (FLB) and in the narrow sense (FLN). FLB includes a sensory-motor system, a conceptual-intentional system, and the computational mechanisms for recursion, providing the capacity to generate an infinite range of expressions from a finite set of elements. We hypothesize that FLN only includes recursion and is the only uniquely human component of the faculty of language. We further argue that FLN may have evolved for reasons other than language, hence comparative studies might look for evidence of such computations outside of the domain of communication (for example, number, navigation, and social relations).},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/HY959NW7/Hauser et al. - 2002 - The Faculty of Language What Is It, Who Has It, a.pdf}
}

@book{hopp2012animal,
  title = {Animal Acoustic Communication: Sound Analysis and Research Methods},
  author = {Hopp, Steven L and Owren, Michael J and Evans, Christopher S},
  year = {2012},
  publisher = {{Springer Science \& Business Media}}
}

@article{hudsonModelingHowPopulation2022,
  title = {Modeling How Population Size Drives the Evolution of Birdsong, a Functional Cultural Trait},
  author = {Hudson, Emily J. and Creanza, Nicole},
  year = {2022},
  month = jun,
  journal = {Evolution},
  volume = {76},
  number = {6},
  pages = {1139--1152},
  issn = {0014-3820},
	note = {\url{https://doi.org/10.1111/evo.14489}},
  urldate = {2023-06-02},
  abstract = {Oscine songbirds have been an important study system for social learning, particularly because their learned songs provide an analog for human languages and music. Here, we propose a different analogy: from an evolutionary perspective, could birds' songs change over time more like arrowheads than arias? Small improvements to a bird's song can lead to large fitness differences for its singer, which could make songs more analogous to human tools than languages. We modify a model of human tool evolution to accommodate cultural evolution of birdsong: each song learner chooses the most skilled available tutor to emulate, and each is more likely to produce an inferior copy than a superior one. Similar to human tool evolution, our model suggests that larger populations of birds could foster improvements in song over time, even when learners restrict their pool of tutors to a subset of individuals in their social network. We also demonstrate that song elements could be simplified instead of lost after population bottlenecks if lower quality traits are easier to imitate than higher quality ones. We show that these processes could plausibly generate empirically observed patterns of song evolution for some song traits, and we make predictions about the types of song elements most likely to be lost when populations shrink. More broadly, we aim to connect the modeling approaches used in human and nonhuman systems, moving toward a cohesive theoretical framework that accounts for both cognitive and demographic processes.},
  file = {/Users/davidnicholson/Zotero/storage/LI7LY66A/Hudson and Creanza - 2022 - Modeling how population size drives the evolution .pdf;/Users/davidnicholson/Zotero/storage/4S8Z9GGM/6728751.html}
}

@article{Hunter:2007,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  publisher = {{IEEE COMPUTER SOC}},
	note = {\url{https://doi.org/10.1109/MCSE.2007.55}},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.}
}

@article{kakishitaEthologicalDataMining2009,
  title = {Ethological Data Mining: An Automata-Based Approach to Extract Behavioral Units and Rules},
  shorttitle = {Ethological Data Mining},
  author = {Kakishita, Yasuki and Sasahara, Kazutoshi and Nishino, Tetsuro and Takahasi, Miki and Okanoya, Kazuo},
  year = {2009},
  month = jun,
  journal = {Data Mining and Knowledge Discovery},
  volume = {18},
  number = {3},
  pages = {446--471},
  issn = {1384-5810, 1573-756X},
	note = {\url{https://doi.org/10.1007/s10618-008-0122-1}},
  urldate = {2021-11-27},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/TRH7H79Q/Kakishita et al. - 2009 - Ethological data mining an automata-based approac.pdf}
}

@article{katzdaniels.UnderstandingSoftwareCommunity2022,
  title = {Understanding {{Software Community Formation}}},
  author = {Katz, Daniel S. and Barker, Michelle and Bangerth, Wolfgang and Crawford, T. Daniel and Elmer, Peter and Harrow, Jen and Hwang, Lorraine and Psomopoulos, Fotis and Stewart, Graeme A. and Zentner, Michael},
  year = {2022},
  month = feb,
  publisher = {{Zenodo}},
	note = {\url{https://doi.org/10.5281/ZENODO.6249316}},
  urldate = {2022-07-02},
  abstract = {a talk presented in the Topology Optimization Webinar, webinar 20, "Future of Scientific and Engineering Computing in Research Community and Industry", 24 February 2022.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/CAMC93ET/Katz, Daniel S. et al. - 2022 - Understanding Software Community Formation.pdf}
}

@article{kershenbaumAcousticSequencesNonhuman2016,
  title = {Acoustic Sequences in Non-Human Animals: A Tutorial Review and Prospectus: {{Acoustic}} Sequences in Animals},
  shorttitle = {Acoustic Sequences in Non-Human Animals},
  author = {Kershenbaum, Arik and Blumstein, Daniel T. and Roch, Marie A. and Ak{\c c}ay, {\c C}a{\u g}lar and Backus, Gregory and Bee, Mark A. and Bohn, Kirsten and Cao, Yan and Carter, Gerald and C{\"a}sar, Cristiane and Coen, Michael and DeRuiter, Stacy L. and Doyle, Laurance and Edelman, Shimon and {Ferrer-i-Cancho}, Ramon and Freeberg, Todd M. and Garland, Ellen C. and Gustison, Morgan and Harley, Heidi E. and Huetz, Chlo{\'e} and Hughes, Melissa and Hyland Bruno, Julia and Ilany, Amiyaal and Jin, Dezhe Z. and Johnson, Michael and Ju, Chenghui and Karnowski, Jeremy and Lohr, Bernard and Manser, Marta B. and McCowan, Brenda and Mercado, Eduardo and Narins, Peter M. and Piel, Alex and Rice, Megan and Salmi, Roberta and Sasahara, Kazutoshi and Sayigh, Laela and Shiu, Yu and Taylor, Charles and Vallejo, Edgar E. and Waller, Sara and {Zamora-Gutierrez}, Veronica},
  year = {2016},
  month = feb,
  journal = {Biological Reviews},
  volume = {91},
  number = {1},
  pages = {13--52},
  issn = {14647931},
	note = {\url{https://doi.org/10.1111/brv.12160}},
  urldate = {2021-11-22},
  abstract = {Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise \textendash{} let alone understand \textendash{} the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, `Analysing vocal sequences in animals'. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/ABH3RYWI/Kershenbaum et al. - 2016 - Acoustic sequences in non-human animals a tutoria.pdf}
}

@article{kershenbaumQuantifyingSimilarityAnimal2015,
  title = {Quantifying Similarity in Animal Vocal Sequences: Which Metric Performs Best?},
  shorttitle = {Quantifying Similarity in Animal Vocal Sequences},
  author = {Kershenbaum, Arik and Garland, Ellen C.},
  editor = {Nakagawa, Shinichi},
  year = {2015},
  month = dec,
  journal = {Methods in Ecology and Evolution},
  volume = {6},
  number = {12},
  pages = {1452--1461},
  issn = {2041-210X, 2041-210X},
	note = {\url{https://doi.org/10.1111/2041-210X.12433}},
  urldate = {2022-07-03},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/AEMKE8UM/Kershenbaum and Garland - 2015 - Quantifying similarity in animal vocal sequences .pdf}
}

@article{koumura_automatic_2016-1,
  ids = {koumuraAutomaticRecognitionElement2016,koumuraAutomaticRecognitionElement2016a,koumura_automatic_2016},
  title = {Automatic Recognition of Element Classes and Boundaries in the Birdsong with Variable Sequences},
  author = {Koumura, Takuya and Okanoya, Kazuo},
  year = {2016},
  journal = {PLoS ONE},
  volume = {11},
  number = {7},
  issn = {19326203},
	note = {\url{https://doi.org/10.1371/journal.pone.0159188}},
  abstract = {Researches on sequential vocalization often require analysis of vocalizations in long continuous sounds. In such studies as developmental ones or studies across generations in which days or months of vocalizations must be analyzed, methods for automatic recognition would be strongly desired. Although methods for automatic speech recognition for application purposes have been intensively studied, blindly applying them for biological purposes may not be an optimal solution. This is because, unlike human speech recognition, analysis of sequential vocalizations often requires accurate extraction of timing information. In the present study we propose automated systems suitable for recognizing birdsong, one of the most intensively investigated sequential vocalizations, focusing on the three properties of the birdsong. First, a song is a sequence of vocal elements, called notes, which can be grouped into categories. Second, temporal structure of birdsong is precisely controlled, meaning that temporal information is important in song analysis. Finally, notes are produced according to certain probabilistic rules, which may facilitate the accurate song recognition. We divided the procedure of song recognition into three sub-steps: local classification, boundary detection, and global sequencing, each of which corresponds to each of the three properties of birdsong. We compared the performances of several different ways to arrange these three steps. As results, we demonstrated a hybrid model of a deep convolutional neural network and a hidden Markov model was effective. We propose suitable arrangements of methods according to whether accurate boundary detection is needed. Also we designed the new measure to jointly evaluate the accuracy of note classification and boundary detection. Our methods should be applicable, with small modification and tuning, to the songs in other species that hold the three properties of the sequential vocalization.},
  keywords = {Bird song,Birds,Hidden Markov models,Markov models,Neural networks,speech,Syntax,vocalization},
  file = {/Users/davidnicholson/Zotero/storage/6NDJN668/Koumura and Okanoya - 2016 - Automatic Recognition of Element Classes and Bound.pdf;/Users/davidnicholson/Zotero/storage/CTHYJ43M/article.html}
}

@article{linhartPotentialAcousticIndividual2022,
  title = {The Potential for Acoustic Individual Identification in Mammals},
  author = {Linhart, Pavel and {Mahamoud-Issa}, Mathieu and Stowell, Dan and Blumstein, Daniel T.},
  year = {2022},
  month = jun,
  journal = {Mammalian Biology},
  volume = {102},
  number = {3},
  pages = {667--683},
  issn = {1618-1476},
	note = {\url{https://doi.org/10.1007/s42991-021-00222-2}},
  urldate = {2023-06-03},
  abstract = {Many studies have revealed that animal vocalizations, including those from mammals, are individually distinctive. Therefore, acoustic identification of individuals (AIID) has been repeatedly suggested as a non-invasive and labor efficient alternative to mark-recapture identification methods. We present a pipeline of steps for successful AIID in a given species. By conducting such work, we will also improve our understanding of identity signals in general. Strong and stable acoustic signatures are necessary for successful AIID. We reviewed studies of individual variation in mammalian vocalizations as well as pilot studies using acoustic identification to census mammals and birds. We found the greatest potential for AIID (characterized by strong and stable acoustic signatures) was in Cetacea and Primates (including humans). In species with weaker acoustic signatures, AIID could still be a valuable tool once its limitations are fully acknowledged. A major obstacle for widespread utilization of AIID is the absence of tools integrating all AIID subtasks within a single package. Automation of AIID could be achieved with the use of advanced machine learning techniques inspired by those used in human speaker recognition or tailored to specific challenges of animal AIID. Unfortunately, further progress in this area is currently hindered by the lack of appropriate publicly available datasets. However, we believe that after overcoming the issues outlined above, AIID can quickly become a widespread and valuable tool in field research and conservation of mammals and other animals.},
  langid = {english},
  keywords = {Acoustic communication,Acoustic individual identification (AIID),Acoustic signature,Individual identity,Individual variation,Machine learning,Mammal,Vocalizations}
}

@inproceedings{marcel_torchvision_2010,
  title = {Torchvision the Machine-Vision Package of Torch},
  booktitle = {Proceedings of the 18th {{ACM}} International Conference on {{Multimedia}}},
  author = {Marcel, S{\'e}bastien and Rodriguez, Yann},
  year = {2010},
  month = oct,
  series = {{{MM}} '10},
  pages = {1485--1488},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
	note = {\url{https://doi.org/10.1145/1873951.1874254}},
  urldate = {2020-08-26},
  abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
  isbn = {978-1-60558-933-6}
}

@inproceedings{marcelTorchvisionMachinevisionPackage2010,
  title = {Torchvision the Machine-Vision Package of Torch},
  booktitle = {Proceedings of the 18th {{ACM}} International Conference on {{Multimedia}}},
  author = {Marcel, S{\'e}bastien and Rodriguez, Yann},
  year = {2010},
  month = oct,
  series = {{{MM}} '10},
  pages = {1485--1488},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
	note = {\url{https://doi.org/10.1145/1873951.1874254}},
  urldate = {2020-08-26},
  abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
  isbn = {978-1-60558-933-6}
}

@article{markowitz_long-range_2013,
  ids = {markowitzLongrangeOrderCanary2013},
  title = {Long-Range {{Order}} in {{Canary Song}}},
  author = {Markowitz, Jeffrey E. and Ivie, Elizabeth and Kligler, Laura and Gardner, Timothy J.},
  year = {2013},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {5},
  pages = {e1003052},
  issn = {1553-7358},
	note = {\url{https://doi.org/10.1371/journal.pcbi.1003052}},
  urldate = {2018-06-13},
  abstract = {Bird songs range in form from the simple notes of a Chipping Sparrow to the rich performance of the nightingale. Non-adjacent correlations can be found in the syntax of some birdsongs, indicating that the choice of what to sing next is determined not only by the current syllable, but also by previous syllables sung. Here we examine the song of the domesticated canary, a complex singer whose song consists of syllables, grouped into phrases that are arranged in flexible sequences. Phrases are defined by a fundamental time-scale that is independent of the underlying syllable duration. We show that the ordering of phrases is governed by long-range rules: the choice of what phrase to sing next in a given context depends on the history of the song, and for some syllables, highly specific rules produce correlations in song over timescales of up to ten seconds. The neural basis of these long-range correlations may provide insight into how complex behaviors are assembled from more elementary, stereotyped modules.},
  langid = {english},
  keywords = {Acoustics,Bird song,Birds,Canaries,Entropy,Markov processes,Syllables,Syntax},
  file = {/Users/davidnicholson/Zotero/storage/NLJREIAM/Markowitz et al. - 2013 - Long-range Order in Canary Song.pdf;/Users/davidnicholson/Zotero/storage/IQ6M3IAQ/article.html}
}

@article{martinsSoundProductionLearning2020,
  title = {Sound Production Learning across Species: {{Beyond}} the Vocal Learning Dichotomy},
  author = {Martins, Pedro Tiago},
  year = {2020},
  publisher = {{Universitat de Barcelona}},
  file = {/Users/davidnicholson/Zotero/storage/52B3YGK5/PTM_PhD_THESIS.pdf}
}

@article{martinsVocalLearningContinuum2020,
  title = {Vocal Learning: {{Beyond}} the Continuum},
  shorttitle = {Vocal Learning},
  author = {Martins, Pedro Tiago and Boeckx, Cedric},
  year = {2020},
  month = mar,
  journal = {PLOS Biology},
  volume = {18},
  number = {3},
  pages = {e3000672},
  issn = {1545-7885},
	note = {\url{https://doi.org/10.1371/journal.pbio.3000672}},
  urldate = {2022-12-11},
  abstract = {Vocal learning is the ability to modify vocal output on the basis of experience. Traditionally, species have been classified as either displaying or lacking this ability. A recent proposal, the vocal learning continuum, recognizes the need to have a more nuanced view of this phenotype and abandon the yes\textendash no dichotomy. However, it also limits vocal learning to production of novel calls through imitation, moreover subserved by a forebrain-to-phonatorymuscles circuit. We discuss its limitations regarding the characterization of vocal learning across species and argue for a more permissive view.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/QFVTWX2C/Martins and Boeckx - 2020 - Vocal learning Beyond the continuum.pdf}
}

@article{mcgregorSharedMechanismsAuditory2022,
  title = {Shared Mechanisms of Auditory and Non-Auditory Vocal Learning in the Songbird Brain},
  author = {McGregor, James N and Grassler, Abigail L and Jaffe, Paul I and Jacob, Amanda Louise and Brainard, Michael S and Sober, Samuel J},
  editor = {Goldberg, Jesse H and {Shinn-Cunningham}, Barbara G and Giret, Nicolas},
  year = {2022},
  month = sep,
  journal = {eLife},
  volume = {11},
  pages = {e75691},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
	note = {\url{https://doi.org/10.7554/eLife.75691}},
  urldate = {2023-03-09},
  abstract = {Songbirds and humans share the ability to adaptively modify their vocalizations based on sensory feedback. Prior studies have focused primarily on the role that auditory feedback plays in shaping vocal output throughout life. In contrast, it is unclear how non-auditory information drives vocal plasticity. Here, we first used a reinforcement learning paradigm to establish that somatosensory feedback (cutaneous electrical stimulation) can drive vocal learning in adult songbirds. We then assessed the role of a songbird basal ganglia thalamocortical pathway critical to auditory vocal learning in this novel form of vocal plasticity. We found that both this circuit and its dopaminergic inputs are necessary for non-auditory vocal learning, demonstrating that this pathway is critical for guiding adaptive vocal changes based on both auditory and somatosensory signals. The ability of this circuit to use both auditory and somatosensory information to guide vocal learning may reflect a general principle for the neural systems that support vocal plasticity across species.},
  keywords = {Bengalese finch,lonchura striata var. domestica,songbird},
  file = {/Users/davidnicholson/Zotero/storage/VMYTLYGD/McGregor et al. - 2022 - Shared mechanisms of auditory and non-auditory voc.pdf}
}

@article{nicholson_bengalese_2017,
  title = {Bengalese {{Finch}} Song Repository},
  author = {Nicholson, David and Queen, Jonah E. and Sober, Samuel J.},
  year = {2017},
  month = oct,
	note = {\url{https://doi.org/10.6084/m9.figshare.4805749.v5}},
  urldate = {2019-08-17},
  abstract = {This is a collection of song from four Bengalese finches recorded in the Sober lab at Emory University. The song has been hand-labeled by two of the authors. To make it easy to work with the dataset, we have created a Python package, "evfuncs", available at https://github.com/soberlab/evfuncs (Please see "References" section below for a direct link).How to work with the files is described on the README of that library, but we describe the types of files here briefly. The actual sound files have the extension .cbin and were created by an application that runs behavioral experiments and collects data called EvTAF. Each .cbin file has an associated .cbin.not.mat file that contains song syllable onsets, offsets, labels, etc., created by a GUI for song annotation called evsonganaly. Each .cbin file also has associated .tmp and .rec files, also created by EvTAF. Those files are not strictly required to work with this dataset but are included for completeness.We share this collection as a means of testing different machine learning algorithms for classifying the elements of birdsong, known as syllables. A Python package for that purpose, "hybrid-vocal-classifier", was developed in part using this dataset.To learn more about hybrid-vocal-classifier, please visit https://hybrid-vocal-classifier.readthedocs.io/en/latest/ (see "References" section below for a direct link).},
  keywords = {Bengalese finch,Bengalese Finch Birdsongs,Bengalese finch song,Keras,machine learning,neuroscience/behavioral neuroscience,NumPy,Python,scikit-learn,SciPy,songbird studies,songbirds}
}

@inproceedings{nicholson2016comparison,
  title = {Comparison of Machine Learning Methods Applied to Birdsong Element Classification},
  booktitle = {Proceedings of the 15th Python in Science Conference},
  author = {Nicholson, David},
  year = {2016},
  pages = {57--61},
	note = {\url{https://doi.org/10.25080/majora-629e541a-008}},
}

@misc{nicholsonVak2022,
  title = {Vak},
  author = {Nicholson, David and Cohen, Yarden},
  year = {2022},
  month = mar,
	note = {\url{https://doi.org/10.5281/zenodo.6808839}},
  howpublished = {Zenodo}
}

@article{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  month = oct,
  urldate = {2020-08-27},
  abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
  file = {/Users/davidnicholson/Zotero/storage/4KSDQNJX/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf;/Users/davidnicholson/Zotero/storage/GTYCW2RJ/forum.html}
}

@misc{petersonUnsupervisedDiscoveryFamily2023,
  title = {Unsupervised Discovery of Family Specific Vocal Usage in the {{Mongolian}} Gerbil},
  author = {Peterson, Ralph E. and Choudhri, Aman and Mitelut, Catalin and Tanelus, Aramis and {Capo-Battaglia}, Athena and Williams, Alex H. and Schneider, David M. and Sanes, Dan H.},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2023.03.11.532197},
  publisher = {{bioRxiv}},
	note = {\url{https://doi.org/10.1101/2023.03.11.532197}},
  urldate = {2023-06-02},
  abstract = {Many animal species use vocalizations to communicate social information and previous experiments in rodents have identified a range of vocal types that may be used for this purpose. However, social vocalizations are typically acquired during brief interactions between animals with no prior social relationship, and under environmental conditions with limited ethological relevance. Here, we establish long-term acoustic recordings from Mongolian gerbil families, a core social group that uses an array of sonic and ultrasonic vocalizations which vary with social context. Three separate gerbil families (two parents and four pups) were transferred to an enlarged environment and continuous 20-day audio recordings were obtained. We leveraged deep-learning based unsupervised analysis of 583,237 vocalizations to show that gerbils exhibit a more complex vocal repertoire than has been previously reported. Furthermore, gerbils displayed family-specific vocal repertoires, including differences in vocal type usage and transitions. Since gerbils live naturally as extended families in complex underground burrows that are adjacent to other families, these results suggest the presence of a vocal dialect which could be exploited by animals to represent kinship. These findings offer insight into the naturalistic vocal tendencies of gerbil families and position the Mongolian gerbil as a compelling animal to study the neural basis of vocal communication.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/KFIIK9ES/Peterson et al. - 2023 - Unsupervised discovery of family specific vocal us.pdf}
}

@article{petkovBirdsPrimatesSpoken2012,
  title = {Birds, Primates, and Spoken Language Origins: Behavioral Phenotypes and Neurobiological Substrates},
  shorttitle = {Birds, Primates, and Spoken Language Origins},
  author = {Petkov, Christopher I. and Jarvis, Erich D.},
  year = {2012},
  journal = {Frontiers in Evolutionary Neuroscience},
  volume = {4},
  issn = {1663-070X},
	note = {\url{https://doi.org/10.3389/fnevo.2012.00012}},
  urldate = {2022-07-06},
  abstract = {Vocal learners such as humans and songbirds can learn to produce elaborate patterns of structurally organized vocalizations, whereas many other vertebrates such as non-human primates and most other bird groups either cannot or do so to a very limited degree. To explain the similarities among humans and vocal-learning birds and the differences with other species, various theories have been proposed. One set of theories are motor theories, which underscore the role of the motor system as an evolutionary substrate for vocal production learning. For instance, the motor theory of speech and song perception proposes enhanced auditory perceptual learning of speech in humans and song in birds, which suggests a considerable level of neurobiological specialization. Another, a motor theory of vocal learning origin, proposes that the brain pathways that control the learning and production of song and speech were derived from adjacent motor brain pathways. Another set of theories are cognitive theories, which address the interface between cognition and the auditory-vocal domains to support language learning in humans. Here we critically review the behavioral and neurobiological evidence for parallels and differences between the so-called vocal learners and vocal non-learners in the context of motor and cognitive theories. In doing so, we note that behaviorally vocal-production learning abilities are more distributed than categorical, as are the auditory-learning abilities of animals. We propose testable hypotheses on the extent of the specializations and cross-species correspondences suggested by motor and cognitive theories. We believe that determining how spoken language evolved is likely to become clearer with concerted efforts in testing comparative data from many non-human animal species.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/8M2GEIZP/Petkov and Jarvis - 2012 - Birds, primates, and spoken language origins beha.pdf}
}

@article{pratEverydayBatVocalizations2016,
  title = {Everyday Bat Vocalizations Contain Information about Emitter, Addressee, Context, and Behavior},
  author = {Prat, Yosef and Taub, Mor and Yovel, Yossi},
  year = {2016},
  month = dec,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {39419},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
	note = {\url{https://doi.org/10.1038/srep39419}},
  urldate = {2023-06-02},
  abstract = {Animal vocal communication is often diverse and structured. Yet, the information concealed in animal vocalizations remains elusive. Several studies have shown that animal calls convey information about their emitter and the context. Often, these studies focus on specific types of calls, as it is rarely possible to probe an entire vocal repertoire at once. In this study, we continuously monitored Egyptian fruit bats for months, recording audio and video around-the-clock. We analyzed almost 15,000 vocalizations, which accompanied the everyday interactions of the bats, and were all directed toward specific individuals, rather than broadcast. We found that bat vocalizations carry ample information about the identity of the emitter, the context of the call, the behavioral response to the call, and even the call's addressee. Our results underline the importance of studying the mundane, pairwise, directed, vocal interactions of animals.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Animal behaviour,Evolution of language,Social behaviour},
  file = {/Users/davidnicholson/Zotero/storage/TX3RQY9E/Prat et al. - 2016 - Everyday bat vocalizations contain information abo.pdf}
}

@article{provostImpactsFinetuningPhylogenetic2022,
  title = {The Impacts of Fine-Tuning, Phylogenetic Distance, and Sample Size on Big-Data Bioacoustics},
  author = {Provost, Kaiya L. and Yang, Jiaying and Carstens, Bryan C.},
  year = {2022},
  month = dec,
  journal = {PLOS ONE},
  volume = {17},
  number = {12},
  pages = {e0278522},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
	note = {\url{https://doi.org/10.1371/journal.pone.0278522}},
  urldate = {2023-03-09},
  abstract = {Vocalizations in animals, particularly birds, are critically important behaviors that influence their reproductive fitness. While recordings of bioacoustic data have been captured and stored in collections for decades, the automated extraction of data from these recordings has only recently been facilitated by artificial intelligence methods. These have yet to be evaluated with respect to accuracy of different automation strategies and features. Here, we use a recently published machine learning framework to extract syllables from ten bird species ranging in their phylogenetic relatedness from 1 to 85 million years, to compare how phylogenetic relatedness influences accuracy. We also evaluate the utility of applying trained models to novel species. Our results indicate that model performance is best on conspecifics, with accuracy progressively decreasing as phylogenetic distance increases between taxa. However, we also find that the application of models trained on multiple distantly related species can improve the overall accuracy to levels near that of training and analyzing a model on the same species. When planning big-data bioacoustics studies, care must be taken in sample design to maximize sample size and minimize human labor without sacrificing accuracy.},
  langid = {english},
  keywords = {Animal phylogenetics,Bioacoustics,Bird song,Birds,Machine learning,Machine learning algorithms,Syllables,Vocalization},
  file = {/Users/davidnicholson/Zotero/storage/JM8VWYDR/Provost et al. - 2022 - The impacts of fine-tuning, phylogenetic distance,.pdf}
}

@techreport{renteriaBirdsongPhraseVerification2021,
  type = {Preprint},
  title = {Birdsong {{Phrase Verification}} and {{Classification Using Siamese Neural Networks}}},
  author = {Renter{\'i}a, Santiago and Vallejo, Edgar E. and Taylor, Charles E.},
  year = {2021},
  month = mar,
  institution = {{Bioinformatics}},
	note = {\url{https://doi.org/10.1101/2021.03.16.435625}},
  urldate = {2021-11-05},
  abstract = {Abstract           The process of learning good features to discriminate among numerous and different bird phrases is computationally expensive. Moreover, it might be impossible to achieve acceptable performance in cases where training data is scarce and classes are unbalanced. To address this issue, we propose a few-shot learning task in which an algorithm must make predictions given only a few instances of each class. We compared the performance of different Siamese Neural Networks at metric learning over the set of Cassini's Vireo syllables. Then, the network features were reused for the few-shot classification task. With this approach we overcame the limitations of data scarcity and class imbalance while achieving state-of-the-art performance.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/IE76RI8G/Rentera et al. - 2021 - Birdsong Phrase Verification and Classification Us.pdf}
}

@article{robinsonSpecieslevelRepertoireSize2019,
  title = {Species-Level Repertoire Size Predicts a Correlation between Individual Song Elaboration and Reproductive Success},
  author = {Robinson, Cristina M. and Creanza, Nicole},
  year = {2019},
  journal = {Ecology and Evolution},
  volume = {9},
  number = {14},
  pages = {8362--8377},
  issn = {2045-7758},
	note = {\url{https://doi.org/10.1002/ece3.5418}},
  urldate = {2023-06-02},
  abstract = {Birdsong has long been considered a sexually selected trait that relays honest information about male quality, and laboratory studies generally suggest that female songbirds prefer larger repertoires. However, analysis of field studies across species surprisingly revealed a weak correlation between song elaboration and reproductive success, and it remains unknown why only certain species show this correlation in nature. Taken together, these studies suggest that females in numerous species can detect and prefer larger repertoires in a laboratory setting, but larger individual repertoires correlate with reproductive success only in a subset of these species. This prompts the question: Do the species that show a stronger correlation between reproductive success and larger individual repertoires in nature have anything in common? In this study, we test whether between-species differences in two song-related variables\textemdash species average syllable repertoire size and adult song stability over time\textemdash can be used to predict the importance of individual song elaboration in reproductive success within a species. Our cross-species meta-analysis of field studies revealed that species with larger average syllable repertoire sizes exhibited a stronger correlation between individual elaboration and reproductive success than species with smaller syllable repertoires. Song stability versus plasticity in adulthood provided little predictive power on its own, suggesting that the putative correlation between repertoire size and age in open-ended learners does not explain the association between song elaboration and reproductive success.},
  copyright = {\textcopyright{} 2019 The Authors. Ecology and Evolution Published by John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {Bayesian analysis,closed-ended leaning,elaborate song,learned vocalization,meta-analysis,open-ended learning,passeriformes,reproductive success,sexual selection,songbirds,syllable repertoire},
  file = {/Users/davidnicholson/Zotero/storage/Z4DHZ8LA/Robinson and Creanza - 2019 - Species-level repertoire size predicts a correlati.pdf;/Users/davidnicholson/Zotero/storage/E9R2EAVS/ece3.html}
}

@article{sainburgComputationalNeuroethologyVocal2021,
  title = {Toward a {{Computational Neuroethology}} of {{Vocal Communication}}: {{From Bioacoustics}} to {{Neurophysiology}}, {{Emerging Tools}} and {{Future Directions}}},
  shorttitle = {Toward a {{Computational Neuroethology}} of {{Vocal Communication}}},
  author = {Sainburg, Tim and Gentner, Timothy Q.},
  year = {2021},
  month = dec,
  journal = {Frontiers in Behavioral Neuroscience},
  volume = {15},
  pages = {811737},
  issn = {1662-5153},
	note = {\url{https://doi.org/10.3389/fnbeh.2021.811737}},
  urldate = {2022-01-22},
  abstract = {Recently developed methods in computational neuroethology have enabled increasingly detailed and comprehensive quantification of animal movements and behavioral kinematics. Vocal communication behavior is well poised for application of similar large-scale quantification methods in the service of physiological and ethological studies. This review describes emerging techniques that can be applied to acoustic and vocal communication signals with the goal of enabling study beyond a small number of model species. We review a range of modern computational methods for bioacoustics, signal processing, and brain-behavior mapping. Along with a discussion of recent advances and techniques, we include challenges and broader goals in establishing a framework for the computational neuroethology of vocal communication.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/AZFQEZ85/Sainburg and Gentner - 2021 - Toward a Computational Neuroethology of Vocal Comm.pdf}
}

@article{sainburgFindingVisualizingQuantifying2020,
  title = {Finding, Visualizing, and Quantifying Latent Structure across Diverse Animal Vocal Repertoires},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
  year = {2020},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {10},
  pages = {e1008228},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
	note = {\url{https://doi.org/10.1371/journal.pcbi.1008228}},
  urldate = {2023-03-09},
  abstract = {Animals produce vocalizations that range in complexity from a single repeated call to hundreds of unique vocal elements patterned in sequences unfolding over hours. Characterizing complex vocalizations can require considerable effort and a deep intuition about each species' vocal behavior. Even with a great deal of experience, human characterizations of animal communication can be affected by human perceptual biases. We present a set of computational methods for projecting animal vocalizations into low dimensional latent representational spaces that are directly learned from the spectrograms of vocal signals. We apply these methods to diverse datasets from over 20 species, including humans, bats, songbirds, mice, cetaceans, and nonhuman primates. Latent projections uncover complex features of data in visually intuitive and quantifiable ways, enabling high-powered comparative analyses of vocal acoustics. We introduce methods for analyzing vocalizations as both discrete sequences and as continuous latent variables. Each method can be used to disentangle complex spectro-temporal structure and observe long-timescale organization in communication.},
  langid = {english},
  keywords = {Animal communication,Bioacoustics,Birds,Finches,Hidden Markov models,Speech,Syllables,Vocalization},
  file = {/Users/davidnicholson/Zotero/storage/B5AANVYI/Sainburg et al. - 2020 - Finding, visualizing, and quantifying latent struc.pdf}
}

@article{smith-vidaurreIndividualSignaturesOutweigh2020,
  title = {Individual Signatures Outweigh Social Group Identity in Contact Calls of a Communally Nesting Parrot},
  author = {{Smith-Vidaurre}, Grace and {Araya-Salas}, Marcelo and Wright, Timothy F},
  editor = {Naguib, Marc},
  year = {2020},
  month = mar,
  journal = {Behavioral Ecology},
  volume = {31},
  number = {2},
  pages = {448--458},
  issn = {1045-2249, 1465-7279},
	note = {\url{https://doi.org/10.1093/beheco/arz202}},
  urldate = {2022-08-05},
  abstract = {Abstract             Despite longstanding interest in the evolutionary origins and maintenance of vocal learning, we know relatively little about how social dynamics influence vocal learning processes in natural populations. The ``signaling group membership'' hypothesis proposes that socially learned calls evolved and are maintained as signals of group membership. However, in fission\textendash fusion societies, individuals can interact in social groups across various social scales. For learned calls to signal group membership over multiple social scales, they must contain information about group membership over each of these scales, a concept termed ``hierarchical mapping.'' Monk parakeets (Myiopsitta monachus), small parrots native to South America, exhibit vocal mimicry in captivity and fission\textendash fusion social dynamics in the wild. We examined patterns of contact call acoustic similarity in Uruguay to test the hierarchical mapping assumption of the signaling group membership hypothesis. We also asked whether geographic variation patterns matched regional dialects or geographic clines that have been documented in other vocal learning species. We used visual inspection, spectrographic cross-correlation and random forests, a machine learning approach, to evaluate contact call similarity. We compared acoustic similarity across social scales and geographic distance using Mantel tests and spatial autocorrelation. We found high similarity within individuals, and low, albeit significant, similarity within groups at the pair, flock and site social scales. Patterns of acoustic similarity over geographic distance did not match mosaic or graded patterns expected in dialectal or clinal variation. Our findings suggest that monk parakeet social interactions rely more heavily upon individual recognition than group membership at higher social scales.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/AC397BYS/Smith-Vidaurre et al. - 2020 - Individual signatures outweigh social group identi.pdf}
}

@article{sober2009adult,
  title = {Adult Birdsong Is Actively Maintained by Error Correction},
  author = {Sober, Samuel J and Brainard, Michael S},
  year = {2009},
  journal = {Nature neuroscience},
  volume = {12},
  number = {7},
  pages = {927},
  publisher = {{Nature Publishing Group}},
	note = {\url{https://doi.org/10.1038/nn.2336}},
}

@article{sober2012vocal,
  title = {Vocal Learning Is Constrained by the Statistics of Sensorimotor Experience},
  author = {Sober, Samuel J and Brainard, Michael S},
  year = {2012},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {51},
  pages = {21099--21103},
  publisher = {{National Acad Sciences}},
	note = {\url{https://doi.org/10.1073/pnas.1213622109}},
}

@article{soberCentralContributionsAcoustic2008,
  title = {Central {{Contributions}} to {{Acoustic Variation}} in {{Birdsong}}},
  author = {Sober, S. J. and Wohlgemuth, M. J. and Brainard, M. S.},
  year = {2008},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {28},
  number = {41},
  pages = {10370--10379},
  issn = {0270-6474, 1529-2401},
	note = {\url{https://doi.org/10.1523/JNEUROSCI.2448-08.2008}},
  urldate = {2021-11-27},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/RQM7ELQB/Sober et al. - 2008 - Central Contributions to Acoustic Variation in Bir.pdf}
}

@article{steinfathFastAccurateAnnotation,
  title = {Fast and Accurate Annotation of Acoustic Signals with Deep Neural Networks},
  author = {Steinfath, Elsa and Palacios, Adrian and Rottsch{\"a}fer, Julian and Yuezak, Deniz and Clemens, Jan},
  pages = {30},
  abstract = {Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, fast.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/T7IJV8E9/Steinfath et al. - Fast and accurate annotation of acoustic signals w.pdf}
}

@article{steinfathFastAccurateAnnotation2021,
  title = {Fast and Accurate Annotation of Acoustic Signals with Deep Neural Networks},
  author = {Steinfath, Elsa and {Palacios-Mu{\~n}oz}, Adrian and Rottsch{\"a}fer, Julian R and Yuezak, Deniz and Clemens, Jan},
  editor = {Calabrese, Ronald L and Egnor, SE Roian and Troyer, Todd},
  year = {2021},
  month = nov,
  journal = {eLife},
  volume = {10},
  pages = {e68837},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
	note = {\url{https://doi.org/10.7554/eLife.68837}},
  urldate = {2023-03-09},
  abstract = {Acoustic signals serve communication within and across species throughout the animal kingdom. Studying the genetics, evolution, and neurobiology of acoustic communication requires annotating acoustic signals: segmenting and identifying individual acoustic elements like syllables or sound pulses. To be useful, annotations need to be accurate, robust to noise, and fast. We here introduce DeepAudioSegmenter (DAS), a method that annotates acoustic signals across species based on a deep-learning derived hierarchical presentation of sound. We demonstrate the accuracy, robustness, and speed of DAS using acoustic signals with diverse characteristics from insects, birds, and mammals. DAS comes with a graphical user interface for annotating song, training the network, and for generating and proofreading annotations. The method can be trained to annotate signals from new species with little manual annotation and can be combined with unsupervised methods to discover novel signal types. DAS annotates song with high throughput and low latency for experimental interventions in realtime. Overall, DAS is a universal, versatile, and accessible tool for annotating acoustic communication signals.},
  keywords = {acoustic communication,annotation,bird,deep learning,fly,song},
  file = {/Users/davidnicholson/Zotero/storage/5BMYZ5SB/Steinfath et al. - 2021 - Fast and accurate annotation of acoustic signals w.pdf}
}

@article{stowellComputationalBioacousticsDeep2022,
  title = {Computational Bioacoustics with Deep Learning: A Review and Roadmap},
  author = {Stowell, Dan},
  year = {2022},
  pages = {46},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/PAAGY2CU/Stowell - 2022 - Computational bioacoustics with deep learning a r.pdf}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-03},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/davidnicholson/Zotero/storage/TYJ6WDQX/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@article{tachibana2014semi,
  title = {Semi-Automatic Classification of Birdsong Elements Using a Linear Support Vector Machine},
  author = {Tachibana, Ryosuke O and Oosugi, Naoya and Okanoya, Kazuo},
  year = {2014},
  journal = {PloS one},
  volume = {9},
  number = {3},
  pages = {e92584},
  publisher = {{Public Library of Science}},
	note = {\url{https://doi.org/10.1371/journal.pone.0092584}},
}

@article{tchernichovskiProcedureAutomatedMeasurement2000,
  title = {A Procedure for an Automated Measurement of Song Similarity},
  author = {Tchernichovski, Ofer and Nottebohm, Fernando and Ho, Ching Elizabeth and Pesaran, Bijan and Mitra, Partha Pratim},
  year = {2000},
  journal = {Animal behaviour},
  volume = {59},
  number = {6},
  pages = {1167--1176},
  file = {/Users/davidnicholson/Zotero/storage/XQCXXYS7/Tchernichovski et al. - 2000 - A procedure for an automated measurement of song s.pdf;/Users/davidnicholson/Zotero/storage/79PFTB3I/S0003347299914161.html},
	note = {\url{https://doi.org/10.1006/anbe.1999.1416}},
}

@misc{team_pandas-devpandas_2020,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {pandas development {team}, The},
  year = {2020},
  month = feb,
  publisher = {{Zenodo}},
	note = {\url{https://doi.org/10.5281/zenodo.3509134}},
}

@misc{thomas_a_caswell_2020_4030140,
  title = {Matplotlib/Matplotlib: {{REL}}: V3.3.2},
  author = {Caswell, Thomas A and Droettboom, Michael and Lee, Antony and Hunter, John and {de Andrade}, Elliott Sales and Firing, Eric and Hoffmann, Tim and Klymak, Jody and Stansby, David and Varoquaux, Nelle and Nielsen, Jens Hedegaard and Root, Benjamin and May, Ryan and Elson, Phil and Sepp{\"a}nen, Jouni K. and Dale, Darren and Lee, Jae-Joon and McDougall, Damon and Straw, Andrew and Hobson, Paul and Gohlke, Christoph and Yu, Tony S and Ma, Eric and Vincent, Adrien F. and Silvester, Steven and Moad, Charlie and Kniazev, Nikita and {hannah} and Ernest, Elan and Ivanov, Paul},
  year = {2020},
  month = sep,
	note = {\url{https://doi.org/10.5281/zenodo.4030140}},
  howpublished = {Zenodo}
}

@misc{torchvision2016,
  title = {{{TorchVision}}: {{PyTorch}}'s Computer Vision Library},
  author = {{maintainers}, TorchVision and {contributors}},
  year = {2016},
  howpublished = {GitHub}
}

@incollection{trouvainCanarySongDecoder2021,
  title = {Canary {{Song Decoder}}: {{Transduction}} and {{Implicit Segmentation}} with {{ESNs}} and {{LTSMs}}},
  shorttitle = {Canary {{Song Decoder}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2021},
  author = {Trouvain, Nathan and Hinaut, Xavier},
  editor = {Farka{\v s}, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
  year = {2021},
  volume = {12895},
  pages = {71--82},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
	note = {\url{https://doi.org/10.1007/978-3-030-86383-8_6}},
  urldate = {2023-01-18},
  abstract = {Domestic canaries produce complex vocal patterns embedded in various levels of abstraction. Studying such temporal organization is of particular relevance to understand how animal brains represent and process vocal inputs such as language. However, this requires a large amount of annotated data. We propose a fast and easy-to-train transducer model based on RNN architectures to automate parts of the annotation process. This is similar to a speech recognition task. We demonstrate that RNN architectures can be efficiently applied on spectral features (MFCC) to annotate songs at time frame level and at phrase level. We achieved around 95\% accuracy at frame level on particularly complex canary songs, and ESNs achieved around 5\% of word error rate (WER) at phrase level. Moreover, we are able to build this model using only around 13 to 20 minutes of annotated songs. Training time takes only 35 seconds using 2 hours and 40 minutes of data for the ESN, allowing to quickly run experiments without the need of powerful hardware.},
  isbn = {978-3-030-86382-1 978-3-030-86383-8},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/9BD2MEDL/Trouvain and Hinaut - 2021 - Canary Song Decoder Transduction and Implicit Seg.pdf}
}

@article{vernesMultidimensionalNatureVocal2021,
  title = {The Multi-Dimensional Nature of Vocal Learning},
  author = {Vernes, Sonja C. and Kriengwatana, Buddhamas Pralle and Beeck, Veronika C. and Fischer, Julia and Tyack, Peter L. and {ten Cate}, Carel and Janik, Vincent M.},
  year = {2021},
  month = oct,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {376},
  number = {1836},
  pages = {20200236},
  publisher = {{Royal Society}},
	note = {\url{https://doi.org/10.1098/rstb.2020.0236}},
  urldate = {2021-11-05},
  abstract = {How learning affects vocalizations is a key question in the study of animal communication and human language. Parallel efforts in birds and humans have taught us much about how vocal learning works on a behavioural and neurobiological level. Subsequent efforts have revealed a variety of cases among mammals in which experience also has a major influence on vocal repertoires. Janik and Slater (Anim. Behav.60, 1\textendash 11. (doi:10.1006/anbe.2000.1410)) introduced the distinction between vocal usage and production learning, providing a general framework to categorize how different types of learning influence vocalizations. This idea was built on by Petkov and Jarvis (Front. Evol. Neurosci.4, 12. (doi:10.3389/fnevo.2012.00012)) to emphasize a more continuous distribution between limited and more complex vocal production learners. Yet, with more studies providing empirical data, the limits of the initial frameworks become apparent. We build on these frameworks to refine the categorization of vocal learning in light of advances made since their publication and widespread agreement that vocal learning is not a binary trait. We propose a novel classification system, based on the definitions by Janik and Slater, that deconstructs vocal learning into key dimensions to aid in understanding the mechanisms involved in this complex behaviour. We consider how vocalizations can change without learning, and a usage learning framework that considers context specificity and timing. We identify dimensions of vocal production learning, including the copying of auditory models (convergence/divergence on model sounds, accuracy of copying), the degree of change (type and breadth of learning) and timing (when learning takes place, the length of time it takes and how long it is retained). We consider grey areas of classification and current mechanistic understanding of these behaviours. Our framework identifies research needs and will help to inform neurobiological and evolutionary studies endeavouring to uncover the multi-dimensional nature of vocal learning.This article is part of the theme issue `Vocal learning in animals and humans'.},
  keywords = {behaviour,cognition,evolution,language,songbird,vocal learning},
  file = {/Users/davidnicholson/Zotero/storage/A92W6X75/Vernes et al. - 2021 - The multi-dimensional nature of vocal learning.pdf}
}

@article{virtanen_scipy_2019,
  title = {{{SciPy}} 1.0\textendash{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10121 [physics]},
  eprint = {1907.10121},
  primaryclass = {physics},
  urldate = {2019-08-17},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics}
}

@article{walt_numpy_2011,
  title = {The {{NumPy Array}}: {{A Structure}} for {{Efficient Numerical Computation}}},
  shorttitle = {The {{NumPy Array}}},
  author = {van der Walt, S. and Colbert, S. C. and Varoquaux, G.},
  year = {2011},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {13},
  number = {2},
  pages = {22--30},
  issn = {1521-9615},
	note = {\url{https://doi.org/10.1109/MCSE.2011.37}},
  abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
  keywords = {Arrays,Computational efficiency,data structures,Finite element methods,high level language,high level languages,mathematics computing,numerical analysis,Numerical analysis,numerical computation,numerical computations,numerical data,NumPy,numpy array,Performance evaluation,programming libraries,Python,Python programming language,Resource management,scientific programming,Vector quantization}
}

@article{wirthlinModularApproachVocal2019,
  title = {A {{Modular Approach}} to {{Vocal Learning}}: {{Disentangling}} the {{Diversity}} of a {{Complex Behavioral Trait}}},
  shorttitle = {A {{Modular Approach}} to {{Vocal Learning}}},
  author = {Wirthlin, Morgan and Chang, Edward F. and Kn{\"o}rnschild, Mirjam and Krubitzer, Leah A. and Mello, Claudio V. and Miller, Cory T. and Pfenning, Andreas R. and Vernes, Sonja C. and Tchernichovski, Ofer and Yartsev, Michael M.},
  year = {2019},
  month = oct,
  journal = {Neuron},
  volume = {104},
  number = {1},
  pages = {87--99},
  issn = {08966273},
	note = {\url{https://doi.org/10.1016/j.neuron.2019.09.036}},
  urldate = {2021-05-22},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/L7JW7EGY/Wirthlin et al. - 2019 - A Modular Approach to Vocal Learning Disentanglin.pdf}
}

@techreport{zandbergBirdSongComparison2022,
  type = {Preprint},
  title = {Bird Song Comparison Using Deep Learning Trained from Avian Perceptual Judgments},
  author = {Zandberg, Lies and Morfi, Veronica and George, Julia and Clayton, David F. and Stowell, Dan and Lachlan, Robert F.},
  year = {2022},
  month = dec,
  institution = {{Animal Behavior and Cognition}},
	note = {\url{https://doi.org/10.1101/2022.12.23.521425}},
  urldate = {2023-01-05},
  abstract = {Abstract                        Our understanding of bird song, a model system for animal communication and the neurobiology of learning, depends critically on making reliable, validated comparisons between the complex multidimensional syllables that are used in songs. However, most assessments of song similarity are based on human inspection of spectrograms, or computational methods developed from human intuitions. Using a novel automated operant conditioning system, we collected a large corpus of zebra finches' (             Taeniopygia guttata             ) decisions about song syllable similarity. We use this dataset to compare and externally validate similarity algorithms in widely-used publicly available software (Raven, Sound Analysis Pro, Luscinia). Although these methods all perform better than chance, they do not closely emulate the avian assessments. We then introduce a novel deep learning method that can produce perceptual similarity judgements trained on such avian decisions. We find that this new method outperforms the established methods in accuracy and more closely approaches the avian assessments. Inconsistent (hence ambiguous) decisions are a common occurrence in animal behavioural data; we show that a modification of the deep learning training that accommodates these leads to the strongest performance. We argue this approach is the best way to validate methods to compare song similarity, that our dataset can be used to validate novel methods, and that the general approach can easily be extended to other species.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/ALVHUX8W/Zandberg et al. - 2022 - Bird song comparison using deep learning trained f.pdf}
}

@inproceedings{lea2017temporal,
  title={Temporal convolutional networks for action segmentation and detection},
  author={Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={156--165},
  year={2017},
	note = {\url{https://doi.org/10.1109/cvpr.2017.113}},
}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@article{sainburg2021parametric,
  title={Parametric UMAP embeddings for representation and semisupervised learning},
  author={Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q},
  journal={Neural Computation},
  volume={33},
  number={11},
  pages={2881--2907},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~},
	note = {\url{https://doi.org/10.1162/neco_a_01434}},
}

@article{nicholson2023crowsetta,
  title={Crowsetta: A Python tool to work with any format for annotating animal vocalizations and bioacoustics data.},
  author={Nicholson, David},
  journal={Journal of Open Source Software},
  volume={8},
  number={84},
  pages={5338},
  year={2023},
	note = {\url{https://doi.org/10.21105/joss.05338}},
}

@misc{nicholson_vocalpyvocalpy_2023,
	title = {vocalpy/vocalpy: 0.3.0},
	shorttitle = {vocalpy/vocalpy},
	url = {https://zenodo.org/record/7925888},
	abstract = {What's Changed Add initial plot module by @NickleDave in https://github.com/vocalpy/vocalpy/pull/35 Add MVP of SequenceDataset saving to / loading from SQLite by @NickleDave in https://github.com/vocalpy/vocalpy/pull/36 Full Changelog: https://github.com/vocalpy/vocalpy/compare/0.2.0...0.3.0},
	urldate = {2023-07-16},
	publisher = {Zenodo},
	author = {Nicholson, David},
	month = may,
	year = {2023},
	note = {\url{https://doi.org/10.5281/zenodo.7925888}},
	file = {Zenodo Snapshot:/home/pimienta/Zotero/storage/T8GR265Q/7925888.html:text/html},
}

@article{cohen2022recent,
  title={Recent Advances at the Interface of Neuroscience and Artificial Neural Networks},
  author={Cohen, Yarden and Engel, Tatiana A and Langdon, Christopher and Lindsay, Grace W and Ott, Torben and Peters, Megan AK and Shine, James M and Breton-Provencher, Vincent and Ramaswamy, Srikanth},
  journal={Journal of Neuroscience},
  volume={42},
  number={45},
  pages={8514--8523},
  year={2022},
  publisher={Soc Neuroscience}
}