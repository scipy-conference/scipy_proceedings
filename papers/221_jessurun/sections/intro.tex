\section{Introduction}
Labeled image data is essential for tuning and evaluating the performance of machine learning applications.
Such labels are typically defined with approximate enclosing shapes (i.e. simple polygons or parametric shapes), which tend to misrepresent more complex components.

Alternatively, semantic segmentation is a technique providing pixel-level accuracy which avoids poor foreground representation~\cite{chengSurveyAnalysisAutomatic2018}.
One such field applying this method include Bill-of-Material (BoM) extraction. This BoM, or list of all surface-mount devices (SMDs) on a printed circuit board (PCB) surface, can be generated from optical images of a board under test. Next, it can be compared against a reference design to detect likely counterfeit, tampered, or defective SMDs~\cite{paradis2020color,azhaganReviewAutomaticBill2019}. To create such a BoM using optical data alone, it is crucial that detected SMDs use segmentation masks that are representative of structures such as pins, solder flow, and more. This is the only way the collected data will remain indicative of true BoM properties. An example annotation is shown in \autoref{fig:pcb}

\makePcbFig

Beyond PCB analysis, semantic segmentation is important in several other domains including quality control during manufacturing \cite{fergusonDetectionSegmentationManufacturing2018,anagnostopoulosComputerVisionApproach2001,anagnostopoulosHighPerformanceComputing2002}, manuscript restoration / digitization \cite{gatosSegmentationfreeRecognitionTechnique2004,kesimanNewSchemeText2016,jainTextSegmentationUsing1992,taxtSegmentationDocumentImages1989,fujisawaSegmentationMethodsCharacter1992}, and effective patient diagnosis \cite{seifertSemanticAnnotationMedical2010,rajchlDeepCutObjectSegmentation2017,yushkevichUserguided3DActive2006,iakovidisRatsnakeVersatileImage2014}.
In all these cases, imprecise annotations severely limit the development of automated solutions and can decrease the accuracy of standard trained segmentation models.

Quality semantic segmentation is difficult due to a reliance on large, high-quality datasets, which are often created by manually labeling each image.
Manual annotation is error-prone, costly, and greatly hinders scalability.
Several tools have been proposed to alleviate the burden of collecting ground-truth labels for semantic segmentation~\cite{BestImageAnnotation}.
% Unfortunately, existing tools are heavily biased toward lower-resolution images with few regions of interest (ROI) (\autoref{fig:sampleSegData}).
% While this may not be an issue for some datasets, such assumptions are \emph{crippling} for high-fidelity images with hundreds of annotated ROIs (\autoref{fig:bees}) \cite{Ladicky_whatWhereCombiningCRFs,Wang_multiLabelImageAnnotation}.

% \makeSampleSegFig
% \makeBeesFig

S3A circumvents these pitfalls by providing deep customization of the annotation workflow at both a granular and high level.
Arbitrary Python functions can be seamlessly integrated into the processing workflow, and every algorithm developed within S3A can be accessed in a headless (i.e. automated or scripted) manner independent of the GUI.

In this paper, we present significant improvements to the fundamentals of S3A as previously introduced in \cite{jessurunComponentDetectionEvaluation2020}.
This includes live app-level property customization, real-time algorithm modification, and feedback, region prediction assistance, constrained component table editing based on allowed data types, a multitude of data export formats, and a highly adaptable set of plugin interfaces for application-specific extensions to S3A.
We also note the potential impact on the image processing community improving labeling speed, accuracy, and multiple insertion points for algorithm feedback.
Beyond software improvements, these features play significant roles in bridging the gap between human annotation efforts and scalable, automated segmentation methods \cite{Branson_humansInLoop}.