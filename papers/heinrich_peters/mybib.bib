
@misc{paszke_pytorch_2019,
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	shorttitle = {{PyTorch}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. {PyTorch} is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as {GPUs}. In this paper, we detail the principles that drove the implementation of {PyTorch} and how they are reflected in its architecture. We emphasize that every aspect of {PyTorch} is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of {PyTorch} on several common benchmarks.},
	number = {{arXiv}:1912.01703},
	publisher = {{arXiv}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and {DeVito}, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	urldate = {2023-04-26},
	date = {2019-12-03},
	eprinttype = {arxiv},
	eprint = {1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/28FXZJP3/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/MYXX8EFR/1912.html:text/html},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: A system for large-scale machine learning},
	abstract = {{TensorFlow} is a machine learning system that operates at large scale and in heterogeneous environments. {TensorFlow} uses dataﬂow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataﬂow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore {CPUs}, generalpurpose {GPUs}, and custom-designed {ASICs} known as Tensor Processing Units ({TPUs}). This architecture gives ﬂexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, {TensorFlow} enables developers to experiment with novel optimizations and training algorithms. {TensorFlow} supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use {TensorFlow} in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the {TensorFlow} dataﬂow model and demonstrate the compelling performance that {TensorFlow} achieves for several real-world applications.},
	author = {Abadi, Martın and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	date = {2016},
	langid = {english},
	file = {Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:/Users/heinrichpeters/Zotero/storage/3IAC56CM/Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@misc{olston_tensorflow-serving_2017,
	title = {{TensorFlow}-Serving: Flexible, High-Performance {ML} Serving},
	url = {http://arxiv.org/abs/1712.06139},
	doi = {10.48550/arXiv.1712.06139},
	shorttitle = {{TensorFlow}-Serving},
	abstract = {We describe {TensorFlow}-Serving, a system to serve machine learning models inside Google which is also available in the cloud and via open-source. It is extremely flexible in terms of the types of {ML} platforms it supports, and ways to integrate with systems that convey new models and updated versions from training to serving. At the same time, the core code paths around model lookup and inference have been carefully optimized to avoid performance pitfalls observed in naive implementations. Google uses it in many production deployments, including a multi-tenant model hosting service called {TFS}{\textasciicircum}2.},
	number = {{arXiv}:1712.06139},
	publisher = {{arXiv}},
	author = {Olston, Christopher and Fiedel, Noah and Gorovoy, Kiril and Harmsen, Jeremiah and Lao, Li and Li, Fangwei and Rajashekhar, Vinu and Ramesh, Sukriti and Soyke, Jordan},
	urldate = {2023-04-26},
	date = {2017-12-27},
	eprinttype = {arxiv},
	eprint = {1712.06139 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/KNAZMEP4/Olston et al. - 2017 - TensorFlow-Serving Flexible, High-Performance ML .pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/BS842RQS/1712.html:text/html},
}

@article{zaharia_accelerating_2018,
	title = {Accelerating the Machine Learning Lifecycle with {MLflow}},
	url = {https://www.semanticscholar.org/paper/Accelerating-the-Machine-Learning-Lifecycle-with-Zaharia-Chen/b2e0b79e6f180af2e0e559f2b1faba66b2bd578a},
	abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an {ML} application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe {MLflow}, an open source platform we recently launched to streamline the machine learning lifecycle. {MLflow} covers three key challenges: experimentation, reproducibility, and model deployment, using generic {APIs} that work with any {ML} library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
	journaltitle = {{IEEE} Data Eng. Bull.},
	author = {Zaharia, M. and Chen, A. and Davidson, A. and Ghodsi, A. and Hong, S. and Konwinski, A. and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
	urldate = {2023-04-26},
	date = {2018},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/287BGYJS/Zaharia et al. - 2018 - Accelerating the Machine Learning Lifecycle with M.pdf:application/pdf},
}

@online{noauthor_mlflow_2023,
	title = {{MLflow}},
	url = {https://mlflow.org/},
	abstract = {An open source platform for the end-to-end machine learning lifecycle},
	titleaddon = {{MLflow}},
	urldate = {2023-04-26},
	date = {2023},
	langid = {english},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/ME8MLKXY/mlflow.org.html:text/html},
}

@article{zaharia_accelerating_nodate,
	title = {Accelerating the Machine Learning Lifecycle with {MLﬂow}},
	abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an {ML} application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe {MLﬂow}, an open source platform we recently launched to streamline the machine learning lifecycle. {MLﬂow} covers three key challenges: experimentation, reproducibility, and model deployment, using generic {APIs} that work with any {ML} library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
	author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
	langid = {english},
	file = {Zaharia et al. - Accelerating the Machine Learning Lifecycle with M.pdf:/Users/heinrichpeters/Zotero/storage/HDLZ3XG9/Zaharia et al. - Accelerating the Machine Learning Lifecycle with M.pdf:application/pdf},
}

@inproceedings{chen_developments_2020,
	location = {New York, {NY}, {USA}},
	title = {Developments in {MLflow}: A System to Accelerate the Machine Learning Lifecycle},
	isbn = {978-1-4503-8023-2},
	url = {https://doi.org/10.1145/3399579.3399867},
	doi = {10.1145/3399579.3399867},
	series = {{DEEM}'20},
	shorttitle = {Developments in {MLflow}},
	abstract = {{MLflow} is a popular open source platform for managing {ML} development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since {MLflow} was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying {ML} code instrumentation, and experiment analytics functions for extracting insights from millions of {ML} experiments.},
	pages = {1--4},
	booktitle = {Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning},
	publisher = {Association for Computing Machinery},
	author = {Chen, Andrew and Chow, Andy and Davidson, Aaron and {DCunha}, Arjun and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Mewald, Clemens and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Singh, Avesh and Xie, Fen and Zaharia, Matei and Zang, Richard and Zheng, Juntai and Zumar, Corey},
	urldate = {2023-04-26},
	date = {2020-06-17},
}

@online{noauthor_hugging_2023,
	title = {Hugging Face Hub},
	url = {https://huggingface.co/docs/hub/index},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-04-26},
	date = {2023},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/IPXCRLE2/index.html:text/html},
}

@online{noauthor_tensorflow_2023,
	title = {{TensorFlow} Hub},
	url = {https://www.tensorflow.org/hub},
	abstract = {{TensorFlow} Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. Reuse trained models like {BERT} and Faster R-{CNN} with just a few lines of code.},
	titleaddon = {{TensorFlow}},
	urldate = {2023-04-26},
	date = {2023},
	langid = {english},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/YMKKA3L3/hub.html:text/html},
}

@misc{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771},
	doi = {10.48550/arXiv.1910.03771},
	shorttitle = {{HuggingFace}'s Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	number = {{arXiv}:1910.03771},
	publisher = {{arXiv}},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	urldate = {2023-04-26},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {1910.03771 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/6JYGW8XW/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/LX74PAWL/1910.html:text/html},
}

@online{siegel_models_2022,
	title = {Models Are Rarely Deployed: An Industry-wide Failure in Machine Learning Leadership},
	url = {https://www.kdnuggets.com/models-are-rarely-deployed-an-industry-wide-failure-in-machine-learning-leadership.html},
	shorttitle = {Models Are Rarely Deployed},
	abstract = {In this article, Eric Siegel summarizes the recent {KDnuggets} poll results and argues that the pervasive failure of {ML} projects comes from a lack of prudent leadership. He also argues that {MLops} is not the fundamental missing ingredient – instead, an effective {ML} leadership practice must be the dog that…},
	titleaddon = {{KDnuggets}},
	author = {Siegel, Eric},
	urldate = {2023-04-26},
	date = {2022},
	langid = {american},
	note = {Section: {KDnuggets} Originals},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/QVBCEM9Y/models-rarely-deployed-industrywide-failure-machine-learning-leadership.html:text/html},
}

@inproceedings{derakhshan_optimizing_2020,
	location = {New York, {NY}, {USA}},
	title = {Optimizing Machine Learning Workloads in Collaborative Environments},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3389715},
	doi = {10.1145/3318464.3389715},
	series = {{SIGMOD} '20},
	abstract = {Effective collaboration among data scientists results in high-quality and efficient machine learning ({ML}) workloads. In a collaborative environment, such as Kaggle or Google Colabratory, users typically re-execute or modify published scripts to recreate or improve the result. This introduces many redundant data processing and model training operations. Reusing the data generated by the redundant operations leads to the more efficient execution of future workloads. However, existing collaborative environments lack a data management component for storing and reusing the result of previously executed operations. In this paper, we present a system to optimize the execution of {ML} workloads in collaborative environments by reusing previously performed operations and their results. We utilize a so-called Experiment Graph ({EG}) to store the artifacts, i.e., raw and intermediate data or {ML} models, as vertices and operations of {ML} workloads as edges. In theory, the size of {EG} can become unnecessarily large, while the storage budget might be limited. At the same time, for some artifacts, the overall storage and retrieval cost might outweigh the recomputation cost. To address this issue, we propose two algorithms for materializing artifacts based on their likelihood of future reuse. Given the materialized artifacts inside {EG}, we devise a linear-time reuse algorithm to find the optimal execution plan for incoming {ML} workloads. Our reuse algorithm only incurs a negligible overhead and scales for the high number of incoming {ML} workloads in collaborative environments. Our experiments show that we improve the run-time by one order of magnitude for repeated execution of the workloads and 50\% for the execution of modified workloads in collaborative environments.},
	pages = {1701--1716},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} International Conference on Management of Data},
	publisher = {Association for Computing Machinery},
	author = {Derakhshan, Behrouz and Rezaei Mahdiraji, Alireza and Abedjan, Ziawasch and Rabl, Tilmann and Markl, Volker},
	urldate = {2023-04-26},
	date = {2020-05-31},
	keywords = {collaborative {ML}, machine learning, materialization and reuse},
}

@article{salganik_measuring_2020,
	title = {Measuring the predictability of life outcomes with a scientific mass collaboration},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1915006117},
	doi = {10.1073/pnas.1915006117},
	abstract = {How predictable are life trajectories? We investigated this question with a scientific mass collaboration using the common task method; 160 teams built predictive models for six life outcomes using data from the Fragile Families and Child Wellbeing Study, a high-quality birth cohort study. Despite using a rich dataset and applying machine-learning methods optimized for prediction, the best predictions were not very accurate and were only slightly better than those from a simple benchmark model. Within each outcome, prediction error was strongly associated with the family being predicted and weakly associated with the technique used to generate the prediction. Overall, these results suggest practical limits to the predictability of life outcomes in some settings and illustrate the value of mass collaborations in the social sciences.},
	pages = {8398--8403},
	number = {15},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Salganik, Matthew J. and Lundberg, Ian and Kindel, Alexander T. and Ahearn, Caitlin E. and Al-Ghoneim, Khaled and Almaatouq, Abdullah and Altschul, Drew M. and Brand, Jennie E. and Carnegie, Nicole Bohme and Compton, Ryan James and Datta, Debanjan and Davidson, Thomas and Filippova, Anna and Gilroy, Connor and Goode, Brian J. and Jahani, Eaman and Kashyap, Ridhi and Kirchner, Antje and {McKay}, Stephen and Morgan, Allison C. and Pentland, Alex and Polimis, Kivan and Raes, Louis and Rigobon, Daniel E. and Roberts, Claudia V. and Stanescu, Diana M. and Suhara, Yoshihiko and Usmani, Adaner and Wang, Erik H. and Adem, Muna and Alhajri, Abdulla and {AlShebli}, Bedoor and Amin, Redwane and Amos, Ryan B. and Argyle, Lisa P. and Baer-Bositis, Livia and Büchi, Moritz and Chung, Bo-Ryehn and Eggert, William and Faletto, Gregory and Fan, Zhilin and Freese, Jeremy and Gadgil, Tejomay and Gagné, Josh and Gao, Yue and Halpern-Manners, Andrew and Hashim, Sonia P. and Hausen, Sonia and He, Guanhua and Higuera, Kimberly and Hogan, Bernie and Horwitz, Ilana M. and Hummel, Lisa M. and Jain, Naman and Jin, Kun and Jurgens, David and Kaminski, Patrick and Karapetyan, Areg and Kim, E. H. and Leizman, Ben and Liu, Naijia and Möser, Malte and Mack, Andrew E. and Mahajan, Mayank and Mandell, Noah and Marahrens, Helge and Mercado-Garcia, Diana and Mocz, Viola and Mueller-Gastell, Katariina and Musse, Ahmed and Niu, Qiankun and Nowak, William and Omidvar, Hamidreza and Or, Andrew and Ouyang, Karen and Pinto, Katy M. and Porter, Ethan and Porter, Kristin E. and Qian, Crystal and Rauf, Tamkinat and Sargsyan, Anahit and Schaffner, Thomas and Schnabel, Landon and Schonfeld, Bryan and Sender, Ben and Tang, Jonathan D. and Tsurkov, Emma and van Loon, Austin and Varol, Onur and Wang, Xiafei and Wang, Zhi and Wang, Julia and Wang, Flora and Weissman, Samantha and Whitaker, Kirstie and Wolters, Maria K. and Woon, Wei Lee and Wu, James and Wu, Catherine and Yang, Kengran and Yin, Jingwen and Zhao, Bingyu and Zhu, Chenyun and Brooks-Gunn, Jeanne and Engelhardt, Barbara E. and Hardt, Moritz and Knox, Dean and Levy, Karen and Narayanan, Arvind and Stewart, Brandon M. and Watts, Duncan J. and {McLanahan}, Sara},
	urldate = {2023-04-26},
	date = {2020-04-14},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/YLXP3NK3/Salganik et al. - 2020 - Measuring the predictability of life outcomes with.pdf:application/pdf},
}

@article{feurer_practical_2018,
	title = {Practical Automated Machine Learning for the {AutoML} Challenge 2018},
	abstract = {Despite great successes in many ﬁelds, machine learning typically requires substantial human resources to determine a good machine learning pipeline (including various types of preprocessing, and the choice of classiﬁers and hyperparameters). {AutoML} aims to free human practitioners and researchers from these menial tasks. The current state-of-the-art in {AutoML} has been evaluated in the {AutoML} challenge 2018. Here, we describe our winning entry to this challenge, dubbed {PoSH} Auto-sklearn, which combines an automatically preselected portfolio, ensemble building and Bayesian optimization with successive halving. Finally, we share insights in the importance of diﬀerent parts of our approach.},
	pages = {12},
	author = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
	date = {2018},
	langid = {english},
	file = {Feurer et al. - Practical Automated Machine Learning for the AutoM.pdf:/Users/heinrichpeters/Zotero/storage/LWMWMC9T/Feurer et al. - Practical Automated Machine Learning for the AutoM.pdf:application/pdf},
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
	shorttitle = {Scikit-learn},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and {API} consistency.  It has minimal dependencies and is distributed under the simplified {BSD} license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	pages = {2825-2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	urldate = {2019-09-08},
	date = {2011-10},
	file = {Fulltext PDF:/Users/heinrichpeters/Zotero/storage/FK276XUL/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@collection{hutter_automated_2019,
	title = {Automated Machine Learning: Methods, Systems, Challenges},
	isbn = {978-3-030-05317-8},
	url = {https://www.springer.com/gp/book/9783030053178},
	series = {The Springer Series on Challenges in Machine Learning},
	shorttitle = {Automated Machine Learning},
	abstract = {This open access book presents the first comprehensive overview of general methods in Automated Machine Learning ({AutoML}), collects descriptions of existing systems based on these methods, and discusses the first series of international challenges of {AutoML} systems. The recent success of commercial {ML} applications and the rapid growth of the field has created a high demand for off-the-shelf {ML} methods that can be used easily and without expert knowledge. However, many of the recent machine learning successes crucially rely on human experts, who manually select appropriate {ML} architectures (deep learning architectures or more traditional {ML} workflows) and their hyperparameters. To overcome this problem, the field of {AutoML} targets a progressive automation of machine learning, based on principles from optimization and machine learning itself. This book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use {AutoML} in their work.},
	publisher = {Springer International Publishing},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	urldate = {2019-09-01},
	date = {2019},
	langid = {english},
	file = {automl_book.pdf:/Users/heinrichpeters/Zotero/storage/M7SHGMZP/automl_book.pdf:application/pdf;Snapshot:/Users/heinrichpeters/Zotero/storage/SWZZUM8F/9783030053178.html:text/html},
}

@incollection{feurer_efficient_2015,
	title = {Efficient and Robust Automated Machine Learning},
	url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
	pages = {2962--2970},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2019-08-26},
	date = {2015},
	file = {NIPS Full Text PDF:/Users/heinrichpeters/Zotero/storage/UFGQHSUZ/Feurer et al. - 2015 - Efficient and Robust Automated Machine Learning.pdf:application/pdf;NIPS Snapshot:/Users/heinrichpeters/Zotero/storage/ARZJ2JFL/5872-efficient-and-robust-automated-machine-learning.html:text/html},
}

@article{chen_techniques_2019,
	title = {Techniques for Automated Machine Learning},
	url = {http://arxiv.org/abs/1907.08908},
	abstract = {Automated machine learning ({AutoML}) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of {AutoML} in terms of three categories, automated feature engineering ({AutoFE}), automated model and hyperparameter learning ({AutoMHL}), and automated deep learning ({AutoDL}). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular {AutoML} frameworks and conclude with current open challenges of {AutoML}.},
	journaltitle = {{arXiv}:1907.08908 [cs, stat]},
	author = {Chen, Yi-Wei and Song, Qingquan and Hu, Xia},
	urldate = {2019-08-26},
	date = {2019-07-21},
	eprinttype = {arxiv},
	eprint = {1907.08908},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1907.08908 PDF:/Users/heinrichpeters/Zotero/storage/IPKM4RQX/Chen et al. - 2019 - Techniques for Automated Machine Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/XURMFRBM/1907.html:text/html},
}

@inproceedings{van_rijn_openml_2013,
	location = {Berlin, Heidelberg},
	title = {{OpenML}: A Collaborative Science Platform},
	isbn = {978-3-642-40994-3},
	doi = {10.1007/978-3-642-40994-3_46},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{OpenML}},
	abstract = {We present {OpenML}, a novel open science platform that provides easy access to machine learning data, software and results to encourage further study and application. It organizes all submitted results online so they can be easily found and reused, and features a web {API} which is being integrated in popular machine learning tools such as Weka, {KNIME}, {RapidMiner} and R packages, so that experiments can be shared easily.},
	pages = {645--649},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer},
	author = {van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis and Gao, Bo and Umaashankar, Venkatesh and Fischer, Simon and Winter, Patrick and Wiswedel, Bernd and Berthold, Michael R. and Vanschoren, Joaquin},
	editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Železný, Filip},
	date = {2013},
	langid = {english},
	keywords = {Machine Learning, Databases, Experimental Methodology, Meta-Learning},
	file = {Springer Full Text PDF:/Users/heinrichpeters/Zotero/storage/KG9AXA9L/van Rijn et al. - 2013 - OpenML A Collaborative Science Platform.pdf:application/pdf},
}

@online{chollet_keras_2015,
	title = {Keras: the Python deep learning {API}},
	url = {https://keras.io/},
	author = {Chollet, Francois},
	urldate = {2020-08-21},
	date = {2015},
	file = {Keras\: the Python deep learning API:/Users/heinrichpeters/Zotero/storage/94YSICRG/keras.io.html:text/html},
}

@article{casalicchio_openml_2019,
	title = {{OpenML}: An R package to connect to the machine learning platform {OpenML}},
	volume = {34},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-017-0742-2},
	doi = {10.1007/s00180-017-0742-2},
	shorttitle = {{OpenML}},
	abstract = {{OpenML} is an online machine learning platform where researchers can easily share data, machine learning tasks and experiments as well as organize them online to work and collaborate more efficiently. In this paper, we present an R package to interface with the {OpenML} platform and illustrate its usage in combination with the machine learning R package mlr (Bischl et al. J Mach Learn Res 17(170):1–5, 2016). We show how the {OpenML} package allows R users to easily search, download and upload data sets and machine learning tasks. Furthermore, we also show how to upload results of experiments, share them with others and download results from other users. Beyond ensuring reproducibility of results, the {OpenML} platform automates much of the drudge work, speeds up research, facilitates collaboration and increases the users’ visibility online.},
	pages = {977--991},
	number = {3},
	journaltitle = {Computational Statistics},
	shortjournal = {Comput Stat},
	author = {Casalicchio, Giuseppe and Bossek, Jakob and Lang, Michel and Kirchhoff, Dominik and Kerschke, Pascal and Hofner, Benjamin and Seibold, Heidi and Vanschoren, Joaquin and Bischl, Bernd},
	urldate = {2022-05-09},
	date = {2019-09-01},
	langid = {english},
	keywords = {Machine learning, Databases, R, Reproducible research},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/CIMMK2YQ/Casalicchio et al. - 2019 - OpenML An R package to connect to the machine lea.pdf:application/pdf},
}

@article{vanschoren_openml_2014,
	title = {{OpenML}: networked science in machine learning},
	volume = {15},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/2641190.2641198},
	doi = {10.1145/2641190.2641198},
	shorttitle = {{OpenML}},
	abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce {OpenML}, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how {OpenML} relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
	pages = {49--60},
	number = {2},
	journaltitle = {{ACM} {SIGKDD} Explorations Newsletter},
	shortjournal = {{SIGKDD} Explor. Newsl.},
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	urldate = {2022-05-09},
	date = {2014-06-16},
	file = {Submitted Version:/Users/heinrichpeters/Zotero/storage/49Q3XUPE/Vanschoren et al. - 2014 - OpenML networked science in machine learning.pdf:application/pdf},
}

@misc{symeonidis_mlops_2022,
	title = {{MLOps} -- Definitions, Tools and Challenges},
	url = {http://arxiv.org/abs/2201.00162},
	abstract = {This paper is an overview of the Machine Learning Operations ({MLOps}) area. Our aim is to define the operation and the components of such systems by highlighting the current problems and trends. In this context, we present the different tools and their usefulness in order to provide the corresponding guidelines. Moreover, the connection between {MLOps} and {AutoML} (Automated Machine Learning) is identified and how this combination could work is proposed.},
	number = {{arXiv}:2201.00162},
	publisher = {{arXiv}},
	author = {Symeonidis, G. and Nerantzis, E. and Kazakis, A. and Papakostas, G. A.},
	urldate = {2022-12-01},
	date = {2022-01-01},
	eprinttype = {arxiv},
	eprint = {2201.00162 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, I.2},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/J3SL8PG8/Symeonidis et al. - 2022 - MLOps -- Definitions, Tools and Challenges.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/TMN7VTJ5/2201.html:text/html},
}

@article{feurer_openml-python_nodate,
	title = {{OpenML}-Python: an extensible Python {API} for {OpenML}},
	abstract = {{OpenML} is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper, we introduce {OpenML}-Python, a client {API} for Python, which opens up the {OpenML} platform for a wide range of Python-based machine learning tools. It provides easy access to all datasets, tasks and experiments on {OpenML} from within Python. It also provides functionality to conduct machine learning experiments, upload the results to {OpenML}, and reproduce results which are stored on {OpenML}. Furthermore, it comes with a scikit-learn extension and an extension mechanism to easily integrate other machine learning libraries written in Python into the {OpenML} ecosystem. Source code and documentation are available at https://github.com/openml/openml-python/.},
	author = {Feurer, Matthias},
	langid = {english},
	file = {Feurer - OpenML-Python an extensible Python API for OpenML.pdf:/Users/heinrichpeters/Zotero/storage/WM6524SZ/Feurer - OpenML-Python an extensible Python API for OpenML.pdf:application/pdf},
}

@article{feurer_openml-python_2021,
	title = {Openml-python: an extensible python api for openml},
	volume = {22},
	shorttitle = {Openml-python},
	pages = {4573--4577},
	number = {1},
	journaltitle = {The Journal of Machine Learning Research},
	author = {Feurer, Matthias and Van Rijn, Jan N. and Kadra, Arlind and Gijsbers, Pieter and Mallik, Neeratyoy and Ravi, Sahithya and Müller, Andreas and Vanschoren, Joaquin and Hutter, Frank},
	date = {2021},
	note = {Publisher: {JMLRORG}},
}

@online{gavrilova_best_2022,
	title = {The Best Open-Source {MLOps} Tools You Should Know},
	url = {https://neptune.ai/blog/best-open-source-mlops-tools},
	abstract = {You don’t need to spend a lot on {MLOps} tools to bring the magic of {DevOps} to your machine learning projects. There is plenty of open-source tools to choose from. It’s a good solution when you’re trying to address unique problems and a community to rely on is needed. But there are some prons to…},
	titleaddon = {neptune.ai},
	author = {Gavrilova, Yulia},
	urldate = {2023-03-31},
	date = {2022-07-22},
	langid = {american},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/M746LAU4/best-open-source-mlops-tools.html:text/html},
}

@online{rocco_4_2021,
	title = {The 4 Pillars of {MLOps}: How to Deploy {ML} Models to Production},
	url = {https://www.phdata.io/blog/the-ultimate-mlops-guide-how-to-deploy-ml-models-to-production/},
	shorttitle = {The 4 Pillars of {MLOps}},
	abstract = {Learn how to deploy models to production more effectively with this ultimate guide that explore {MLOps} and the 4 pillars of machine learning.},
	titleaddon = {{phData}},
	author = {Rocco, Dominick},
	urldate = {2023-03-29},
	date = {2021-05-21},
	langid = {american},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/INNCT396/the-ultimate-mlops-guide-how-to-deploy-ml-models-to-production.html:text/html},
}

@article{hewage_machine_2022,
	title = {Machine Learning Operations: A Survey on {MLOps} Tool Support},
	url = {http://arxiv.org/abs/2202.10169},
	doi = {10.48550/arXiv.2202.10169},
	shorttitle = {Machine Learning Operations},
	abstract = {Machine Learning ({ML}) has become a fast-growing, trending approach in solution development in practice. Deep Learning ({DL}) which is a subset of {ML}, learns using deep neural networks to simulate the human brain. It trains machines to learn techniques and processes individually using computer algorithms, which is also considered to be a role of Artificial Intelligence ({AI}). In this paper, we study current technical issues related to software development and delivery in organizations that work on {ML} projects. Therefore, the importance of the Machine Learning Operations ({MLOps}) concept, which can deliver appropriate solutions for such concerns, is discussed. We investigate commercially available {MLOps} tool support in software development. The comparison between {MLOps} tools analyzes the performance of each system and its use cases. Moreover, we examine the features and usability of {MLOps} tools to identify the most appropriate tool support for given scenarios. Finally, we recognize that there is a shortage in the availability of a fully functional {MLOps} platform on which processes can be automated by reducing human intervention.},
	author = {Hewage, Nipuni and Meedeniya, Dulani},
	urldate = {2023-03-20},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2202.10169 [cs]},
	keywords = {Computer Science - Software Engineering, A.1, D.2.6, K.6.3},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/I6V5CA5A/Hewage and Meedeniya - 2022 - Machine Learning Operations A Survey on MLOps Too.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/GDCUNPSB/2202.html:text/html},
}

@article{testi_mlops_2022,
	title = {{MLOps}: A Taxonomy and a Methodology},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3181730},
	shorttitle = {{MLOps}},
	abstract = {Over the past few decades, the substantial growth in enterprise-data availability and the advancements in Artificial Intelligence ({AI}) have allowed companies to solve real-world problems using Machine Learning ({ML}). {ML} Operations ({MLOps}) represents an effective strategy for bringing {ML} models from academic resources to useful tools for solving problems in the corporate world. The current literature on {MLOps} is still mostly disconnected and sporadic. In this work, we review the existing scientific literature and we propose a taxonomy for clustering research papers on {MLOps}. In addition, we present methodologies and operations aimed at defining an {ML} pipeline to simplify the release of {ML} applications in the industry. The pipeline is based on ten steps: business problem understanding, data acquisition, {ML} methodology, {ML} training \& testing, continuous integration, continuous delivery, continuous training, continuous monitoring, explainability, and sustainability. The scientific and business interest and the impact of {MLOps} have grown significantly over the past years: the definition of a clear and standardized methodology for conducting {MLOps} projects is the main contribution of this paper.},
	pages = {63606--63618},
	journaltitle = {{IEEE} Access},
	author = {Testi, Matteo and Ballabio, Matteo and Frontoni, Emanuele and Iannello, Giulio and Moccia, Sara and Soda, Paolo and Vessio, Gennaro},
	date = {2022},
	note = {Conference Name: {IEEE} Access},
	keywords = {Automation, Pipelines, Training, Business, sustainability, continuous delivery, continuous integration, continuous monitoring, continuous training, {MLOps}, Monitoring, Production, Surgery, {XAI}},
	file = {IEEE Xplore Abstract Record:/Users/heinrichpeters/Zotero/storage/CB79W7RI/9792270.html:text/html;IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/T9CWNISD/Testi et al. - 2022 - MLOps A Taxonomy and a Methodology.pdf:application/pdf},
}

@inproceedings{symeonidis_mlops_2022-1,
	title = {{MLOps} - Definitions, Tools and Challenges},
	doi = {10.1109/CCWC54503.2022.9720902},
	abstract = {This paper is an concentrated overview of the Machine Learning Operations ({MLOps}) area. Our aim is to define the operation and the components of such systems by highlighting the current problems and trends. In this context we present the different tools and their usefulness in order to provide the corresponding guidelines. Moreover, the connection between {MLOps} and {AutoML} (Automated Machine Learning) is identified and how this combination could work is proposed. The novelty of our approach relies on the combination of state-of-the-art topics such as {AutoML}, exlainability and sustain-ability in order to overcome the current challenges in {MLOps} identifying them not only as the answer for the incorporation of {ML} models in production but also as a possible tool for efficient, robust and accurate machine learning models.},
	eventtitle = {2022 {IEEE} 12th Annual Computing and Communication Workshop and Conference ({CCWC})},
	pages = {0453--0460},
	booktitle = {2022 {IEEE} 12th Annual Computing and Communication Workshop and Conference ({CCWC})},
	author = {Symeonidis, Georgios and Nerantzis, Evangelos and Kazakis, Apostolos and Papakostas, George A.},
	date = {2022-01},
	keywords = {Machine learning, machine learning, Training, Computational modeling, sustainability, Conferences, {MLOps}, Production, {AutoML}, Deployment, explainability, fairness, Market research, monitoring, re-training, robustness, Robustness},
	file = {IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/L4TE6GJT/Symeonidis et al. - 2022 - MLOps - Definitions, Tools and Challenges.pdf:application/pdf},
}

@inproceedings{zhou_towards_2020,
	title = {Towards {MLOps}: A Case Study of {ML} Pipeline Platform},
	doi = {10.1109/ICAICE51518.2020.00102},
	shorttitle = {Towards {MLOps}},
	abstract = {The development and deployment of machine learning ({ML}) applications differ significantly from traditional applications in many ways, which have led to an increasing need for efficient and reliable production of {ML} applications and supported infrastructures. Though platforms such as {TensorFlow} Extended ({TFX}), {ModelOps}, and Kubeflow have provided end-to-end lifecycle management for {ML} applications by orchestrating its phases into multistep {ML} pipelines, their performance is still uncertain. To address this, we built a functional {ML} platform with {DevOps} capability from existing continuous integration ({CI}) or continuous delivery ({CD}) tools and Kubeflow, constructed and ran {ML} pipelines to train models with different layers and hyperparameters while time and computing resources consumed were recorded. On this basis, we analyzed the time and resource consumption of each step in the {ML} pipeline, explored the consumption concerning the {ML} platform and computational models, and proposed potential performance bottlenecks such as {GPU} utilization. Our work provides a valuable reference for {ML} pipeline platform construction in practice.},
	eventtitle = {2020 International Conference on Artificial Intelligence and Computer Engineering ({ICAICE})},
	pages = {494--500},
	booktitle = {2020 International Conference on Artificial Intelligence and Computer Engineering ({ICAICE})},
	author = {Zhou, Yue and Yu, Yue and Ding, Bo},
	date = {2020-10},
	keywords = {machine learning, Data models, Pipelines, Training, Computational modeling, Task analysis, continuous training, {MLOps}, {DevOps}, end-to-end platform, Graphics processing units, Tools},
	file = {IEEE Xplore Abstract Record:/Users/heinrichpeters/Zotero/storage/SJR5QAAU/9361315.html:text/html;IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/6SKIT2W2/Zhou et al. - 2020 - Towards MLOps A Case Study of ML Pipeline Platfor.pdf:application/pdf},
}

@article{ruf_demystifying_2021,
	title = {Demystifying {MLOps} and Presenting a Recipe for the Selection of Open-Source Tools},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/19/8861},
	doi = {10.3390/app11198861},
	abstract = {Nowadays, machine learning projects have become more and more relevant to various real-world use cases. The success of complex Neural Network models depends upon many factors, as the requirement for structured and machine learning-centric project development management arises. Due to the multitude of tools available for different operational phases, responsibilities and requirements become more and more unclear. In this work, Machine Learning Operations ({MLOps}) technologies and tools for every part of the overall project pipeline, as well as involved roles, are examined and clearly defined. With the focus on the inter-connectivity of specific tools and comparison by well-selected requirements of {MLOps}, model performance, input data, and system quality metrics are briefly discussed. By identifying aspects of machine learning, which can be reused from project to project, open-source tools which help in specific parts of the pipeline, and possible combinations, an overview of support in {MLOps} is given. Deep learning has revolutionized the field of Image processing, and building an automated machine learning workflow for object detection is of great interest for many organizations. For this, a simple {MLOps} workflow for object detection with images is portrayed.},
	pages = {8861},
	number = {19},
	journaltitle = {Applied Sciences},
	author = {Ruf, Philipp and Madan, Manav and Reich, Christoph and Ould-Abdeslam, Djaffar},
	urldate = {2023-03-20},
	date = {2021-01},
	langid = {english},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {quality metrics, {MlOps}, tool comparison, workflow automation},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/EZXSL3Y4/Ruf et al. - 2021 - Demystifying MLOps and Presenting a Recipe for the.pdf:application/pdf},
}

@inproceedings{john_towards_2021,
	title = {Towards {MLOps}: A Framework and Maturity Model},
	doi = {10.1109/SEAA53835.2021.00050},
	shorttitle = {Towards {MLOps}},
	abstract = {The adoption of continuous software engineering practices such as {DevOps} (Development and Operations) in business operations has contributed to significantly shorter software development and deployment cycles. Recently, the term {MLOps} (Machine Learning Operations) has gained increasing interest as a practice that brings together data scientists and operations teams. However, the adoption of {MLOps} in practice is still in its infancy and there are few common guidelines on how to effectively integrate it into existing software development practices. In this paper, we conduct a systematic literature review and a grey literature review to derive a framework that identifies the activities involved in the adoption of {MLOps} and the stages in which companies evolve as they become more mature and advanced. We validate this framework in three case companies and show how they have managed to adopt and integrate {MLOps} in their large-scale software development companies. The contribution of this paper is threefold. First, we review contemporary literature to provide an overview of the state-of-the-art in {MLOps}. Based on this review, we derive an {MLOps} framework that details the activities involved in the continuous development of machine learning models. Second, we present a maturity model in which we outline the different stages that companies go through in evolving their {MLOps} practices. Third, we validate our framework in three embedded systems case companies and map the companies to the stages in the maturity model.},
	eventtitle = {2021 47th Euromicro Conference on Software Engineering and Advanced Applications ({SEAA})},
	pages = {1--8},
	booktitle = {2021 47th Euromicro Conference on Software Engineering and Advanced Applications ({SEAA})},
	author = {John, Meenu Mary and Olsson, Helena Holmström and Bosch, Jan},
	date = {2021-09},
	keywords = {Machine learning, {MLOps}, Bibliographies, Companies, Embedded systems, Framework, {GLR}, Maturity Model, {SLR}, Software, Software engineering, Systematics, Validation Study},
	file = {IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/2785DWIS/John et al. - 2021 - Towards MLOps A Framework and Maturity Model.pdf:application/pdf},
}

@inproceedings{makinen_who_2021,
	title = {Who Needs {MLOps}: What Data Scientists Seek to Accomplish and How Can {MLOps} Help?},
	doi = {10.1109/WAIN52551.2021.00024},
	shorttitle = {Who Needs {MLOps}},
	abstract = {Following continuous software engineering practices, there has been an increasing interest in rapid deployment of machine learning ({ML}) features, called {MLOps}. In this paper, we study the importance of {MLOps} in the context of data scientists' daily activities, based on a survey where we collected responses from 331 professionals from 63 different countries in {ML} domain, indicating on what they were working on in the last three months. Based on the results, up to 40\% respondents say that they work with both models and infrastructure; the majority of the work revolves around relational and time series data; and the largest categories of problems to be solved are predictive analysis, time series data, and computer vision. The biggest perceived problems revolve around data, although there is some awareness of problems related to deploying models to production and related procedures. To hypothesise, we believe that organisations represented in the survey can be divided to three categories - (i) figuring out how to best use data; (ii) focusing on building the first models and getting them to production; and (iii) managing several models, their versions and training datasets, as well as retraining and frequent deployment of retrained models. In the results, the majority of respondents are in category (i) or (ii), focusing on data and models; however the benefits of {MLOps} only emerge in category (iii) when there is a need for frequent retraining and redeployment. Hence, setting up an {MLOps} pipeline is a natural step to take, when an organization takes the step from {ML} as a proof-of-concept to {ML} as a part of nominal activities.},
	eventtitle = {2021 {IEEE}/{ACM} 1st Workshop on {AI} Engineering - Software Engineering for {AI} ({WAIN})},
	pages = {109--112},
	booktitle = {2021 {IEEE}/{ACM} 1st Workshop on {AI} Engineering - Software Engineering for {AI} ({WAIN})},
	author = {Mäkinen, Sasu and Skogström, Henrik and Laaksonen, Eero and Mikkonen, Tommi},
	date = {2021-05},
	keywords = {machine learning, Artificial intelligence, Data models, Training, Focusing, Computational modeling, Time series analysis, {MLOps}, Production, {DevOps}, Tools, {AI}, continuous software engineering, delivery pipeline, {ML}},
	file = {IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/45P2VCSF/Mäkinen et al. - 2021 - Who Needs MLOps What Data Scientists Seek to Acco.pdf:application/pdf},
}

@misc{kreuzberger_machine_2022,
	title = {Machine Learning Operations ({MLOps}): Overview, Definition, and Architecture},
	url = {http://arxiv.org/abs/2205.02302},
	doi = {10.48550/arXiv.2205.02302},
	shorttitle = {Machine Learning Operations ({MLOps})},
	abstract = {The final goal of all industrial machine learning ({ML}) projects is to develop {ML} products and rapidly bring them into production. However, it is highly challenging to automate and operationalize {ML} products and thus many {ML} endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations ({MLOps}) addresses this issue. {MLOps} includes several aspects, such as best practices, sets of concepts, and development culture. However, {MLOps} is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of {MLOps} and highlight open challenges in the field. Finally, this work provides guidance for {ML} researchers and practitioners who want to automate and operate their {ML} products with a designated set of technologies.},
	number = {{arXiv}:2205.02302},
	publisher = {{arXiv}},
	author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
	urldate = {2023-03-20},
	date = {2022-05-14},
	eprinttype = {arxiv},
	eprint = {2205.02302 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/NVSB8DWC/Kreuzberger et al. - 2022 - Machine Learning Operations (MLOps) Overview, Def.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/MKYDHXR5/2205.html:text/html},
}

@inproceedings{tamburri_sustainable_2020,
	title = {Sustainable {MLOps}: Trends and Challenges},
	doi = {10.1109/SYNASC51798.2020.00015},
	shorttitle = {Sustainable {MLOps}},
	abstract = {Even simply through a {GoogleTrends} search it becomes clear that Machine-Learning Operations-or {MLOps}, for short-are climbing in interest from both a scientific and practical perspective. On the one hand, software components and middleware are proliferating to support all manners of {MLOps}, from {AutoML} (i.e., software which enables developers with limited machine-learning expertise to train high-quality models specific to their domain or data) to feature-specific {ML} engineering, e.g., Explainability and Interpretability. On the other hand, the more these platforms penetrate the day-to-day activities of software operations, the more the risk for {AI} Software becoming unsustainable from a social, technical, or organisational perspective. This paper offers a concise definition of {MLOps} and {AI} Software Sustainability and outlines key challenges in its pursuit.},
	eventtitle = {2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC})},
	pages = {17--23},
	booktitle = {2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC})},
	author = {Tamburri, Damian A.},
	date = {2020-09},
	keywords = {Machine learning, Decision making, {MLOps}, Market research, {DataOps}, Machine-Learning Operations, Middleware, Scientific computing, Software Sustainability, Software systems, Sustainable development},
	file = {IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/VRCAEYQ9/Tamburri - 2020 - Sustainable MLOps Trends and Challenges.pdf:application/pdf},
}

@incollection{alla_what_2021,
	location = {Berkeley, {CA}},
	title = {What Is {MLOps}?},
	isbn = {978-1-4842-6549-9},
	url = {https://doi.org/10.1007/978-1-4842-6549-9_3},
	abstract = {In this chapter, we will cover the concepts behind the term “{MLOps}” and go over what it is, why it’s useful, and how it’s implemented.},
	pages = {79--124},
	booktitle = {Beginning {MLOps} with {MLFlow}: Deploy Models in {AWS} {SageMaker}, Google Cloud, and Microsoft Azure},
	publisher = {Apress},
	author = {Alla, Sridhar and Adari, Suman Kalyan},
	editor = {Alla, Sridhar and Adari, Suman Kalyan},
	urldate = {2023-03-20},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-1-4842-6549-9_3},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/76KAI56J/Alla and Adari - 2021 - What Is MLOps.pdf:application/pdf},
}

@inproceedings{heine_evaluation_2020,
	title = {On the Evaluation and Deployment of Machine Learning Approaches for Intrusion Detection},
	doi = {10.1109/BigData50022.2020.9378479},
	abstract = {Machine learning for intrusion detection is an active research field. However, instances of real-world application of methods proposed in the literature are still rare. Although a plethora of papers applying machine learning methods to benchmark data sets report excellent results, these methods seem to be hard to deploy in practice.In this paper, we investigate this gap between research and practical application by focusing on two questions: Firstly, we ask whether the current evaluation methodology is able to adequately forecast the performance of machine learning methods in practice. Secondly, we ask what needs to be done to facilitate the deployment of these methods.As a consequence to our findings, we formulate requirements for future evaluation methodologies and data sets, aiming to help evaluations better reflect actual performance in the field. Additionally, we identify a research road map with respect to the application of machine learning models in network intrusion detection systems, in order to further close the gap.},
	eventtitle = {2020 {IEEE} International Conference on Big Data (Big Data)},
	pages = {4594--4603},
	booktitle = {2020 {IEEE} International Conference on Big Data (Big Data)},
	author = {Heine, Felix and Laue, Tim and Kleiner, Carsten},
	date = {2020-12},
	keywords = {Machine learning, machine learning, Big Data, Training, Measurement, Tools, anomaly detection, Benchmark testing, dataset creation, evaluation, {IDS}, intrusion detection, Roads},
	file = {IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/9TEE7J9P/Heine et al. - 2020 - On the Evaluation and Deployment of Machine Learni.pdf:application/pdf},
}

@article{cohen_problems_2021,
	title = {Problems in the deployment of machine-learned models in health care},
	volume = {193},
	issn = {0820-3946, 1488-2329},
	url = {http://www.cmaj.ca/lookup/doi/10.1503/cmaj.202066},
	doi = {10.1503/cmaj.202066},
	pages = {E1391--E1394},
	number = {35},
	journaltitle = {Canadian Medical Association Journal},
	shortjournal = {{CMAJ}},
	author = {Cohen, Joseph Paul and Cao, Tianshi and Viviano, Joseph D. and Huang, Chin-Wei and Fralick, Michael and Ghassemi, Marzyeh and Mamdani, Muhammad and Greiner, Russell and Bengio, Yoshua},
	urldate = {2023-03-20},
	date = {2021-09-07},
	langid = {english},
	file = {Cohen et al. - 2021 - Problems in the deployment of machine-learned mode.pdf:/Users/heinrichpeters/Zotero/storage/4IE95SBH/Cohen et al. - 2021 - Problems in the deployment of machine-learned mode.pdf:application/pdf},
}

@misc{lu_deploying_2022,
	title = {Deploying clinical machine learning? Consider the following...},
	url = {http://arxiv.org/abs/2109.06919},
	doi = {10.48550/arXiv.2109.06919},
	shorttitle = {Deploying clinical machine learning?},
	abstract = {Despite the intense attention and considerable investment into clinical machine learning research, relatively few applications have been deployed at a large-scale in a real-world clinical environment. While research is important in advancing the state-of-the-art, translation is equally important in bringing these techniques and technologies into a position to ultimately impact healthcare. We believe a lack of appreciation for several considerations are a major cause for this discrepancy between expectation and reality. To better characterize a holistic perspective among researchers and practitioners, we survey several practitioners with commercial experience in developing {CML} for clinical deployment. Using these insights, we identify several main categories of challenges in order to better design and develop clinical machine learning applications.},
	number = {{arXiv}:2109.06919},
	publisher = {{arXiv}},
	author = {Lu, Charles and Chang, Ken and Singh, Praveer and Pomerantz, Stuart and Doyle, Sean and Kakarmath, Sujay and Bridge, Christopher and Kalpathy-Cramer, Jayashree},
	urldate = {2023-03-20},
	date = {2022-01-12},
	eprinttype = {arxiv},
	eprint = {2109.06919 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/KSMHV8UA/Lu et al. - 2022 - Deploying clinical machine learning Consider the .pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/7KJE5VQY/2109.html:text/html},
}

@inproceedings{openja_studying_2022,
	location = {New York, {NY}, {USA}},
	title = {Studying the Practices of Deploying Machine Learning Projects on Docker},
	isbn = {978-1-4503-9613-4},
	url = {https://dl.acm.org/doi/10.1145/3530019.3530039},
	doi = {10.1145/3530019.3530039},
	series = {{EASE} '22},
	abstract = {Docker is a containerization service that allows for convenient deployment of websites, databases, applications’ {APIs}, and machine learning ({ML}) models with a few lines of code. Studies have recently explored the use of Docker for deploying general software projects with no specific focus on how Docker is used to deploy {ML}-based projects. In this study, we conducted an exploratory study to understand how Docker is being used to deploy {ML}-based projects. As the initial step, we examined the categories of {ML}-based projects that use Docker. We then examined why and how these projects use Docker, and the characteristics of the resulting Docker images. Our results indicate that six categories of {ML}-based projects use Docker for deployment, including {ML} Applications, {MLOps}/ {AIOps}, Toolkits, {DL} Frameworks, Models, and Documentation. We derived the taxonomy of 21 major categories representing the purposes of using Docker, including those specific to models such as model management tasks (e.g., testing, training). We then showed that {ML} engineers use Docker images mostly to help with the platform portability, such as transferring the software across the operating systems, runtimes such as {GPU}, and language constraints. However, we also found that more resources may be required to run the Docker images for building {ML}-based software projects due to the large number of files contained in the image layers with deeply nested directories. We hope to shed light on the emerging practices of deploying {ML} software projects using containers and highlight aspects that should be improved.},
	pages = {190--200},
	booktitle = {Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022},
	publisher = {Association for Computing Machinery},
	author = {Openja, Moses and Majidi, Forough and Khomh, Foutse and Chembakottu, Bhagya and Li, Heng},
	urldate = {2023-03-20},
	date = {2022-06-13},
	keywords = {Machine Learning, Deployment, Deep Neural Network, Docker},
	file = {Submitted Version:/Users/heinrichpeters/Zotero/storage/DIAJAPG3/Openja et al. - 2022 - Studying the Practices of Deploying Machine Learni.pdf:application/pdf},
}

@article{morse_estimate_2020,
	title = {Estimate the hidden deployment cost of predictive models to improve patient care},
	volume = {26},
	rights = {2020 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-019-0651-8},
	doi = {10.1038/s41591-019-0651-8},
	abstract = {Although examples of algorithms designed to improve healthcare delivery abound, for many, clinical integration will not be achieved. The deployment cost of machine learning models is an underappreciated barrier to success. Experts propose three criteria that, assessed early, could help estimate the deployment cost.},
	pages = {18--19},
	number = {1},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Morse, Keith E. and Bagley, Steven C. and Shah, Nigam H.},
	urldate = {2023-03-20},
	date = {2020-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computational models, Software, Predictive medicine},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/8RFN5XQ5/Morse et al. - 2020 - Estimate the hidden deployment cost of predictive .pdf:application/pdf},
}

@article{prapas_continuous_2021,
	title = {Continuous Training and Deployment of Deep Learning Models},
	volume = {21},
	issn = {1610-1995},
	url = {https://doi.org/10.1007/s13222-021-00386-8},
	doi = {10.1007/s13222-021-00386-8},
	abstract = {Deep Learning ({DL}) has consistently surpassed other Machine Learning methods and achieved state-of-the-art performance in multiple cases. Several modern applications like financial and recommender systems require models that are constantly updated with fresh data. The prominent approach for keeping a {DL} model fresh is to trigger full retraining from scratch when enough new data are available. However, retraining large and complex {DL} models is time-consuming and compute-intensive. This makes full retraining costly, wasteful, and slow. In this paper, we present an approach to continuously train and deploy {DL} models. First, we enable continuous training through proactive training that combines samples of historical data with new streaming data. Second, we enable continuous deployment through gradient sparsification that allows us to send a small percentage of the model updates per training iteration. Our experimental results with {LeNet}5 on {MNIST} and modern {DL} models on {CIFAR}-10 show that proactive training keeps models fresh with comparable—if not superior—performance to full retraining at a fraction of the time. Combined with gradient sparsification, sparse proactive training enables very fast updates of a deployed model with arbitrarily large sparsity, reducing communication per iteration up to four orders of magnitude, with minimal—if any—losses in model quality. Sparse training, however, comes at a price; it incurs overhead on the training that depends on the size of the model and increases the training time by factors ranging from 1.25 to 3 in our experiments. Arguably, a small price to pay for successfully enabling the continuous training and deployment of large {DL} models.},
	pages = {203--212},
	number = {3},
	journaltitle = {Datenbank-Spektrum},
	shortjournal = {Datenbank Spektrum},
	author = {Prapas, Ioannis and Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Markl, Volker},
	urldate = {2023-03-20},
	date = {2021-11-01},
	langid = {english},
	keywords = {Deep learning, Continuous deployment, Continuous training, Model deployment},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/4RN494EI/Prapas et al. - 2021 - Continuous Training and Deployment of Deep Learnin.pdf:application/pdf},
}

@inproceedings{hirt_end--end_2017,
	title = {An end-to-end process model for supervised machine learning classification: from problem to deployment in information systems},
	rights = {https://creativecommons.org/licenses/by-sa/4.0/deed.en},
	url = {https://cora.ucc.ie/handle/10468/4442},
	shorttitle = {An end-to-end process model for supervised machine learning classification},
	abstract = {Extracting meaningful knowledge from (big) data represents a key success factor in many industries today. Supervised machine learning ({SML}) has emerged as a popular technique to learn patterns in complex data sets and to identify hidden correlations. When this insight is turned into action, business value is created. However, common data mining processes are generally not tailored to {SML}. In addition, they fall short of providing an end-to-end view that not only supports building a ”one off” model, but also covers its operational deployment within an information system. In this research-in-progress work we apply a Design Science Research ({DSR}) approach to develop a {SML} process model artifact that comprises model initiation, error estimation and deployment. In a first cycle, we evaluate the artifact in an illustrative scenario to demonstrate suitability. The results encourage us to further refine the approach and to prepare evaluations in concrete use cases. Thus, we move towards contributing a general process model that supports the systematic design of machine learning solutions to turn insights into continuous action.},
	eventtitle = {Designing the Digital Transformation: {DESRIST} 2017 Research in Progress Proceedings of the 12th International Conference on Design Science Research in Information Systems and Technology. Karlsruhe, Germany. 30 May - 1 Jun.},
	pages = {55--63},
	publisher = {Karlsruher Institut für Technologie ({KIT})},
	author = {Hirt, Robin and Koehl, Niklas J. and Satzger, Gerhard},
	urldate = {2023-03-20},
	date = {2017},
	langid = {english},
	note = {Accepted: 2017-08-11T08:39:01Z
{ISSN}: 2194-1629},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/5JYLBWSZ/Hirt et al. - 2017 - An end-to-end process model for supervised machine.pdf:application/pdf},
}

@article{crankshaw_prediction-serving_2018,
	title = {Prediction-Serving Systems: What happens when we wish to actually deploy a machine learning model to production?},
	volume = {16},
	issn = {1542-7730},
	url = {https://dl.acm.org/doi/10.1145/3194653.3210557},
	doi = {10.1145/3194653.3210557},
	shorttitle = {Prediction-Serving Systems},
	abstract = {This installment of Research for Practice features a curated selection from Dan Crankshaw and Joey Gonzalez, who provide an overview of machine learning serving systems. What happens when we wish to actually deploy a machine learning model to production, and how do we serve predictions with high accuracy and high computational efficiency? Dan and Joey’s selection provides a thoughtful selection of cutting-edge techniques spanning database-level integration, video processing, and prediction middleware. Given the explosion of interest in machine learning and its increasing impact on seemingly every application vertical, it’s possible that systems such as these will become as commonplace as relational databases are today},
	pages = {83--97},
	number = {1},
	journaltitle = {Queue},
	shortjournal = {Queue},
	author = {Crankshaw, Dan and Gonzalez, Joseph},
	urldate = {2023-03-20},
	date = {2018-02-01},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/VWYD3MNZ/Crankshaw and Gonzalez - 2018 - Prediction-Serving Systems What happens when we w.pdf:application/pdf},
}

@article{crankshaw_prediction-serving_2018-1,
	title = {Prediction-Serving Systems: What happens when we wish to actually deploy a machine learning model to production?},
	volume = {16},
	issn = {1542-7730, 1542-7749},
	url = {https://dl.acm.org/doi/10.1145/3194653.3210557},
	doi = {10.1145/3194653.3210557},
	shorttitle = {Prediction-Serving Systems},
	abstract = {This installment of Research for Practice features a curated selection from Dan Crankshaw and Joey Gonzalez, who provide an overview of machine learning serving systems. What happens when we wish to actually deploy a machine learning model to production, and how do we serve predictions with high accuracy and high computational efficiency? Dan and Joey’s selection provides a thoughtful selection of cutting-edge techniques spanning database-level integration, video processing, and prediction middleware. Given the explosion of interest in machine learning and its increasing impact on seemingly every application vertical, it’s possible that systems such as these will become as commonplace as relational databases are today},
	pages = {83--97},
	number = {1},
	journaltitle = {Queue},
	shortjournal = {Queue},
	author = {Crankshaw, Dan and Gonzalez, Joseph},
	urldate = {2023-03-20},
	date = {2018-02},
	langid = {english},
}

@article{derakhshan_continuous_nodate,
	title = {Continuous Deployment of Machine Learning Pipelines},
	abstract = {Today machine learning is entering many business and scienti c applications. The life cycle of machine learning applications consists of data preprocessing for transforming the raw data into features, training a model using the features, and deploying the model for answering prediction queries. In order to guarantee accurate predictions, one has to continuously monitor and update the deployed model and pipeline. Current deployment platforms update the model using online learning methods. When online learning alone is not adequate to guarantee the prediction accuracy, some deployment platforms provide a mechanism for automatic or manual retraining of the model. While the online training is fast, the retraining of the model is time-consuming and adds extra overhead and complexity to the process of deployment. We propose a novel continuous deployment approach for updating the deployed model using a combination of the incoming realtime data and the historical data. We utilize sampling techniques to include the historical data in the training process, thus eliminating the need for retraining the deployed model. We also o er online statistics computation and dynamic materialization of the preprocessed features, which further reduces the total training and data preprocessing time. In our experiments, we design and deploy two pipelines and models to process two real-world datasets. The experiments show that continuous deployment reduces the total training cost up to 15 times while providing the same level of quality when compared to the state-of-the-art deployment approaches.},
	author = {Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Rabl, Tilmann and Markl, Volker},
	langid = {english},
	file = {Derakhshan et al. - Continuous Deployment of Machine Learning Pipeline.pdf:/Users/heinrichpeters/Zotero/storage/ELK9K9CA/Derakhshan et al. - Continuous Deployment of Machine Learning Pipeline.pdf:application/pdf},
}

@inproceedings{warnett_architectural_2022,
	title = {Architectural Design Decisions for Machine Learning Deployment},
	doi = {10.1109/ICSA53651.2022.00017},
	abstract = {Deploying machine learning models to production is challenging, partially due to the misalignment between software engineering and machine learning disciplines but also due to potential practitioner knowledge gaps. To reduce this gap and guide decision-making, we conducted a qualitative investigation into the technical challenges faced by practitioners based on studying the grey literature and applying the Straussian Grounded Theory research method. We modelled current practices in machine learning, resulting in a {UML}-based architectural design decision model based on current practitioner understanding of the domain and a subset of the decision space and identified seven architectural design decisions, various relations between them, twenty-six decision options and forty-four decision drivers in thirty-five sources. Our results intend to help bridge the gap between science and practice, increase understanding of how practitioners approach deployment of their solutions, and support practitioners in their decision-making.},
	eventtitle = {2022 {IEEE} 19th International Conference on Software Architecture ({ICSA})},
	pages = {90--100},
	booktitle = {2022 {IEEE} 19th International Conference on Software Architecture ({ICSA})},
	author = {Warnett, Stephen John and Zdun, Uwe},
	date = {2022-03},
	keywords = {Machine learning, Decision making, Machine Learning, Conferences, Production, Architectural Design Decisions, Grey Literature, Grounded Theory, Knowledge engineering, Software architecture, Software Architecture, Unified modeling language},
	file = {IEEE Xplore Full Text PDF:/Users/heinrichpeters/Zotero/storage/J3QBU6HU/Warnett and Zdun - 2022 - Architectural Design Decisions for Machine Learnin.pdf:application/pdf},
}

@inproceedings{ginart_mldemondeployment_2022,
	title = {{MLDemon}:Deployment Monitoring for Machine Learning Systems},
	url = {https://proceedings.mlr.press/v151/ginart22a.html},
	shorttitle = {{MLDemon}},
	abstract = {Post-deployment monitoring of {ML} systems is critical for ensuring reliability, especially as new user inputs can differ from the training distribution. Here we propose a novel approach, {MLDemon}, for {ML} {DEployment} {MONitoring}. {MLDemon} integrates both unlabeled data and a small amount of on-demand labels to produce a real-time estimate of the {ML} model’s current performance on a given data stream. Subject to budget constraints, {MLDemon} decides when to acquire additional, potentially costly, expert supervised labels to verify the model. On temporal datasets with diverse distribution drifts and models, {MLDemon} outperforms existing approaches. Moreover, we provide theoretical analysis to show that {MLDemon} is minimax rate optimal for a broad class of distribution drifts.},
	eventtitle = {International Conference on Artificial Intelligence and Statistics},
	pages = {3962--3997},
	booktitle = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Ginart, Tony and Zhang, Martin Jinye and Zou, James},
	urldate = {2023-03-20},
	date = {2022-05-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/GQ7ULCJL/Ginart et al. - 2022 - MLDemonDeployment Monitoring for Machine Learning.pdf:application/pdf},
}

@software{noauthor_use_2023,
	title = {Use {ONNX}},
	rights = {Apache-2.0},
	url = {https://github.com/onnx/onnx},
	abstract = {Open standard for machine learning interoperability},
	publisher = {Open Neural Network Exchange},
	urldate = {2023-03-20},
	date = {2023-03-20},
	note = {original-date: 2017-09-07T04:53:45Z},
	keywords = {deep-learning, machine-learning, tensorflow, deep-neural-networks, dnn, keras, ml, mxnet, neural-network, onnx, pytorch, scikit-learn},
}

@misc{nigenda_amazon_2022,
	title = {Amazon {SageMaker} Model Monitor: A System for Real-Time Insights into Deployed Machine Learning Models},
	url = {http://arxiv.org/abs/2111.13657},
	doi = {10.48550/arXiv.2111.13657},
	shorttitle = {Amazon {SageMaker} Model Monitor},
	abstract = {With the increasing adoption of machine learning ({ML}) models and systems in high-stakes settings across different industries, guaranteeing a model's performance after deployment has become crucial. Monitoring models in production is a critical aspect of ensuring their continued performance and reliability. We present Amazon {SageMaker} Model Monitor, a fully managed service that continuously monitors the quality of machine learning models hosted on Amazon {SageMaker}. Our system automatically detects data, concept, bias, and feature attribution drift in models in real-time and provides alerts so that model owners can take corrective actions and thereby maintain high quality models. We describe the key requirements obtained from customers, system design and architecture, and methodology for detecting different types of drift. Further, we provide quantitative evaluations followed by use cases, insights, and lessons learned from more than two years of production deployment.},
	number = {{arXiv}:2111.13657},
	publisher = {{arXiv}},
	author = {Nigenda, David and Karnin, Zohar and Zafar, Muhammad Bilal and Ramesha, Raghu and Tan, Alan and Donini, Michele and Kenthapadi, Krishnaram},
	urldate = {2023-03-20},
	date = {2022-08-05},
	eprinttype = {arxiv},
	eprint = {2111.13657 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/MVT5P6KH/Nigenda et al. - 2022 - Amazon SageMaker Model Monitor A System for Real-.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/HJIVZZNC/2111.html:text/html},
}

@article{kulkarni_key_2021,
	title = {Key Technology Considerations in Developing and Deploying Machine Learning Models in Clinical Radiology Practice},
	volume = {9},
	rights = {This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published {JMIR} Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this copyright and license information must be included.},
	url = {https://medinform.jmir.org/2021/9/e28776},
	doi = {10.2196/28776},
	abstract = {The use of machine learning to develop intelligent software tools for the interpretation of radiology images has gained widespread attention in recent years. The development, deployment, and eventual adoption of these models in clinical practice, however, remains fraught with challenges. In this paper, we propose a list of key considerations that machine learning researchers must recognize and address to make their models accurate, robust, and usable in practice. We discuss insufficient training data, decentralized data sets, high cost of annotations, ambiguous ground truth, imbalance in class representation, asymmetric misclassification costs, relevant performance metrics, generalization of models to unseen data sets, model decay, adversarial attacks, explainability, fairness and bias, and clinical validation. We describe each consideration and identify the techniques used to address it. Although these techniques have been discussed in prior research, by freshly examining them in the context of medical imaging and compiling them in the form of a laundry list, we hope to make them more accessible to researchers, software developers, radiologists, and other stakeholders.},
	pages = {e28776},
	number = {9},
	journaltitle = {{JMIR} Medical Informatics},
	author = {Kulkarni, Viraj and Gawali, Manish and Kharat, Amit},
	urldate = {2023-03-20},
	date = {2021-09-09},
	note = {Company: {JMIR} Medical Informatics
Distributor: {JMIR} Medical Informatics
Institution: {JMIR} Medical Informatics
Label: {JMIR} Medical Informatics
Publisher: {JMIR} Publications Inc., Toronto, Canada},
	file = {Full Text:/Users/heinrichpeters/Zotero/storage/TID8MNQT/Kulkarni et al. - 2021 - Key Technology Considerations in Developing and De.pdf:application/pdf},
}

@incollection{singh_machine_2021,
	location = {Berkeley, {CA}},
	title = {Machine Learning Deployment as a Web Service},
	isbn = {978-1-4842-6546-8},
	url = {https://doi.org/10.1007/978-1-4842-6546-8_3},
	abstract = {In this chapter, we are going to go over how to use different web frameworks for deploying machine learning and deep learning models as web services hosted on the local system. This chapter covers three main topics.},
	pages = {67--90},
	booktitle = {Deploy Machine Learning Models to Production: With Flask, Streamlit, Docker, and Kubernetes on Google Cloud Platform},
	publisher = {Apress},
	author = {Singh, Pramod},
	editor = {Singh, Pramod},
	urldate = {2023-03-20},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-1-4842-6546-8_3},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/4YTPWLSA/Singh - 2021 - Machine Learning Deployment as a Web Service.pdf:application/pdf},
}

@article{heymann_guideline_2022,
	title = {Guideline for Deployment of Machine Learning Models for Predictive Quality in Production},
	volume = {107},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827122003523},
	doi = {10.1016/j.procir.2022.05.068},
	series = {Leading manufacturing systems transformation – Proceedings of the 55th {CIRP} Conference on Manufacturing Systems 2022},
	abstract = {Predicting product quality represents a common area of application of machine learning ({ML}) in manufacturing. However, manifold challenges occur during the integration of {ML} models into production processes. Therefore, this paper aims to provide a guideline for the deployment of {ML} models in production environments. Relevant decisions and steps for deploying models in predictive quality use cases are demonstrated. The results for each component of the proposed guideline - deployment design, productionizing \& testing, monitoring, and retraining - have been validated with industry experts including exemplary implementations.},
	pages = {815--820},
	journaltitle = {Procedia {CIRP}},
	shortjournal = {Procedia {CIRP}},
	author = {Heymann, Henrik and Kies, Alexander D. and Frye, Maik and Schmitt, Robert H. and Boza, Andrés},
	urldate = {2023-03-20},
	date = {2022-01-01},
	langid = {english},
	keywords = {Machine Learning, Production, Deployment, Artificial Intelligence, Manufacturing, Predictive Quality},
	file = {ScienceDirect Full Text PDF:/Users/heinrichpeters/Zotero/storage/FU2JZX2C/Heymann et al. - 2022 - Guideline for Deployment of Machine Learning Model.pdf:application/pdf},
}

@inproceedings{ackermann_deploying_2018,
	location = {New York, {NY}, {USA}},
	title = {Deploying Machine Learning Models for Public Policy: A Framework},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219911},
	doi = {10.1145/3219819.3219911},
	series = {{KDD} '18},
	shorttitle = {Deploying Machine Learning Models for Public Policy},
	abstract = {Machine learning research typically focuses on optimization and testing on a few criteria, but deployment in a public policy setting requires more. Technical and non-technical deployment issues get relatively little attention. However, for machine learning models to have real-world benefit and impact, effective deployment is crucial. In this case study, we describe our implementation of a machine learning early intervention system ({EIS}) for police officers in the Charlotte-Mecklenburg (North Carolina) and Metropolitan Nashville (Tennessee) Police Departments. The {EIS} identifies officers at high risk of having an adverse incident, such as an unjustified use of force or sustained complaint. We deployed the same code base at both departments, which have different underlying data sources and data structures. Deployment required us to solve several new problems, covering technical implementation, governance of the system, the cost to use the system, and trust in the system. In this paper we describe how we addressed and solved several of these challenges and provide guidance and a framework of important issues to consider for future deployments.},
	pages = {15--22},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Ackermann, Klaus and Walsh, Joe and De Unánue, Adolfo and Naveed, Hareem and Navarrete Rivera, Andrea and Lee, Sun-Joo and Bennett, Jason and Defoe, Michael and Cody, Crystal and Haynes, Lauren and Ghani, Rayid},
	urldate = {2023-03-20},
	date = {2018-07-19},
	keywords = {machine learning, deployment, early intervention system, public policy},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/J6WZ3R3Y/Ackermann et al. - 2018 - Deploying Machine Learning Models for Public Polic.pdf:application/pdf},
}

@book{singh_deploy_2021,
	location = {Berkeley, {CA}},
	title = {Deploy Machine Learning Models to Production: With Flask, Streamlit, Docker, and Kubernetes on Google Cloud Platform},
	isbn = {978-1-4842-6545-1 978-1-4842-6546-8},
	url = {http://link.springer.com/10.1007/978-1-4842-6546-8},
	shorttitle = {Deploy Machine Learning Models to Production},
	publisher = {Apress},
	author = {Singh, Pramod},
	urldate = {2023-03-20},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-1-4842-6546-8},
	file = {Singh - 2021 - Deploy Machine Learning Models to Production With.pdf:/Users/heinrichpeters/Zotero/storage/97FPU229/Singh - 2021 - Deploy Machine Learning Models to Production With.pdf:application/pdf},
}

@article{zaharia_accelerating_nodate-1,
	title = {Accelerating the Machine Learning Lifecycle with {MLﬂow}},
	abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an {ML} application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe {MLﬂow}, an open source platform we recently launched to streamline the machine learning lifecycle. {MLﬂow} covers three key challenges: experimentation, reproducibility, and model deployment, using generic {APIs} that work with any {ML} library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
	author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
	langid = {english},
	file = {Zaharia et al. - Accelerating the Machine Learning Lifecycle with M.pdf:/Users/heinrichpeters/Zotero/storage/8TZKAMBN/Zaharia et al. - Accelerating the Machine Learning Lifecycle with M.pdf:application/pdf},
}

@article{maleki_machine_2020,
	title = {Machine Learning Algorithm Validation: From Essentials to Advanced Applications and Implications for Regulatory Certification and Deployment},
	volume = {30},
	issn = {1052-5149, 1557-9867},
	url = {https://www.neuroimaging.theclinics.com/article/S1052-5149(20)30059-9/fulltext},
	doi = {10.1016/j.nic.2020.08.004},
	shorttitle = {Machine Learning Algorithm Validation},
	pages = {433--445},
	number = {4},
	journaltitle = {Neuroimaging Clinics},
	shortjournal = {Neuroimaging Clinics},
	author = {Maleki, Farhad and Muthukrishnan, Nikesh and Ovens, Katie and Reinhold, Caroline and Forghani, Reza},
	urldate = {2023-03-20},
	date = {2020-11-01},
	pmid = {33038994},
	note = {Publisher: Elsevier},
	keywords = {Machine learning, Artificial intelligence, Reproducibility, Deep learning, Ability to generalize, Cross-validation, Evaluation, Validation},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/7VDHSHWD/Maleki et al. - 2020 - Machine Learning Algorithm Validation From Essent.pdf:application/pdf},
}

@article{zhang_shifting_2022,
	title = {Shifting machine learning for healthcare from development to deployment and from models to data},
	volume = {6},
	rights = {2022 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00898-y},
	doi = {10.1038/s41551-022-00898-y},
	abstract = {In the past decade, the application of machine learning ({ML}) to healthcare has helped drive the automation of physician tasks as well as enhancements in clinical capabilities and access to care. This progress has emphasized that, from model development to model deployment, data play central roles. In this Review, we provide a data-centric view of the innovations and challenges that are defining {ML} for healthcare. We discuss deep generative models and federated learning as strategies to augment datasets for improved model performance, as well as the use of the more recent transformer models for handling larger datasets and enhancing the modelling of clinical text. We also discuss data-focused problems in the deployment of {ML}, emphasizing the need to efficiently deliver data to {ML} models for timely clinical predictions and to account for natural data shifts that can deteriorate model performance.},
	pages = {1330--1345},
	number = {12},
	journaltitle = {Nature Biomedical Engineering},
	shortjournal = {Nat. Biomed. Eng},
	author = {Zhang, Angela and Xing, Lei and Zou, James and Wu, Joseph C.},
	urldate = {2023-03-20},
	date = {2022-12},
	langid = {english},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computational science, Biomedical engineering, Medical imaging, Translational research},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/BTMVHLCC/Zhang et al. - 2022 - Shifting machine learning for healthcare from deve.pdf:application/pdf},
}

@article{paleyes_challenges_2022,
	title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3533378},
	doi = {10.1145/3533378},
	shorttitle = {Challenges in Deploying Machine Learning},
	abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.},
	pages = {114:1--114:29},
	number = {6},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
	urldate = {2023-03-20},
	date = {2022-12-07},
	keywords = {Machine learning applications, sofware deployment},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/AIKEAPMV/Paleyes et al. - 2022 - Challenges in Deploying Machine Learning A Survey.pdf:application/pdf},
}

@article{pruneski_development_2022,
	title = {The development and deployment of machine learning models},
	volume = {30},
	issn = {1433-7347},
	url = {https://doi.org/10.1007/s00167-022-07155-4},
	doi = {10.1007/s00167-022-07155-4},
	abstract = {Applications of artificial intelligence, specifically machine learning, are becoming increasingly popular in Orthopaedic Surgery, and medicine as a whole. This growing interest is shared by data scientists and physicians alike. However, there is an asymmetry of understanding of the developmental process and potential applications of machine learning. As new technology will undoubtedly affect clinical practice in the coming years, it is important for physicians to understand how these processes work. The purpose of this paper is to provide clarity and a general framework for building and assessing machine learning models.},
	pages = {3917--3923},
	number = {12},
	journaltitle = {Knee Surgery, Sports Traumatology, Arthroscopy},
	shortjournal = {Knee Surg Sports Traumatol Arthrosc},
	author = {Pruneski, James A. and Williams, Riley J. and Nwachukwu, Benedict U. and Ramkumar, Prem N. and Kiapour, Ata M. and Martin, R. Kyle and Karlsson, Jón and Pareek, Ayoosh},
	urldate = {2023-03-20},
	date = {2022-12-01},
	langid = {english},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/2JVZ4HNS/Pruneski et al. - 2022 - The development and deployment of machine learning.pdf:application/pdf},
}

@book{baier_challenges_2019,
	title = {{CHALLENGES} {IN} {THE} {DEPLOYMENT} {AND} {OPERATION} {OF} {MACHINE} {LEARNING} {IN} {PRACTICE}},
	abstract = {Machine learning has recently emerged as a powerful technique to increase operational efficiency or to develop new value propositions. However, the translation of a prediction algorithm into an operationally usable machine learning model is a time-consuming and in various ways challenging task. In this work, we target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications. To this end, we first identify relevant challenges with a structured literature analysis. Subsequently, we conduct an interview study with machine learning practitioners across various industries, perform a qualitative content analysis, and identify challenges organized along three distinct categories as well as six overarching clusters. Eventually, results from both literature and interviews are evaluated with a comparative analysis. Key issues identified include automated strategies for data drift detection and handling, standardization of machine learning infrastructure, and appropriate communication and expectation management.},
	author = {Baier, Lucas and Jöhren, Fabian and Seebacher, Stefan},
	date = {2019-05-10},
	file = {Challengesinthedeploymentandoperationofmachinelearninginpractice.pdf:/Users/heinrichpeters/Zotero/storage/2IK8W8QM/Challengesinthedeploymentandoperationofmachinelearninginpractice.pdf:application/pdf},
}

@article{zhang_model_2017,
	title = {Model provenance tracking and inference for integrated environmental modelling},
	volume = {96},
	issn = {1364-8152},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815216308398},
	doi = {10.1016/j.envsoft.2017.06.051},
	abstract = {Integrated environmental modeling ({IEM}) provides a systematic way to couple models for integrated analysis. Coupled models in {IEM} often exchange data at runtime for time-step based executions. It is a challenge to track which raw observations or intermediate data exchanged at runtime contribute to individual model outputs. Time-step level provenance is needed to audit the trail of model execution or perform diagnosis in case of anomalies. This paper introduces a method to support provenance awareness in {IEM}. It suggests that individual models should expose necessary interfaces for provenance capturing in {IEM} environments. The provenance is represented using the W3C {PROV} model for interoperability. Fine-grained provenance is inferred based on coarse-grained provenance and temporal characteristics of computations of numerical time marching models. The approach is implemented in {OpenMI}-compliant models. A case study of model provenance tracking and inference on the watershed runoff simulation scenario illustrates the applicability of the approach.},
	pages = {95--105},
	journaltitle = {Environmental Modelling \& Software},
	shortjournal = {Environmental Modelling \& Software},
	author = {Zhang, Mingda and Yue, Peng and Wu, Zhaoyan and Ziebelin, Danielle and Wu, Huayi and Zhang, Chenxiao},
	urldate = {2023-03-20},
	date = {2017-10-01},
	langid = {english},
	keywords = {Environmental models, Integrated environmental modelling, {OpenMI}, Provenance, Scientific workflow, W3C {PROV}},
	file = {ScienceDirect Full Text PDF:/Users/heinrichpeters/Zotero/storage/I6PJJGRH/Zhang et al. - 2017 - Model provenance tracking and inference for integr.pdf:application/pdf;ScienceDirect Snapshot:/Users/heinrichpeters/Zotero/storage/NSCKZE9Q/S1364815216308398.html:text/html},
}

@inproceedings{wolf_transformers_2020,
	location = {Online},
	title = {Transformers: State-of-the-Art Natural Language Processing},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	shorttitle = {Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	pages = {38--45},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	urldate = {2023-03-11},
	date = {2020-10},
	file = {Full Text PDF:/Users/heinrichpeters/Zotero/storage/XGWG2P6T/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf},
}

@article{davenport_is_2022,
	title = {Is Data Scientist Still the Sexiest Job of the 21st Century?},
	issn = {0017-8012},
	url = {https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century},
	abstract = {Ten years ago, the authors posited that being a data scientist was the “sexiest job of the 21st century.” A decade later, does the claim stand up? The job has grown in popularity and is generally well-paid, and the field is projected to experience more growth than almost any other by 2029. But the job has changed, in both large and small ways. It’s become better institutionalized, the scope of the job has been redefined, the technology it relies on has made huge strides, and the importance of non-technical expertise, such as ethics and change management, has grown. How it operates in companies — and how executives need to think about managing data science efforts — has changed, too, as businesses now need to create and oversee diverse data science teams rather than searching for data scientist unicorns. Finally, companies need to think about what comes next, and how they can begin to think about democratizing data science.},
	journaltitle = {Harvard Business Review},
	author = {Davenport, Thomas H. and Patil, D. J.},
	urldate = {2023-07-23},
	date = {2022-07-15},
	note = {Section: Analytics and data science},
	keywords = {{AI} and machine learning, Analytics and data science, Data management, Technology and analytics},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/ZKS8IPJY/is-data-scientist-still-the-sexiest-job-of-the-21st-century.html:text/html},
}

@online{noauthor_citing_nodate,
	title = {Citing {ONNX}? · onnx/onnx · Discussion \#1441},
	url = {https://github.com/onnx/onnx/discussions/1441},
	shorttitle = {Citing {ONNX}?},
	abstract = {What's the appropriate way to cite {ONNX} in an academic publication?},
	titleaddon = {{GitHub}},
	urldate = {2023-07-23},
	langid = {english},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/6KR6JB3V/onnx.html:text/html},
}

@software{noauthor_use_2023-1,
	title = {Use {ONNX}},
	rights = {Apache-2.0},
	url = {https://github.com/onnx/onnx},
	abstract = {Open standard for machine learning interoperability},
	publisher = {Open Neural Network Exchange},
	urldate = {2023-07-23},
	date = {2023-07-23},
	note = {original-date: 2017-09-07T04:53:45Z},
	keywords = {deep-learning, machine-learning, tensorflow, deep-neural-networks, dnn, keras, ml, mxnet, neural-network, onnx, pytorch, scikit-learn},
}

@online{noauthor_onnx_nodate,
	title = {{ONNX} {\textbar} Home},
	url = {https://onnx.ai/},
	urldate = {2023-07-23},
	file = {ONNX | Home:/Users/heinrichpeters/Zotero/storage/7WP6WDH5/onnx.ai.html:text/html},
}

@misc{finn_model-agnostic_2017,
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	url = {http://arxiv.org/abs/1703.03400},
	doi = {10.48550/arXiv.1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	number = {{arXiv}:1703.03400},
	publisher = {{arXiv}},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	urldate = {2023-07-30},
	date = {2017-07-18},
	eprinttype = {arxiv},
	eprint = {1703.03400 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/RC3DNXXL/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/8L75HKSP/1703.html:text/html},
}

@misc{hospedales_meta-learning_2020,
	title = {Meta-Learning in Neural Networks: A Survey},
	url = {http://arxiv.org/abs/2004.05439},
	doi = {10.48550/arXiv.2004.05439},
	shorttitle = {Meta-Learning in Neural Networks},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to {AI} where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	number = {{arXiv}:2004.05439},
	publisher = {{arXiv}},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	urldate = {2023-07-30},
	date = {2020-11-07},
	eprinttype = {arxiv},
	eprint = {2004.05439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/5TRDIB82/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/UJHS6QYU/2004.html:text/html},
}

@online{noauthor_onnx_nodate-1,
	title = {{ONNX} {\textbar} Home},
	url = {https://onnx.ai/},
	urldate = {2023-07-30},
	file = {ONNX | Home:/Users/heinrichpeters/Zotero/storage/VVNCKUMU/onnx.ai.html:text/html},
}

@online{noauthor_model_nodate,
	title = {Model Share {AI} - Share your machine learning models in seconds},
	url = {https://www.modelshare.ai/},
	abstract = {Take advantage of one of the fastest model deployment tools on the market. Initially created for data science researches by researchers, Model Share {AI} is the perfect place for your business to launch models into production with just a couple lines of code.},
	titleaddon = {Model Share {AI}},
	urldate = {2023-07-30},
	langid = {english},
	file = {Snapshot:/Users/heinrichpeters/Zotero/storage/C63F6TCP/www.modelshare.ai.html:text/html},
}

@article{he_automl_2021,
	title = {{AutoML}: A Survey of the State-of-the-Art},
	volume = {212},
	issn = {09507051},
	url = {http://arxiv.org/abs/1908.00709},
	doi = {10.1016/j.knosys.2020.106622},
	shorttitle = {{AutoML}},
	abstract = {Deep learning ({DL}) techniques have penetrated all aspects of our lives and brought us great convenience. However, building a high-quality {DL} system for a specific task highly relies on human expertise, hindering the applications of {DL} to more areas. Automated machine learning ({AutoML}) becomes a promising solution to build a {DL} system without human assistance, and a growing number of researchers focus on {AutoML}. In this paper, we provide a comprehensive and up-to-date review of the state-of-the-art ({SOTA}) in {AutoML}. First, we introduce {AutoML} methods according to the pipeline, covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search ({NAS}). We focus more on {NAS}, as it is currently very hot sub-topic of {AutoML}. We summarize the performance of the representative {NAS} algorithms on the {CIFAR}-10 and {ImageNet} datasets and further discuss several worthy studying directions of {NAS} methods: one/two-stage {NAS}, one-shot {NAS}, and joint hyperparameter and architecture optimization. Finally, we discuss some open problems of the existing {AutoML} methods for future research.},
	pages = {106622},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	urldate = {2023-07-30},
	date = {2021-01},
	eprinttype = {arxiv},
	eprint = {1908.00709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/heinrichpeters/Zotero/storage/Y2ELY55E/He et al. - 2021 - AutoML A Survey of the State-of-the-Art.pdf:application/pdf;arXiv.org Snapshot:/Users/heinrichpeters/Zotero/storage/QIXR6C7N/1908.html:text/html},
}
