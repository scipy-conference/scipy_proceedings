
@article{2000,
  title = {The {{Laser Interferometer Gravitational}}-{{Wave Observatory LIGO}}},
  year = {2000},
  month = jan,
  volume = {25},
  pages = {1165--1169},
  issn = {0273-1177},
  doi = {10.1016/S0273-1177(99)00980-1},
  abstract = {The Laser Interferometer Gravitational-Wave Observatory (LIGO) Project to develop a terrestrial astronomical observatory for gravitational wave detect\ldots},
  file = {/Users/rwb/Zotero/storage/8FMIK7BS/S0273117799009801.html},
  journal = {Advances in Space Research},
  language = {en},
  number = {6}
}

@article{2006,
  title = {Experimental Investigation of Heuristics for Resource-Constrained Project Scheduling: {{An}} Update},
  shorttitle = {Experimental Investigation of Heuristics for Resource-Constrained Project Scheduling},
  year = {2006},
  month = oct,
  volume = {174},
  pages = {23--37},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2005.01.065},
  abstract = {This paper considers heuristics for the well-known resource-constrained project scheduling problem (RCPSP). It provides an update of our survey which \ldots},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/2006_experimental_investigation_of_heuristics_for_resource-constrained_project.pdf;/Users/rwb/Zotero/storage/P26GN42S/S0377221705002596.html},
  journal = {European Journal of Operational Research},
  language = {en},
  number = {1}
}

@article{2008,
  title = {Transit Network Design and Scheduling: {{A}} Global Review},
  shorttitle = {Transit Network Design and Scheduling},
  year = {2008},
  month = dec,
  volume = {42},
  pages = {1251--1273},
  issn = {0965-8564},
  doi = {10.1016/j.tra.2008.03.011},
  abstract = {This paper presents a global review of the crucial strategic and tactical steps of transit planning: the design and scheduling of the network. These s\ldots},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/2008_transit_network_design_and_scheduling.pdf;/Users/rwb/Zotero/storage/ERHU38BN/S0965856408000888.html},
  journal = {Transportation Research Part A: Policy and Practice},
  keywords = {_tablet},
  language = {en},
  number = {10}
}

@misc{2011,
  title = {{{HTTPS Everywhere}}},
  year = {2011},
  month = oct,
  abstract = {HTTPS Everywhere is a Firefox, Chrome, and Opera extension that encrypts your communications with many major websites, making your browsing more secure.},
  file = {/Users/rwb/Zotero/storage/RVPNUGN7/https-everywhere.html},
  howpublished = {https://www.eff.org/https-everywhere},
  journal = {Electronic Frontier Foundation},
  language = {en}
}

@article{2012,
  title = {Mixed Integer Programming of Multiobjective Hydro-Thermal Self Scheduling},
  year = {2012},
  month = aug,
  volume = {12},
  pages = {2137--2146},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2012.03.020},
  abstract = {This paper presents a method for hydro-thermal self scheduling (HTSS) problem in a day-ahead joint energy and reserve market. The HTSS is modeled in t\ldots},
  file = {/Users/rwb/Zotero/storage/U4DW3YEG/S1568494612001111.html},
  journal = {Applied Soft Computing},
  language = {en},
  number = {8}
}

@article{2012a,
  title = {Scheduling Real-Time {{DAGs}} in Heterogeneous Clusters by Combining Imprecise Computations and Bin Packing Techniques for the Exploitation of Schedule Holes},
  year = {2012},
  month = jul,
  volume = {28},
  pages = {977--988},
  issn = {0167-739X},
  doi = {10.1016/j.future.2012.03.002},
  abstract = {In this paper, we investigate the improvement that can be gained in the performance of a heterogeneous cluster dedicated to real-time jobs, by exploit\ldots},
  file = {/Users/rwb/Zotero/storage/L2PXDEGS/S0167739X12000556.html},
  journal = {Future Generation Computer Systems},
  language = {en},
  number = {7}
}

@article{2013,
  title = {Characterizing and Profiling Scientific Workflows},
  year = {2013},
  month = mar,
  volume = {29},
  pages = {682--692},
  issn = {0167-739X},
  doi = {10.1016/j.future.2012.08.015},
  abstract = {Researchers working on the planning, scheduling, and execution of scientific workflows need access to a wide variety of scientific workflows to evalua\ldots},
  file = {/Users/rwb/Zotero/storage/JUHBFK55/S0167739X12001732.html},
  journal = {Future Generation Computer Systems},
  language = {en},
  number = {3}
}

@article{2014,
  title = {Proactive Scheduling in Distributed Computing\textemdash{{A}} Reinforcement Learning Approach},
  year = {2014},
  month = jul,
  volume = {74},
  pages = {2662--2672},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.03.007},
  abstract = {In distributed computing such as grid computing, online users submit their tasks anytime and anywhere to dynamic resources. Task arrival and execution\ldots},
  file = {/Users/rwb/Zotero/storage/AEKL9B2I/S074373151400063X.html},
  journal = {Journal of Parallel and Distributed Computing},
  language = {en},
  number = {7}
}

@misc{2016,
  title = {{{JWST Target}} of {{Opportunity Observations}}},
  year = {2016},
  abstract = {A target for JWST observations is called a target of opportunity (ToO) if the requested observations are linked to an event that may occur at an unknown time. ToO observations must be called out and triggering conditions described in your proposal.},
  file = {/Users/rwb/Zotero/storage/P9INIHLU/JWST+Target+of+Opportunity+Observations.html},
  howpublished = {https://jwst-docs.stsci.edu/display/JPP/JWST+Target+of+Opportunity+Observations},
  journal = {JWST User Documentation}
}

@article{2016a,
  ids = {solar2016},
  title = {A Scheduling Model for Astronomy},
  year = {2016},
  month = apr,
  volume = {15},
  pages = {90--104},
  issn = {2213-1337},
  doi = {10.1016/j.ascom.2016.02.005},
  abstract = {Astronomical scheduling problem has several external conditions that change dynamically at any time during observations, like weather condition (humid\ldots},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/solar_et_al_2016_a_scheduling_model_for_astronomy.pdf;/Users/rwb/Zotero/storage/2VE3BSLI/S2213133716300142.html;/Users/rwb/Zotero/storage/GPHKCPL3/S2213133716300142.html},
  journal = {Astronomy and Computing},
  keywords = {Astronomical scheduling problem,Astronomy scheduler,Dynamic scheduling},
  language = {en}
}

@article{2017,
  ids = {daley2017},
  title = {Performance Characterization of Scientific Workflows for the Optimal Use of {{Burst Buffers}}},
  year = {2017},
  month = dec,
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.12.022},
  abstract = {Scientific discoveries are increasingly dependent upon the analysis of large volumes of data from observations and simulations of complex phenomena. S\ldots},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/daley_et_al_2017_performance_characterization_of_scientific_workflows_for_the_optimal_use_of.pdf;/Users/rwb/Zotero/storage/99Q2PN29/S0167739X16308287.html;/Users/rwb/Zotero/storage/T4BZN7QC/S0167739X16308287.html},
  journal = {Future Generation Computer Systems},
  keywords = {Burst Buffer,DataWarp,HPC,Workflow},
  language = {en}
}

@article{2019,
  title = {An Efficient Fault Tolerant Workflow Scheduling Approach Using Replication Heuristics and Checkpointing in the Cloud},
  year = {2019},
  month = oct,
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2019.09.004},
  abstract = {Scientific workflows have been predominantly used for complex and large scale data analysis and scientific computation/automation and the need for rob\ldots},
  file = {/Users/rwb/Zotero/storage/ICXWE4UQ/S0743731518306580.html},
  journal = {Journal of Parallel and Distributed Computing},
  language = {en}
}

@misc{2019a,
  title = {{{IOR}} and Mdtest. {{Contribute}} to Hpc/Ior Development by Creating an Account on {{GitHub}}},
  year = {2019},
  month = jan,
  copyright = {View license},
  howpublished = {high performance computing}
}

@article{2019b,
  title = {A Multi-Staged Niched Evolutionary Approach for Allocating Parallel Tasks with Joint Optimization of Performance, Energy, and Temperature},
  year = {2019},
  month = dec,
  volume = {134},
  pages = {65--74},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2019.05.009},
  abstract = {This paper presents a multi-stage multi-objective evolutionary approach (MS-MOEA) for allocating parallel computations on multi-core processors by joi\ldots},
  file = {/Users/rwb/Zotero/storage/LWW8LZMT/S0743731518303332.html},
  journal = {Journal of Parallel and Distributed Computing},
  language = {en}
}

@article{2019c,
  title = {Task {{Packing}}: {{Efficient}} Task Scheduling in Unbalanced Parallel Programs to Maximize {{CPU}} Utilization},
  shorttitle = {Task {{Packing}}},
  year = {2019},
  month = dec,
  volume = {134},
  pages = {37--49},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2019.08.003},
  abstract = {Load imbalance in parallel systems can be generated by external factors to the currently running applications like operating system noise or the under\ldots},
  file = {/Users/rwb/Zotero/storage/NPSU5SCX/S0743731519305623.html},
  journal = {Journal of Parallel and Distributed Computing},
  language = {en}
}

@misc{2020,
  title = {{{MOEAFramework}}/{{MOEAFramework}}},
  year = {2020},
  month = mar,
  abstract = {A Free and Open Source Java Framework for Multiobjective Optimization},
  howpublished = {MOEA Framework}
}

@misc{262588213843476,
  title = {Install {{Monaco}} Font in {{Linux}}},
  author = {262588213843476},
  abstract = {Install Monaco font in Linux. GitHub Gist: instantly share code, notes, and snippets.},
  file = {/Users/rwb/Zotero/storage/DZCRPNSH/99819.html},
  howpublished = {https://gist.github.com/rogerleite/99819},
  journal = {Gist},
  language = {en}
}

@inproceedings{abawajy2004,
  title = {Fault-Tolerant Scheduling Policy for Grid Computing Systems},
  booktitle = {18th {{International Parallel}} and {{Distributed Processing Symposium}}, 2004. {{Proceedings}}.},
  author = {Abawajy, J. H.},
  year = {2004},
  month = apr,
  pages = {238-},
  doi = {10.1109/IPDPS.2004.1303290},
  abstract = {Summary form only given. With the momentum gaining for the grid computing systems, the issue of deploying support for integrated scheduling and fault-tolerant approaches becomes paramount importance. Unfortunately, fault-tolerance has not been factored into the design of most existing grid scheduling strategies. To this end, we propose a fault-tolerant scheduling policy that loosely couples job scheduling with job replication scheme such that jobs are efficiently and reliably executed. Performance evaluation of the proposed fault-tolerant scheduler against a nonfault-tolerant scheduling policy is presented and shown that the proposed policy performs reasonably in the presence of various types of failures.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2004/abawajy_2004_fault-tolerant_scheduling_policy_for_grid_computing_systems.pdf;/Users/rwb/Zotero/storage/VQZ6SELW/1303290.html},
  keywords = {Application software,Distributed computing,Fault tolerance,fault tolerant computing,Fault tolerant systems,fault-tolerant scheduler,fault-tolerant scheduling policy,grid computing,Grid computing,grid computing system,grid scheduling strategy,integrated scheduling,job replication scheme,job scheduling,Large-scale systems,Middleware,nonfault-tolerant scheduling policy,Operating systems,performance evaluation,Processor scheduling,Resource management}
}

@inproceedings{abbasi2018,
  title = {Sparse: {{A}} More Modern Sparse Array Library},
  shorttitle = {Sparse},
  booktitle = {Python in {{Science Conference}}},
  author = {Abbasi, Hameer},
  year = {2018},
  pages = {65--68},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-4af1f417-00a},
  abstract = {This paper is about sparse multi-dimensional arrays in Python. We discuss their applications, layouts, and current implementations in the SciPy ecosystem along with strengths and weaknesses. We then introduce a new package for sparse arrays that builds on the legacy of the scipy.sparse implementation, but supports more modern interfaces, dimensions greater than two, and improved integration with newer array packages, like XArray and Dask. We end with performance benchmarks and notes on future work. Additionally, this work provides a concrete implementation of the recent NumPy array protocols to build generic array interfaces for improved interoperability, and so may be useful for broader community discussion.},
  file = {/Users/rwb/Zotero/storage/8K2AYHFV/Abbasi - 2018 - Sparse A more modern sparse array library.pdf},
  language = {en}
}

@inproceedings{abdelbaky2017,
  title = {Enabling {{Distributed Software}}-{{Defined Environments Using Dynamic Infrastructure Service Composition}}},
  booktitle = {Proceedings of the 17th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}}},
  author = {AbdelBaky, Moustafa and {Diaz-Montes}, Javier and Unuvar, Merve and Romanus, Melissa and Rodero, Ivan and Steinder, Malgorzata and Parashar, Manish},
  year = {2017},
  pages = {274--283},
  publisher = {{IEEE Press}},
  address = {{Piscataway, NJ, USA}},
  doi = {10.1109/CCGRID.2017.104},
  abstract = {Service-based access models coupled with emerging application deployment technologies are enabling opportunities for realizing highly customized software-defined environments, which can support dynamic and data-driven applications. However, this requires rethinking traditional resource federation models to support dynamic resource compositions, which can adapt to evolving application needs and the dynamic state of underlying resources. In this paper, we present a programmable approach that leverages software-defined techniques to create a dynamic space-time infrastructure service composition. We propose the use of Constraint Programming as a formal language to allow users, applications, and service providers to define the desired state of the execution environment. The resulting distributed software-defined environment continually adapts to meet objectives/constraints set by the users, applications, and/or resource providers. We present the design and prototype implementation of such distributed software-defined environment. We use a cancer informatics workflow to demonstrate the operation of our framework using resources from five different cloud providers, which are aggregated on-demand based on dynamic user and resource provider constraints.},
  isbn = {978-1-5090-6610-0},
  keywords = {distributed software-defined environments,programmable infrastructure,programmable service composition},
  series = {{{CCGrid}} '17}
}

@article{abdelbaky2018,
  ids = {abdelbaky2018a},
  title = {Software-Defined Environments for Science and Engineering},
  author = {AbdelBaky, Moustafa and {Diaz-Montes}, Javier and Parashar, Manish},
  year = {2018},
  month = jan,
  volume = {32},
  pages = {104--122},
  issn = {1094-3420},
  doi = {10.1177/1094342017710706},
  abstract = {Service-based access models coupled with recent advances in application deployment technologies are enabling opportunities for realizing highly customized software-defined environments that can achieve new levels of efficiencies and can support emerging dynamic and data-driven applications. However, achieving this vision requires new models that can support dynamic (and opportunistic) compositions of infrastructure services, which can adapt to evolving application needs and the state of resources. In this article, we present a programmable dynamic infrastructure service composition approach that uses software-defined environment concepts to control the composition process. The resulting software-defined infrastructure service composition adapts to meet objectives and constraints set by the users, applications, and/or resource providers. We present and compare two different approaches for programming resources and controlling the service composition, one that is based on a rule engine and another that leverages a constraint programming model for resource description. We present the design and prototype implementation of such software-defined service composition and demonstrate its operation through a use case where multiple views of heterogeneous, geographically distributed services are aggregated on demand based on user and resource provider specifications. The resulting compositions are used to run different bioinformatics workloads, which are encapsulated inside Docker containers. Each view independently adapts to various constraints and events that are imposed on the system while minimizing the workload completion time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/abdelbaky_et_al_2018_software-defined_environments_for_science_and_engineering.pdf},
  journal = {The International Journal of High Performance Computing Applications},
  language = {en},
  number = {1}
}

@article{abdul2016,
  title = {Scalable {{Scientific Workflows Management System SWFMS}}},
  author = {Abdul, M.},
  year = {2016},
  volume = {7},
  issn = {21565570, 2158107X},
  doi = {10.14569/IJACSA.2016.071137},
  abstract = {In today\quotedblbase s electronic world conducting scientific experiments, especially in natural sciences domain, has become more and more challenging for domain scientists since ``science'' today has turned out to be more complex due to the two dimensional intricacy; one: assorted as well as complex computational (analytical) applications and two: increasingly large volume as well as heterogeneity of scientific data products processed by these applications. Furthermore, the involvement of increasingly large number of scientific instruments such as sensors and machines makes the scientific data management even more challenging since the data generated from such type of instruments are highly complex. To reduce the amount of complexities in conducting scientific experiments as much as possible, an integrated framework that transparently implements the conceptual separation between both the dimensions is direly needed. In order to facilitate scientific experiments ,,workflow\quotedblbase{} technology has in recent years emerged in scientific disciplines like biology, bioinformatics, geology, environmental science, and eco-informatics. Much more research work has been done to develop the scientific workflow systems. However, our analysis over these existing systems shows that they lack a well-structured conceptual modeling methodology to deal with the two complex dimensions in a transparent manner. This paper presents a scientific workflow framework that properly addresses these two dimensional complexities in a proper manner.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/abdul_2016_scalable_scientific_workflows_management_system_swfms.pdf},
  journal = {International Journal of Advanced Computer Science and Applications},
  language = {en},
  number = {11}
}

@article{abdulhamid2018,
  title = {Fault Tolerance Aware Scheduling Technique for Cloud Computing Environment Using Dynamic Clustering Algorithm},
  author = {Abdulhamid, Shafi'i Muhammad and Abd Latiff, Muhammad Shafie and Madni, Syed Hamid Hussain and Abdullahi, Mohammed},
  year = {2018},
  month = jan,
  volume = {29},
  pages = {279--293},
  issn = {1433-3058},
  doi = {10.1007/s00521-016-2448-8},
  abstract = {In cloud computing, resources are dynamically provisioned and delivered to users in a transparent manner automatically on-demand. Task execution failure is no longer accidental but a common characteristic of cloud computing environment. In recent times, a number of intelligent scheduling techniques have been used to address task scheduling issues in cloud without much attention to fault tolerance. In this research article, we proposed a dynamic clustering league championship algorithm (DCLCA) scheduling technique for fault tolerance awareness to address cloud task execution which would reflect on the current available resources and reduce the untimely failure of autonomous tasks. Experimental results show that our proposed technique produces remarkable fault reduction in task failure as measured in terms of failure rate. It also shows that the DCLCA outperformed the MTCT, MAXMIN, ant colony optimization and genetic algorithm-based NSGA-II by producing lower makespan with improvement of 57.8, 53.6, 24.3 and 13.4 \% in the first scenario and 60.0, 38.9, 31.5 and 31.2 \% in the second scenario, respectively. Considering the experimental results, DCLCA provides better quality fault tolerance aware scheduling that will help to improve the overall performance of the cloud environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/abdulhamid_et_al_2018_fault_tolerance_aware_scheduling_technique_for_cloud_computing_environment.pdf},
  journal = {Neural Computing and Applications},
  keywords = {_tablet,Cloud scheduling,Dynamic clustering,Fault tolerance,League championship algorithm,Task scheduling},
  language = {en},
  number = {1}
}

@article{abeliuk,
  title = {Interdependent {{Scheduling Games}}},
  author = {Abeliuk, Andres and Aziz, Haris and Berbeglia, Gerardo and Gaspers, Serge and Kalina, Petr and Mattei, Nicholas and Peters, Dominik and Stursberg, Paul and Hentenryck, Pascal Van and Walsh, Toby},
  pages = {8},
  abstract = {We propose a model of interdependent scheduling games in which each player controls a set of services that they schedule independently. A player is free to schedule his own services at any time; however, each of these services only begins to accrue reward for the player when all predecessor services, which may or may not be controlled by the same player, have been activated. This model, where players have interdependent services, is motivated by the problems faced in planning and coordinating large-scale infrastructures, e.g., restoring electricity and gas to residents after a natural disaster or providing medical care in a crisis when different agencies are responsible for the delivery of staff, equipment, and medicine. We undertake a game-theoretic analysis of this setting and in particular consider the issues of welfare maximization, computing best responses, Nash dynamics, and existence and computation of Nash equilibria.},
  file = {/Users/rwb/Zotero/storage/XDPICLA8/abeliuk_et_al_interdependent_scheduling_games.pdf},
  keywords = {_tablet},
  language = {en}
}

@inproceedings{abrishami2010,
  ids = {abrishami2010a},
  title = {Cost-Driven Scheduling of Grid Workflows Using {{Partial Critical Paths}}},
  booktitle = {2010 11th {{IEEE}}/{{ACM International Conference}} on {{Grid Computing}}},
  author = {Abrishami, S. and Naghibzadeh, M. and Epema, D.},
  year = {2010},
  month = oct,
  pages = {81--88},
  doi = {10.1109/GRID.2010.5697955},
  abstract = {Recently, utility grids have emerged as a new model of service provisioning in heterogeneous distributed systems. In this model, users negotiate with providers on their required Quality of Service and on the corresponding price to reach a Service Level Agreement. One of the most challenging problems in utility grids is workflow scheduling, i.e., the problem of satisfying users' QoS as well as minimizing the cost of workflow execution. In this paper, we propose a new QoS-based workflow scheduling algorithm based on a novel concept called Partial Critical Path. This algorithm recursively schedules the critical path ending at a recently scheduled node. The proposed algorithm tries to minimize the cost of workflow execution while meeting a user-defined deadline. The simulation results show that the performance of our algorithm is very promising.},
  file = {/Users/rwb/Zotero/storage/U854QL4F/5697955.html},
  keywords = {Communities,Computational modeling,cost-driven scheduling,economic grids,grid computing,grid workflows,heterogeneous distributed systems,partial critical paths,QoS-based scheduling,QoS-based workflow scheduling,Quality of service,Schedules,scheduling,Scheduling,Scheduling algorithm,service level agreement,Unread,utility grids,workflow management software,workflow scheduling}
}

@article{abrishami2012,
  title = {Cost-{{Driven Scheduling}} of {{Grid Workflows Using Partial Critical Paths}}},
  author = {Abrishami, S. and Naghibzadeh, M. and Epema, D. H. J.},
  year = {2012},
  month = aug,
  volume = {23},
  pages = {1400--1414},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2011.303},
  abstract = {Recently, utility Grids have emerged as a new model of service provisioning in heterogeneous distributed systems. In this model, users negotiate with service providers on their required Quality of Service and on the corresponding price to reach a Service Level Agreement. One of the most challenging problems in utility Grids is workflow scheduling, i.e., the problem of satisfying the QoS of the users as well as minimizing the cost of workflow execution. In this paper, we propose a new QoS-based workflow scheduling algorithm based on a novel concept called Partial Critical Paths (PCP), that tries to minimize the cost of workflow execution while meeting a user-defined deadline. The PCP algorithm has two phases: in the deadline distribution phase it recursively assigns subdeadlines to the tasks on the partial critical paths ending at previously assigned tasks, and in the planning phase it assigns the cheapest service to each task while meeting its subdeadline. The simulation results show that the performance of the PCP algorithm is very promising.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/abrishami_et_al_2012_cost-driven_scheduling_of_grid_workflows_using_partial_critical_paths.pdf;/Users/rwb/Zotero/storage/UMSPLYK4/6104035.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Communities,Complexity theory,cost-driven scheduling,deadline distribution phase,economic Grids,grid computing,Grid computing,grid workflows,heterogeneous distributed systems,partial critical paths,PCP algorithm,planning phase,QoS-based scheduling.,QoS-based workflow scheduling algorithm,quality of service,Quality of service,Schedules,scheduling,Scheduling,Scheduling algorithm,service level agreement,service provisioning model,utility grids,utility Grids,workflow management software,workflow scheduling},
  number = {8}
}

@article{abrishami2013,
  ids = {abrishami2013a},
  title = {Deadline-Constrained Workflow Scheduling Algorithms for {{Infrastructure}} as a {{Service Clouds}}},
  author = {Abrishami, Saeid and Naghibzadeh, Mahmoud and Epema, Dick H. J.},
  year = {2013},
  month = jan,
  volume = {29},
  pages = {158--169},
  issn = {0167-739X},
  doi = {10.1016/j.future.2012.05.004},
  abstract = {The advent of Cloud computing as a new model of service provisioning in distributed systems encourages researchers to investigate its benefits and drawbacks on executing scientific applications such as workflows. One of the most challenging problems in Clouds is workflow scheduling, i.e., the problem of satisfying the QoS requirements of the user as well as minimizing the cost of workflow execution. We have previously designed and analyzed a two-phase scheduling algorithm for utility Grids, called Partial Critical Paths (PCP), which aims to minimize the cost of workflow execution while meeting a user-defined deadline. However, we believe Clouds are different from utility Grids in three ways: on-demand resource provisioning, homogeneous networks, and the pay-as-you-go pricing model. In this paper, we adapt the PCP algorithm for the Cloud environment and propose two workflow scheduling algorithms: a one-phase algorithm which is called IaaS Cloud Partial Critical Paths (IC-PCP), and a two-phase algorithm which is called IaaS Cloud Partial Critical Paths with Deadline Distribution (IC-PCPD2). Both algorithms have a polynomial time complexity which make them suitable options for scheduling large workflows. The simulation results show that both algorithms have a promising performance, with IC-PCP performing better than IC-PCPD2 in most cases.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/abrishami_et_al_2013_deadline-constrained_workflow_scheduling_algorithms_for_infrastructure_as_a.pdf;/Users/rwb/Dropbox/PhD/zotero/Future Generation Computer Systems/abrishami_et_al_2013_deadline-constrained_workflow_scheduling_algorithms_for_infrastructure_as_a.pdf;/Users/rwb/Zotero/storage/3M4ES4R7/S0167739X12001008.html;/Users/rwb/Zotero/storage/IQRZ7BAT/S0167739X12001008.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cloud computing,Grid computing,IaaS Clouds,QoS-based scheduling,Unread,Workflow scheduling},
  number = {1},
  series = {Including {{Special}} Section: {{AIRCC}}-{{NetCoM}} 2009 and {{Special}} Section: {{Clouds}} and {{Service}}-{{Oriented Architectures}}}
}

@article{abrishami2013a,
  title = {Deadline-Constrained Workflow Scheduling Algorithms for {{Infrastructure}} as a {{Service Clouds}}},
  author = {Abrishami, Saeid and Naghibzadeh, Mahmoud and Epema, Dick H. J.},
  year = {2013},
  month = jan,
  volume = {29},
  pages = {158--169},
  issn = {0167-739X},
  doi = {10.1016/j.future.2012.05.004},
  abstract = {The advent of Cloud computing as a new model of service provisioning in distributed systems encourages researchers to investigate its benefits and drawbacks on executing scientific applications such as workflows. One of the most challenging problems in Clouds is workflow scheduling, i.e., the problem of satisfying the QoS requirements of the user as well as minimizing the cost of workflow execution. We have previously designed and analyzed a two-phase scheduling algorithm for utility Grids, called Partial Critical Paths (PCP), which aims to minimize the cost of workflow execution while meeting a user-defined deadline. However, we believe Clouds are different from utility Grids in three ways: on-demand resource provisioning, homogeneous networks, and the pay-as-you-go pricing model. In this paper, we adapt the PCP algorithm for the Cloud environment and propose two workflow scheduling algorithms: a one-phase algorithm which is called IaaS Cloud Partial Critical Paths (IC-PCP), and a two-phase algorithm which is called IaaS Cloud Partial Critical Paths with Deadline Distribution (IC-PCPD2). Both algorithms have a polynomial time complexity which make them suitable options for scheduling large workflows. The simulation results show that both algorithms have a promising performance, with IC-PCP performing better than IC-PCPD2 in most cases.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/abrishami_et_al_2013_deadline-constrained_workflow_scheduling_algorithms_for_infrastructure_as_a2.pdf;/Users/rwb/Zotero/storage/VHIE45HZ/S0167739X12001008.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cloud computing,Grid computing,IaaS Clouds,QoS-based scheduling,Workflow scheduling},
  language = {en},
  number = {1},
  series = {Including {{Special}} Section: {{AIRCC}}-{{NetCoM}} 2009 and {{Special}} Section: {{Clouds}} and {{Service}}-{{Oriented Architectures}}}
}

@article{abusharkh2016,
  title = {Building a Cloud on Earth: {{A}} Study of Cloud Computing Data Center Simulators},
  shorttitle = {Building a Cloud on Earth},
  author = {Abu Sharkh, Mohamed and Kanso, Ali and Shami, Abdallah and {\"O}hl{\'e}n, Peter},
  year = {2016},
  month = oct,
  volume = {108},
  pages = {78--96},
  issn = {13891286},
  doi = {10.1016/j.comnet.2016.06.037},
  abstract = {As cloud computing technologies finalize their transformation into the standard technologies for businesses of all sizes, they face more scrutiny than ever. Clients are expecting the benefits of turning infrastructure, platform and network into services payable per use without tolerating any service hiccups caused by performance bottlenecks or overprovision. This puts cloud providers under pressure to deliver data center management solutions and deployment plans in minimal time and with failure allowance close to none. Any comprehensive solution evaluation could gain much from the use of cloud simulators. Cloud simulators have the advantage of practicality over both mathematical proofs and real testbeds. They support any amount of heterogeneous use cases demanded by the cloud provider. Despite being a relatively new concept, multiple cloud simulators were developed. However, they are still in the phase of adapting to the scenarios, objectives and characteristics of the cloud. This paper examines a selected set of the current cloud simulators in terms of vision, features, and architecture. Strong points and limitations are discussed. Moreover, this paper presents a framework for cloud simulator design that can serve as an elaborate design checklist. A discussion of the open research challenges concludes the paper.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/Abu Sharkh et al_2016_Building a cloud on earth.pdf},
  journal = {Computer Networks},
  keywords = {_tablet},
  language = {en}
}

@article{acevedo2017,
  title = {A {{Critical Path File Location}} ({{CPFL}}) Algorithm for Data-Aware Multiworkflow Scheduling on {{HPC}} Clusters},
  author = {Acevedo, C{\'e}sar and Hern{\'a}ndez, Porfidio and Espinosa, Antonio and M{\'e}ndez, V{\'i}ctor},
  year = {2017},
  month = sep,
  volume = {74},
  pages = {51--62},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.04.025},
  abstract = {A representative set of workflows found in bioinformatics pipelines must deal with large data sets. Most scientific workflows are defined as Direct Acyclic Graphs (DAGs). Despite DAGs are useful to understand dependence relationships, they do not provide any information about input, output and temporal data files. This information about the location of files of data intensive applications helps to avoid performance issues. This paper presents a multiworkflow store-aware scheduler in a cluster environment called Critical Path File Location (CPFL) policy where the access time to disk is more relevant than network, as an extension of the classical list scheduling policies. Our purpose is to find the best location of data files in a hierarchical storage system. The resulting algorithm is tested in an HPC cluster and in a simulated cluster scenario with bioinformatics synthetic workflows, and largely used benchmarks like Montage and Epigenomics. The resulting simulator is tuned and validated with the first test results from the real infrastructure. The evaluation of our proposal shows promising results up to 70\% on benchmarks in real HPC clusters using 128 cores and up to 69\% of makespan improvement on simulated 512 cores clusters with a deviation between 0.9\% and 3\% regarding the real HPC cluster.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/acevedo_et_al_2017_a_critical_path_file_location_(cpfl)_algorithm_for_data-aware_multiworkflow.pdf;/Users/rwb/Zotero/storage/3VPDDV9C/S0167739X17306507.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cluster,Critical path,Data processing,Multiworkflows,Scheduler,Simulation}
}

@article{achterberg2008,
  title = {{{SCIP}}: {{Solving}} Constraint Integer Programs},
  shorttitle = {{{SCIP}}},
  author = {Achterberg, Tobias},
  year = {2008},
  month = aug,
  volume = {0},
  pages = {1--41},
  issn = {1867-2957},
  doi = {10.1007/mpc.v0i1.4},
  abstract = {Constraint integer programming (CIP) is a novel paradigm which integrates constraint programming (CP), mixed integer programming (MIP), and satisfiability (SAT) modeling and solving techniques. In this paper we discuss the software framework and solver SCIP (Solving Constraint Integer Programs), which is free for academic and non-commercial use and can be downloaded in source code.  This paper gives an overview of the main design concepts of SCIP and how it can be used to solve constraint integer programs. To illustrate the performance and flexibility of SCIP, we apply it to two different problem classes. First, we consider mixed integer programming and show by computational experiments that SCIP is almost competitive to specialized commercial MIP solvers, even though SCIP supports the more general constraint integer programming paradigm. We develop new ingredients that improve current MIP solving technology.  As a second application, we employ SCIP to solve chip design verification problems as they arise in the logic design of integrated circuits. This application goes far beyond traditional MIP solving, as it includes several highly non-linear constraints, which can be handled nicely within the constraint integer programming framework. We show anecdotally how the different solving techniques from MIP, CP, and SAT work together inside SCIP to deal with such constraint classes. Finally, experimental results show that our approach outperforms current state-of-the-art techniques for proving the validity of properties on circuits containing arithmetic.},
  file = {/Users/rwb/Zotero/storage/335B7TLQ/4.html},
  journal = {Mathematical Programming Computation},
  language = {en},
  number = {1}
}

@article{achterberg2009,
  title = {{{SCIP}}: Solving Constraint Integer Programs},
  shorttitle = {{{SCIP}}},
  author = {Achterberg, Tobias},
  year = {2009},
  month = jul,
  volume = {1},
  pages = {1--41},
  issn = {1867-2957},
  doi = {10.1007/s12532-008-0001-1},
  abstract = {Constraint integer programming (CIP) is a novel paradigm which integrates constraint programming (CP), mixed integer programming (MIP), and satisfiability (SAT) modeling and solving techniques. In this paper we discuss the software framework and solver SCIP (Solving Constraint Integer Programs), which is free for academic and non-commercial use and can be downloaded in source code. This paper gives an overview of the main design concepts of SCIP and how it can be used to solve constraint integer programs. To illustrate the performance and flexibility of SCIP, we apply it to two different problem classes. First, we consider mixed integer programming and show by computational experiments that SCIP is almost competitive to specialized commercial MIP solvers, even though SCIP supports the more general constraint integer programming paradigm. We develop new ingredients that improve current MIP solving technology. As a second application, we employ SCIP to solve chip design verification problems as they arise in the logic design of integrated circuits. This application goes far beyond traditional MIP solving, as it includes several highly non-linear constraints, which can be handled nicely within the constraint integer programming framework. We show anecdotally how the different solving techniques from MIP, CP, and SAT work together inside SCIP to deal with such constraint classes. Finally, experimental results show that our approach outperforms current state-of-the-art techniques for proving the validity of properties on circuits containing arithmetic.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2009/achterberg_2009_scip.pdf},
  journal = {Mathematical Programming Computation},
  keywords = {90-08,Constraint programming,Integer programming,Primary: 90C11,SAT,Secondary: 90-04},
  language = {en},
  number = {1}
}

@inproceedings{addisu2017,
  title = {An Efficient Production Scheduling Based on Queuing Theory in Systems with Synchronous Part Transfer during a Demand Response Event},
  booktitle = {2017 {{IEEE International Conference}} on {{Smart Grid Communications}} ({{SmartGridComm}})},
  author = {Addisu, A. and Badis, H. and George, L. and Courbin, P.},
  year = {2017},
  month = oct,
  pages = {546--552},
  doi = {10.1109/SmartGridComm.2017.8340724},
  abstract = {Due to increased energy costs, manufacturing sectors are investigating viable options to optimize production while minimizing energy costs. Demand response provides one such option to tackle the problems. The objective of this paper is to propose a solution for synchronous part transfer to have higher production rate under demand response available power constraint. To attain our objectives, we first model a synchronous part transfer line using queuing theory to characterize temporal behaviors such as arrival and departure of parts, and utilization of stations (or machines). Then, we propose an efficient scheduling algorithm that provides an optimal schedule taking into account the available power. We also provide illustrating examples and analytical and simulation results to show the performance of the proposed algorithm.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/addisu_et_al_2017_an_efficient_production_scheduling_based_on_queuing_theory_in_systems_with.pdf;/Users/rwb/Zotero/storage/WYQWQ9TL/8340724.html},
  keywords = {Conferences,Demand Response,demand response available power constraint,demand side management,efficient production scheduling,Energy consumption,energy costs,Finite State Machine,Industrial Production Scheduling,Load management,Manufacturing,manufacturing sectors,optimal schedule,Production,production control,production rate,Queueing analysis,queueing theory,queuing theory,Queuing Theory,scheduling,Steady-state,synchronous part transfer line,viable options}
}

@article{adhikari2018,
  title = {Cloud {{Computing}}: {{A Multi}}-Workflow {{Scheduling Algorithm}} with {{Dynamic Reusability}}},
  shorttitle = {Cloud {{Computing}}},
  author = {Adhikari, Mainak and Koley, Santanu},
  year = {2018},
  month = feb,
  volume = {43},
  pages = {645--660},
  issn = {2191-4281},
  doi = {10.1007/s13369-017-2739-0},
  abstract = {Cloud computing provides a dynamic environment of well-organized deployment of hardware and software that are common in nature and the requirement for propping up heterogeneous workflow applications to realize high performance and improved throughput where the most demanding task is multiple workflow applications surrounded by their fixed deadline. These workflow applications consist of interconnected jobs and data. Nevertheless, hardly any initiations are tailored on multi-workflow scheduling exertion. These scheduling problems have been considered methodically in cloud atmosphere. Accessibility of the computing resources on the data center (DC) provides the exact time of execution of each process, whereas the execution time of every process within a workflow is pre-calculated in the majority of the existing multi-workflow scheduling problem. System overhead so far is an additional concern at the same time as dynamically generating virtual machines (VMs) with salvage them dipping the power eating. The aim of this paper is to reduce the execution time of every job and finalize the execution of all workflow within its deadline by producing VMs dynamically in DC and recycle them as necessary. We recommend a dynamic multi-workflow scheduling algorithm formally named as competent dynamic multi-workflow scheduling (CDMWS) algorithm. Simulation process describes one of the best algorithms so far in terms of performance among subsistent algorithm and moves toward a new era of multi-workflow relevance.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/adhikari_koley_2018_cloud_computing.pdf},
  journal = {Arabian Journal for Science and Engineering},
  keywords = {Cloud computing,Data center,Deadline,Heterogeneous workflow application,Multi-workflow scheduling,Virtual machine},
  language = {en},
  number = {2}
}

@inproceedings{afzal2006,
  title = {{{QoS}}-{{Constrained Stochastic Workflow Scheduling}} in {{Enterprise}} and {{Scientific Grids}}},
  booktitle = {2006 7th {{IEEE}}/{{ACM International Conference}} on {{Grid Computing}}},
  author = {Afzal, A. and Darlington, J. and McGough, A. S.},
  year = {2006},
  month = sep,
  pages = {1--8},
  doi = {10.1109/ICGRID.2006.310991},
  abstract = {Grid computing infrastructures are inherently dynamic and unpredictable environments shared by many users. Grid schedulers aim to make efficient use of grid resources while providing the best possible performance to the grid applications and satisfying the associated performance and policy constraints. Additionally, in commercial grid settings, where the grid resource brokering becomes an increasingly important part of grid scheduling, it is necessary to minimise the cost of application execution on the behalf of the grid users, while ensuring that the applications meet their QoS constraints. Efficient resource allocation could in turn also allow the resource broker to maximise its profit by minimising the number of resources procured. Scheduling in such a large-scale, dynamic and distributed environment is a complex undertaking. In this paper, we propose an approach to grid scheduling which abstracts over the details of individual applications, focusing instead on the global cost optimisation problem and the scheduling of the entire grid workload. Our model places particular emphasis on the stochastic and unpredictable nature of the grid, leading to a more accurate reflection of the state of the grid and hence more efficient and accurate scheduling decisions},
  file = {Dropbox/library/2006/Afzal et al/afzal_et_al_2006_qos-constrained_stochastic_workflow_scheduling_in_enterprise_and_scientific.pdf;/Users/rwb/Zotero/storage/I8R42HW3/4100448.html},
  keywords = {distributed environment,dynamic environment,enterprise grid,grid computing,grid resource,grid scheduling,QoS-constrained stochastic workflow scheduling,quality of service,scheduling,scientific grid}
}

@article{afzal2008,
  title = {Capacity Planning and Scheduling in {{Grid}} Computing Environments},
  author = {Afzal, Ali and McGough, A. Stephen and Darlington, John},
  year = {2008},
  month = may,
  volume = {24},
  pages = {404--414},
  issn = {0167-739X},
  doi = {10.1016/j.future.2007.07.004},
  abstract = {Grid computing infrastructures embody a cost-effective computing paradigm that virtualises heterogeneous system resources to meet the dynamic needs of critical business and scientific applications. These applications range from batch processes and long-running tasks to real-time and even transactional applications. Grid computing environments are inherently dynamic and unpredictable environments sharing services amongst many different users. Grid schedulers aim to make the most efficient use of Grid resources (high utilisation) while providing the best possible performance to the Grid applications (reducing makespan) and satisfying the associated performance and Quality of Service (QoS) constraints. Additionally, in commercial Grid settings where economic considerations are an increasingly important part of Grid scheduling, it is necessary to minimise the cost of application execution on the behalf of the Grid users while ensuring that the applications meet their QoS constraints. Furthermore, efficient resource allocation may allow a resource broker to maximise their profit by minimising the quantity of resource procurement. Scheduling in such a large-scale, dynamic and distributed environment is a complex undertaking. In this paper, we propose an approach to Grid scheduling which abstracts over the details of individual applications, focusing instead on the global cost optimisation problem while taking into account the entire workload, dynamically adjusting to the varying service demands. Our model places particular emphasis on the stochastic and unpredictable nature of the Grid, leading to a more accurate reflection of the state of the Grid and hence more efficient and accurate scheduling decisions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/afzal_et_al_2008_capacity_planning_and_scheduling_in_grid_computing_environments.pdf;/Users/rwb/Zotero/storage/L2G7L6KG/S0167739X07001239.html},
  journal = {Future Generation Computer Systems},
  keywords = {Advance reservations,Brokering,Grid computing,Optimisation,Performance,Quality-of-service,Queueing,Scheduling},
  number = {5}
}

@inproceedings{aguilar-reyes2017,
  title = {A {{Taxonomy}} of {{Workflow Scheduling Algorithms}}},
  booktitle = {High {{Performance Computing}}},
  author = {{Aguilar-Reyes}, Fernando and {Gutierrez-Garcia}, J. Octavio},
  editor = {Barrios Hern{\'a}ndez, Carlos Jaime and Gitler, Isidoro and Klapp, Jaime},
  year = {2017},
  pages = {104--115},
  publisher = {{Springer International Publishing}},
  abstract = {A workflow is a set of steps or tasks that model the execution of a process, e.g., protein annotation, invoice generation and composition of astronomical images. Workflow applications commonly require large computational resources. Hence, distributed computing approaches (such as Grid and Cloud computing) emerge as a feasible solution to execute them. Two important factors for executing workflows in distributed computing platforms are (1) workflow scheduling and (2) resource allocation. As a consequence, there is a myriad of workflow scheduling algorithms that map workflow tasks to distributed resources subject to task dependencies, time and budget constraints. In this paper, we present a taxonomy of workflow scheduling algorithms, which categorizes the algorithms into (1) best-effort algorithms (including heuristics, metaheuristics, and approximation algorithms) and (2) quality-of-service algorithms (including budget-constrained, deadline-constrained and algorithms simultaneously constrained by deadline and budget). In addition, a workflow engine simulator was developed to quantitatively compare the performance of scheduling algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/aguilar-reyes_gutierrez-garcia_2017_a_taxonomy_of_workflow_scheduling_algorithms.pdf},
  isbn = {978-3-319-57972-6},
  keywords = {Distributed computing,Scheduling algorithms,Workflow scheduling},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@article{al-dujaili2018,
  title = {Multi-{{Objective Simultaneous Optimistic Optimization}}},
  author = {{Al-Dujaili}, Abdullah and Suresh, S.},
  year = {2018},
  month = jan,
  volume = {424},
  pages = {159--174},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.09.066},
  abstract = {Optimistic methods have been applied with success to single-objective optimization. Here, we attempt to bridge the gap between optimistic methods and multi-objective optimization. In particular, this paper is concerned with solving black-box multi-objective problems given a finite number of function evaluations and proposes an optimistic approach, which we refer to as the Multi-Objective Simultaneous Optimistic Optimization (MO-SOO). Popularized by multi-armed bandits, MO-SOO follows the optimism in the face of uncertainty principle to recognize Pareto optimal solutions, by combining several multi-armed bandits in a hierarchical structure over the feasible decision space of a multi-objective problem. Based on three assumptions about the objective functions smoothness and hierarchical partitioning, the algorithm finite-time and asymptotic convergence behaviors are analyzed. The finite-time analysis establishes an upper bound on the Pareto-compliant unary additive epsilon indicator characterized by the objectives smoothness as well as the structure of the Pareto front with respect to its extrema. On the other hand, the asymptotic analysis indicates the consistency property of MO-SOO. Moreover, we validate the theoretical provable performance of the algorithm on a set of synthetic problems. Finally, three-hundred bi-objective benchmark problems from the literature are used to substantiate the performance of the optimistic approach and compare it with three state-of-the-art stochastic algorithms in terms of two Pareto-compliant quality indicators. Besides sound theoretical properties, MO-SOO shows a performance on a par with the top performing stochastic algorithm.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/al-dujaili_suresh_2018_multi-objective_simultaneous_optimistic_optimization.pdf;/Users/rwb/Zotero/storage/T28NNMEM/S0020025517309854.html},
  journal = {Information Sciences},
  keywords = {Asymptotic analysis,Finite-time analysis,Multi-armed bandits,Multi-objective optimization,Optimistic methods,Simultaneous optimistic optimization}
}

@article{albrecht2017,
  title = {A {{Roadmap}} for {{HEP Software}} and {{Computing R}}\&{{D}} for the 2020s},
  author = {Albrecht, Johannes and Alves Jr, Antonio Augusto and Amadio, Guilherme and Andronico, Giuseppe and {Anh-Ky}, Nguyen and Aphecetche, Laurent and Apostolakis, John and Asai, Makoto and Atzori, Luca and Babik, Marian and Bagliesi, Giuseppe and Bandieramonte, Marilena and Banerjee, Sunanda and Barisits, Martin and Bauerdick, Lothar A. T. and Belforte, Stefano and Benjamin, Douglas and Bernius, Catrin and Bhimji, Wahid and Bianchi, Riccardo Maria and Bird, Ian and Biscarat, Catherine and Blomer, Jakob and Bloom, Kenneth and Boccali, Tommaso and Bockelman, Brian and Bold, Tomasz and Bonacorsi, Daniele and Boveia, Antonio and Bozzi, Concezio and Bracko, Marko and Britton, David and Buckley, Andy and Buncic, Predrag and Calafiura, Paolo and Campana, Simone and Canal, Philippe and Canali, Luca and Carlino, Gianpaolo and Castro, Nuno and Cattaneo, Marco and Cerminara, Gianluca and Villanueva, Javier Cervantes and Chang, Philip and Chapman, John and Chen, Gang and Childers, Taylor and Clarke, Peter and Clemencic, Marco and Cogneras, Eric and Coles, Jeremy and Collier, Ian and Colling, David and Corti, Gloria and Cosmo, Gabriele and Costanzo, Davide and Couturier, Ben and Cranmer, Kyle and Cranshaw, Jack and Cristella, Leonardo and Crooks, David and {Cr{\'e}p{\'e}-Renaudin}, Sabine and Currie, Robert and {Dallmeier-Tiessen}, S{\"u}nje and De, Kaushik and De Cian, Michel and De Roeck, Albert and Peris, Antonio Delgado and Derue, Fr{\'e}d{\'e}ric and Di Girolamo, Alessandro and Di Guida, Salvatore and Dimitrov, Gancho and Doglioni, Caterina and Dotti, Andrea and Duellmann, Dirk and Duflot, Laurent and Dykstra, Dave and {Dziedziniewicz-Wojcik}, Katarzyna and Dziurda, Agnieszka and Egede, Ulrik and Elmer, Peter and Elmsheuser, Johannes and Elvira, V. Daniel and Eulisse, Giulio and Farrell, Steven and Ferber, Torben and Filipcic, Andrej and Fisk, Ian and Fitzpatrick, Conor and Flix, Jos{\'e} and Formica, Andrea and Forti, Alessandra and Franzoni, Giovanni and Frost, James and Fuess, Stu and Gaede, Frank and Ganis, Gerardo and Gardner, Robert and Garonne, Vincent and Gellrich, Andreas and Genser, Krzysztof and George, Simon and Geurts, Frank and Gheata, Andrei and Gheata, Mihaela and Giacomini, Francesco and Giagu, Stefano and Giffels, Manuel and Gingrich, Douglas and Girone, Maria and Gligorov, Vladimir V. and Glushkov, Ivan and Gohn, Wesley and Lopez, Jose Benito Gonzalez and Caballero, Isidro Gonz{\'a}lez and Fern{\'a}ndez, Juan R. Gonz{\'a}lez and Govi, Giacomo and Grandi, Claudio and Grasland, Hadrien and Gray, Heather and Grillo, Lucia and Guan, Wen and Gutsche, Oliver and Gyurjyan, Vardan and Hanushevsky, Andrew and Hariri, Farah and Hartmann, Thomas and Harvey, John and Hauth, Thomas and Hegner, Benedikt and Heinemann, Beate and Heinrich, Lukas and Heiss, Andreas and Hern{\'a}ndez, Jos{\'e} M. and Hildreth, Michael and Hodgkinson, Mark and Hoeche, Stefan and Holzman, Burt and Hristov, Peter and Huang, Xingtao and Ivanchenko, Vladimir N. and Ivanov, Todor and Iven, Jan and Jashal, Brij and Jayatilaka, Bodhitha and Jones, Roger and Jouvin, Michel and Jun, Soon Yung and Kagan, Michael and Kalderon, Charles William and Kane, Meghan and Karavakis, Edward and Katz, Daniel S. and Kcira, Dorian and Keeble, Oliver and Kersevan, Borut Paul and Kirby, Michael and Klimentov, Alexei and Klute, Markus and Komarov, Ilya and Konstantinov, Dmitri and Koppenburg, Patrick and Kowalkowski, Jim and Kreczko, Luke and Kuhr, Thomas and Kutschke, Robert and Kuznetsov, Valentin and Lampl, Walter and Lancon, Eric and Lange, David and Lassnig, Mario and Laycock, Paul and Leggett, Charles and Letts, James and Lewendel, Birgit and Li, Teng and Lima, Guilherme and Linacre, Jacob and Linden, Tomas and Livny, Miron and Presti, Giuseppe Lo and Lopienski, Sebastian and Love, Peter and Lyon, Adam and Magini, Nicol{\`o} and Marshall, Zachary L. and Martelli, Edoardo and {Martin-Haugh}, Stewart and Mato, Pere and Mazumdar, Kajari and McCauley, Thomas and McFayden, Josh and McKee, Shawn and McNab, Andrew and Mehdiyev, Rashid and Meinhard, Helge and Menasce, Dario and Lorenzo, Patricia Mendez and Mete, Alaettin Serhan and Michelotto, Michele and Mitrevski, Jovan and Moneta, Lorenzo and Morgan, Ben and Mount, Richard and Moyse, Edward and Murray, Sean and Nairz, Armin and Neubauer, Mark S. and Norman, Andrew and Novaes, S{\'e}rgio and Novak, Mihaly and Oyanguren, Arantza and Ozturk, Nurcan and Pages, Andres Pacheco and Paganini, Michela and Pansanel, Jerome and Pascuzzi, Vincent R. and Patrick, Glenn and Pearce, Alex and Pearson, Ben and Pedro, Kevin and Perdue, Gabriel and Yzquierdo, Antonio Perez-Calero and Perrozzi, Luca and Petersen, Troels and Petric, Marko and Petzold, Andreas and Piedra, J{\'o}natan and Piilonen, Leo and Piparo, Danilo and Pivarski, Jim and Pokorski, Witold and Polci, Francesco and Potamianos, Karolos and Psihas, Fernanda and Navarro, Albert Puig and Quast, G{\"u}nter and Raven, Gerhard and Reuter, J{\"u}rgen and Ribon, Alberto and Rinaldi, Lorenzo and Ritter, Martin and Robinson, James and Rodrigues, Eduardo and Roiser, Stefan and Rousseau, David and Roy, Gareth and Rybkine, Grigori and Sailer, Andre and Sakuma, Tai and Santana, Renato and Sartirana, Andrea and Schellman, Heidi and Schovancov{\'a}, Jaroslava and Schramm, Steven and Schulz, Markus and Sciab{\`a}, Andrea and Seidel, Sally and Sekmen, Sezen and Serfon, Cedric and Severini, Horst and {Sexton-Kennedy}, Elizabeth and Seymour, Michael and Sgalaberna, Davide and Shapoval, Illya and Shiers, Jamie and Shiu, Jing-Ge and Short, Hannah and Siroli, Gian Piero and Skipsey, Sam and Smith, Tim and Snyder, Scott and Sokoloff, Michael D. and Spentzouris, Panagiotis and Stadie, Hartmut and Stark, Giordon and Stewart, Gordon and Stewart, Graeme A. and S{\'a}nchez, Arturo and {S{\'a}nchez-Hern{\'a}ndez}, Alberto and Taffard, Anyes and Tamponi, Umberto and Templon, Jeff and Tenaglia, Giacomo and Tsulaia, Vakhtang and Tunnell, Christopher and Vaandering, Eric and Valassi, Andrea and Vallecorsa, Sofia and Valsan, Liviu and Van Gemmeren, Peter and Vernet, Renaud and Viren, Brett and Vlimant, Jean-Roch and Voss, Christian and Votava, Margaret and Vuosalo, Carl and Sierra, Carlos V{\'a}zquez and Wartel, Romain and Watts, Gordon T. and Wenaus, Torre and Wenzel, Sandro and Williams, Mike and Winklmeier, Frank and Wissing, Christoph and Wuerthwein, Frank and Wynne, Benjamin and Xiaomei, Zhang and Yang, Wei and Yazgan, Efe},
  year = {2017},
  month = dec,
  doi = {10.1007/s41781-018-0018-8},
  abstract = {Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R\&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R\&D activities required to prepare for this software upgrade.},
  archivePrefix = {arXiv},
  eprint = {1712.06982},
  eprinttype = {arxiv},
  file = {/Users/rwb/Zotero/storage/G9UZKTRA/albrecht_et_al_2017_a_roadmap_for_hep_software_and_computing_r&d_for_the_2020s.pdf},
  journal = {arXiv:1712.06982 [hep-ex, physics:physics]},
  keywords = {_tablet,High Energy Physics - Experiment,Physics - Computational Physics},
  language = {en},
  primaryClass = {hep-ex, physics:physics}
}

@misc{alexander2016,
  title = {Updated {{SDP Cost Basis}} of {{Estimate June}} 2016},
  author = {Alexander, Paul and Bolton, Rose and Graser, Ferdi and Taylor, John},
  year = {2016},
  publisher = {{SKA SDP Consortium}},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/alexander_et_al_2016_updated_sdp_cost_basis_of_estimate_june_2016.pdf}
}

@misc{alexander2016a,
  title = {{{SDP Architecture}}},
  author = {Alexander, P and Allan, V and Bolton, Rose and Broekema, C and {van Diepen}, G and Gounden, S and Mika, A and Nijboer, R and Nikolic, B and Ratcliffe, S and Scaife, A and Simmonds, R and Taylor, J and Wicenec, A},
  year = {2016},
  publisher = {{SKA SDP Consortium}},
  file = {/Users/rwb/Zotero/storage/UB2X8BN6/ska-tel-sdp-0000013_03_dre_sdparchitecture_0.pdf}
}

@misc{alexander2018,
  title = {{{SKA1 SDP High Level Overview}}},
  author = {Alexander, Paul and Nikolic, B and Allan, V and Broekema, C and Deegan, M and Wortmann, P},
  year = {2018},
  publisher = {{SKA SDP Consortium}}
}

@misc{alexander2019,
  title = {{{SKA1 SDP Architecture Reading Guide}}},
  author = {Alexander, P and Allan, V and Bolton, R and Ashdown, M and Badenhorst, U and Broekema, C and Christelis, L and Coles, J and Cornwell, T and Deegan, M and {van Diepen}, G and Dulwich, F and Ensor, A and Fenech, D and Gounden, S and Garbutt, J and Goliath, S and Graser, F and Guzman, J.C. and Harding, P and Kirkham, K and Loo, W.S. and Levin Preston, L and Lyon, R and Mika, A and Mitchell, David and Mort, B and Nikolic, B and Nijober, R and Sanchez, S and Simmonds, R and Stappers, B and Taylor, J and Wicenec, A and Wortman, P},
  year = {2019},
  publisher = {{SKA SDP Consortium}},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/alexander_et_al_2019_ska1_sdp_architecture_reading_guide.pdf}
}

@article{alkhanak2015,
  title = {Cost-Aware Challenges for Workflow Scheduling Approaches in Cloud Computing Environments: {{Taxonomy}} and Opportunities},
  shorttitle = {Cost-Aware Challenges for Workflow Scheduling Approaches in Cloud Computing Environments},
  author = {Alkhanak, Ehab Nabiel and Lee, Sai Peck and Khan, Saif Ur Rehman},
  year = {2015},
  month = sep,
  volume = {50},
  pages = {3--21},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.01.007},
  abstract = {Workflow Scheduling (WFS) mainly focuses on task allocation to achieve the desired workload balancing by pursuing optimal utilization of available resources. At the same time, relevant performance criteria and system distribution structure must be considered to solve specific WFS problems in cloud computing by providing different services to cloud users on pay-as-you-go and on-demand basis. In the literature, various WFS challenges affecting WFS execution cost have been discussed. However, prior work did not consider such challenges collectively. The main objective of this paper is to facilitate researchers in selecting appropriate cost-aware WFS approaches from the available pool of alternatives. To achieve this objective, we conducted an extensive review to investigate and analyze the underlying concepts of the relevant approaches. The cost-aware relevant challenges of WFS in cloud computing are classified based on Quality of Service (QoS) performance, system functionality and system architecture, which ultimately result in a taxonomy set. Some research opportunities are also discussed that help in identifying future research directions in the area of cloud computing. The findings of this review provide a roadmap for developing cost-aware models, which will motivate researchers to propose better cost-aware approaches for service consumers and/or utility providers in cloud computing.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/alkhanak_et_al_2015_cost-aware_challenges_for_workflow_scheduling_approaches_in_cloud_computing.pdf;/Users/rwb/Zotero/storage/4TUG24GJ/S0167739X15000242.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cloud computing,Cost benefit analysis,Quality of service,System architecture,Taxonomy,Workflow scheduling},
  series = {Quality of {{Service}} in {{Grid}} and {{Cloud}} 2015}
}

@article{alkhanak2016,
  title = {Cost Optimization Approaches for Scientific Workflow Scheduling in Cloud and Grid Computing: {{A}} Review, Classifications, and Open Issues},
  shorttitle = {Cost Optimization Approaches for Scientific Workflow Scheduling in Cloud and Grid Computing},
  author = {Alkhanak, Ehab Nabiel and Lee, Sai Peck and Rezaei, Reza and Parizi, Reza Meimandi},
  year = {2016},
  month = mar,
  volume = {113},
  pages = {1--26},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2015.11.023},
  abstract = {Workflow scheduling in scientific computing systems is one of the most challenging problems that focuses on satisfying user-defined quality of service requirements while minimizing the workflow execution cost. Several cost optimization approaches have been proposed to improve the economic aspect of Scientific Workflow Scheduling (SWFS) in cloud and grid computing. To date, the literature has not yet seen a comprehensive review that focuses on approaches for supporting cost optimization in the context of SWFS in cloud and grid computing. Furthermore, providing valuable guidelines and analysis to understand the cost optimization of SWFS approaches is not well-explored in the current literature. This paper aims to analyze the problem of cost optimization in SWFS by extensively surveying existing SWFS approaches in cloud and grid computing and provide a classification of cost optimization aspects and parameters of SWFS. Moreover, it provides a classification of cost based metrics that are categorized into monetary and temporal cost parameters based on various scheduling stages. We believe that our findings would help researchers and practitioners in selecting the most appropriate cost optimization approach considering identified aspects and parameters. In addition, we highlight potential future research directions in this on-going area of research.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/alkhanak_et_al_2016_cost_optimization_approaches_for_scientific_workflow_scheduling_in_cloud_and.pdf;/Users/rwb/Zotero/storage/P6ZC9A5Z/S0164121215002484.html},
  journal = {Journal of Systems and Software},
  keywords = {Cloud computing,Scheduling,Scientific workflow}
}

@inproceedings{amari2017,
  title = {An Optimal Scheduling Algorithm for Data Parallel Hardware Architectures},
  booktitle = {2017 {{International Conference}} on {{Internet}} of {{Things}}, {{Embedded Systems}} and {{Communications}} ({{IINTEC}})},
  author = {Amari, I. and Rebaya, A. and Gasmi, K. and Hasnaoui, S.},
  year = {2017},
  month = oct,
  pages = {111--117},
  doi = {10.1109/IINTEC.2017.8325923},
  abstract = {The problem of dataflow applications scheduling on multi-core architectures is notoriously difficult. This difficulty is related to the rapid evaluation of Telecommunication and multimedia systems accompanied by a rapid increase of user requirements in terms of latency, execution time, consumption, energy, etc. Having an optimal scheduling on multi-cores DSP (Digital signal Processors) platforms is a challenging task. In this context, we present a novel technique and algorithm in order to find a valid schedule that optimizes the key performance metrics such as the latency. Our contribution is based on Satisfiability Modulo Theories (SMT) solver technologies which is strongly driven by the industrial applications and needs. We use an approach which is based on the synchronous and hierarchical behavior of both Simulink and synchronous dataflow. Whence, results of running the scheduler using our proposed SMT solver algorithm refinements produce an optimal scheduling in terms of latency and numbers of cores.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/amari_et_al_2017_an_optimal_scheduling_algorithm_for_data_parallel_hardware_architectures.pdf;/Users/rwb/Zotero/storage/JUPMHY85/8325923.html},
  keywords = {data parallel hardware architectures,dataflow application scheduling,digital signal processing chips,digital signal processors,execution time,industrial applications,key performance metrics,multi-cores architectures,multicore architectures,multicore DSP platforms,multimedia systems,multiprocessing systems,optimal scheduling algorithm,Satisfiability Modulo Theories solver technologies,scheduling,SDF,Simulink model,SMT solver,SMT solver algorithm refinements,synchronous dataflow,user requirements,workflow}
}

@article{anand2017,
  title = {A Comparative Analysis of Optimization Solvers},
  author = {Anand, Rimmi and Aggarwal, Divya and Kumar, Vijay},
  year = {2017},
  month = jul,
  volume = {20},
  pages = {623--635},
  issn = {0972-0510},
  doi = {10.1080/09720510.2017.1395182},
  abstract = {Optimization software provides better design and development of optimization solutions for real-life problems. The software generates different solutions under different constraints. An attempt has been made to compare three well-known optimization solvers. The comparisons have been done in terms of capabilities and problem domain. The results reveal that CPLEX and GuRoBi provide competitive optimization solutions. However, CPLEX performs better than GuRoBi under high dimensionality problems. Besides this, CPLEX is able to solve Non-convex mixed integer quadratic problem.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/anand_et_al_2017_a_comparative_analysis_of_optimization_solvers.pdf;/Users/rwb/Zotero/storage/9VZIXIWL/09720510.2017.html},
  journal = {Journal of Statistics and Management Systems},
  keywords = {CPLEX,GuRoBi,Linear programming,Mixed integer programming,Optimization solver,XPRESS},
  number = {4}
}

@article{and2006,
  title = {Agent-Based Distributed Manufacturing Process Planning and Scheduling: A State-of-the-Art Survey},
  shorttitle = {Agent-Based Distributed Manufacturing Process Planning and Scheduling},
  author = {{and}, and},
  year = {2006},
  month = jul,
  volume = {36},
  pages = {563--577},
  issn = {1094-6977},
  doi = {10.1109/TSMCC.2006.874022},
  abstract = {Manufacturing process planning is the process of selecting and sequencing manufacturing processes such that they achieve one or more goals and satisfy a set of domain constraints. Manufacturing scheduling is the process of selecting a process plan and assigning manufacturing resources for specific time periods to the set of manufacturing processes in the plan. It is, in fact, an optimization process by which limited manufacturing resources are allocated over time among parallel and sequential activities. Manufacturing process planning and scheduling are usually considered to be two separate and distinct phases. Traditional optimization approaches to these problems do not consider the constraints of both domains simultaneously and result in suboptimal solutions. Without considering real-time machine workloads and shop floor dynamics, process plans may become suboptimal or even invalid at the time of execution. Therefore, there is a need for the integration of manufacturing process-planning and scheduling systems for generating more realistic and effective plans. After describing the complexity of the manufacturing process-planning and scheduling problems, this paper reviews the research literature on manufacturing process planning, scheduling as well as their integration, particularly on agent-based approaches to these difficult problems. Major issues in these research areas are discussed, and research opportunities and challenges are identified},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/and_2006_agent-based_distributed_manufacturing_process_planning_and_scheduling.pdf;/Users/rwb/Zotero/storage/AZ2PZATQ/1643848.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  keywords = {agent-based distributed manufacturing process planning,Agents,Constraint optimization,Costs,distributed manufacturing systems,Job shop scheduling,manufacturing process scheduling,manufacturing processes,Manufacturing processes,manufacturing resource allocation,manufacturing scheduling,Manufacturing systems,multi-agent systems,multiagent system,multiagent systems,Multiagent systems,optimisation,optimization process,problem complexity,process planning,Process planning,Processor scheduling,Pulp manufacturing,real-time machine workload,resource allocation,Resource management,scheduling,shop floor dynamics},
  number = {4}
}

@phdthesis{anghel2017,
  title = {On {{Large}}-{{Scale System Performance Analysis}} and {{Software Characterization}}},
  author = {Anghel, Andreea-Simona},
  year = {2017},
  doi = {10.3929/ethz-b-000212482},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/anghel_2017_on_large-scale_system_performance_analysis_and_software_characterization.pdf},
  language = {en},
  school = {ETH Zurich}
}

@article{anthony2017,
  ids = {anthony2017a,anthony2017b,anthony2017c},
  title = {Thinking {{Fast}} and {{Slow}} with {{Deep Learning}} and {{Tree Search}}},
  author = {Anthony, Thomas and Tian, Zheng and Barber, David},
  year = {2017},
  month = may,
  abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
  archivePrefix = {arXiv},
  eprint = {1705.08439},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/anthony_et_al_2017_thinking_fast_and_slow_with_deep_learning_and_tree_search.pdf;/Users/rwb/Dropbox/PhD/zotero/2017/anthony_et_al_2017_thinking_fast_and_slow_with_deep_learning_and_tree_search2.pdf;Dropbox/library/2017/Anthony et al/anthony_et_al_2017_thinking_fast_and_slow_with_deep_learning_and_tree_search.pdf;/Users/rwb/Zotero/storage/D9IJ57XX/1705.html;/Users/rwb/Zotero/storage/JL3DX5IT/1705.html},
  journal = {arXiv:1705.08439 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{arabnejad2014,
  title = {List {{Scheduling Algorithm}} for {{Heterogeneous Systems}} by an {{Optimistic Cost Table}}},
  author = {Arabnejad, H. and Barbosa, J. G.},
  year = {2014},
  month = mar,
  volume = {25},
  pages = {682--694},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2013.57},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  number = {3}
}

@article{arabnejad2016,
  title = {Low-Time Complexity Budget\textendash Deadline Constrained Workflow Scheduling on Heterogeneous Resources},
  author = {Arabnejad, Hamid and Barbosa, Jorge G. and Prodan, Radu},
  year = {2016},
  month = feb,
  volume = {55},
  pages = {29--40},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.07.021},
  abstract = {The execution of scientific applications, under the utility computing model, is constrained to Quality of Service (QoS) parameters. Commonly, applications have time and cost constraints such that all tasks of an application need to be finished within a user-specified Deadline and Budget. Several algorithms have been proposed for multiple QoS workflow scheduling, but most of them use search-based strategies that generally have a high time complexity, making them less useful in realistic scenarios. In this paper, we present a heuristic scheduling algorithm with quadratic time complexity that considers two important constraints for QoS-based workflow scheduling, time and cost, named Deadline\textendash Budget Constrained Scheduling (DBCS). From the deadline and budget defined by the user, the DBCS algorithm finds a feasible solution that accomplishes both constraints with a success rate similar to other state-of-the-art search-based algorithms in terms of the successful rate of feasible solutions, consuming in the worst case only approximately 4\% of the time. The DBCS algorithm has a low-time complexity of O(n2.p) for n tasks and p processors.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/arabnejad_et_al_2016_low-time_complexity_budgetdeadline_constrained_workflow_scheduling_on.pdf;/Users/rwb/Zotero/storage/4G36B9ZE/S0167739X15002587.html},
  journal = {Future Generation Computer Systems},
  keywords = {Clouds,Grids,List scheduling,Quality of Service,Search-based algorithms}
}

@article{aritsugi2001,
  title = {Several Partitioning Strategies for Parallel Image Convolution in a Network of Heterogeneous Workstations},
  author = {Aritsugi, Masayoshi and Fukatsu, Hiroki and Kanamori, Yoshinari},
  year = {2001},
  month = feb,
  volume = {27},
  pages = {269--293},
  issn = {0167-8191},
  doi = {10.1016/S0167-8191(00)00103-4},
  abstract = {Making use of many workstations connected by a network can give better performance than the same number of discrete workstations. We investigate various partitioning strategies for parallel digital image convolution in such a network. CORBA (Common Object Request Broker Architecture) is employed in implementing parallel processing with distributed workstations, allowing heterogeneous workstations to be used for parallel processing. We present a parallel and distributed image convolution processing model. We also describe several heterogeneous partitioning strategies and discuss the performance of each based on experimental results obtained by real implementation.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2001/aritsugi_et_al_2001_several_partitioning_strategies_for_parallel_image_convolution_in_a_network_of.pdf;/Users/rwb/Zotero/storage/6XJJXBL9/S0167819100001034.html},
  journal = {Parallel Computing},
  keywords = {CORBA,Digital image convolution,Distributed computing,Network of workstations,Performance evaluation,Unread},
  number = {3}
}

@article{ashraf,
  title = {Partitioining {{Problems}} in {{Heterogeneous Computer Systems}}},
  author = {Ashraf, Mohammad},
  pages = {6},
  abstract = {Heterogeneous Computer Systems (HCS) have the potential t o achieve enhanced performance and costeffectiveness over hLomogeneous systems for solving problems with diverse computational requirements.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/ashraf_partitioining_problems_in_heterogeneous_computer_systems.pdf},
  keywords = {Unread},
  language = {en}
}

@article{assuncao2015,
  title = {Big {{Data}} Computing and Clouds: {{Trends}} and Future Directions},
  shorttitle = {Big {{Data}} Computing and Clouds},
  author = {Assun{\c c}{\~a}o, Marcos D. and Calheiros, Rodrigo N. and Bianchi, Silvia and Netto, Marco A. S. and Buyya, Rajkumar},
  year = {2015},
  month = may,
  volume = {79-80},
  pages = {3--15},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.08.003},
  abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/assuno_et_al_2015_big_data_computing_and_clouds.pdf;/Users/rwb/Zotero/storage/NGVQKUUG/S0743731514001452.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {Analytics,Big Data,Cloud computing,Data management},
  series = {Special {{Issue}} on {{Scalable Systems}} for {{Big Data Management}} and {{Analytics}}}
}

@article{atkinson2017,
  ids = {atkinson2017},
  title = {Scientific Workflows: {{Past}}, Present and Future},
  shorttitle = {Scientific Workflows},
  author = {Atkinson, Malcolm and Gesing, Sandra and Montagnat, Johan and Taylor, Ian},
  year = {2017},
  month = oct,
  volume = {75},
  pages = {216--227},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.05.041},
  abstract = {This special issue and our editorial celebrate 10~years of progress with data-intensive or scientific workflows. There have been very substantial advances in the representation of workflows and in the engineering of workflow management systems (WMS). The creation and refinement stages are now well supported, with a significant improvement in usability. Improved abstraction supports cross-fertilisation between different workflow communities and consistent interpretation as WMS evolve. Through such re-engineering the WMS deliver much improved performance, significantly increased scale and sophisticated reliability mechanisms. Further improvement is anticipated from substantial advances in optimisation. We invited papers from those who have delivered these advances and selected 14 to represent today's achievements and representative plans for future progress. This editorial introduces those contributions with an overview and categorisation of the papers. Furthermore, it elucidates responses from a survey of major workflow systems, which provides evidence of substantial progress and a structured index of related papers. We conclude with suggestions on areas where further research and development is needed and offer a vision of future research directions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/atkinson_et_al_2017_scientific_workflows.pdf;/Users/rwb/Dropbox/PhD/zotero/2017/atkinson_et_al_2017_scientific_workflows2.pdf;/Users/rwb/Zotero/storage/NILV2T79/S0167739X17311202.html},
  journal = {Future Generation Computer Systems},
  keywords = {Optimisation,Performance,Scientific methods,Scientific workflows,Usability}
}

@article{augonnet,
  ids = {augonnet2011},
  title = {{{StarPU}}: A Unified Platform for Task Scheduling on Heterogeneous Multicore Architectures},
  shorttitle = {{{StarPU}}},
  author = {Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'e}},
  volume = {23},
  pages = {187--198},
  issn = {1532-0634},
  doi = {10.1002/cpe.1631},
  abstract = {In the field of HPC, the current hardware trend is to design multiprocessor architectures featuring heterogeneous technologies such as specialized coprocessors (e.g. Cell/BE) or data-parallel accelerators (e.g. GPUs). Approaching the theoretical performance of these architectures is a complex issue. Indeed, substantial efforts have already been devoted to efficiently offload parts of the computations. However, designing an execution model that unifies all computing units and associated embedded memory remains a main challenge. We therefore designed StarPU, an original runtime system providing a high-level, unified execution model tightly coupled with an expressive data management library. The main goal of StarPU is to provide numerical kernel designers with a convenient way to generate parallel tasks over heterogeneous hardware on the one hand, and easily develop and tune powerful scheduling algorithms on the other hand. We have developed several strategies that can be selected seamlessly at run-time, and we have analyzed their efficiency on several algorithms running simultaneously over multiple cores and a GPU. In addition to substantial improvements regarding execution times, we have obtained consistent superlinear parallelism by actually exploiting the heterogeneous nature of the machine. We eventually show that our dynamic approach competes with the highly optimized MAGMA library and overcomes the limitations of the corresponding static scheduling in a portable way. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\Concurrency and Computation Practice and Experience\\undefined\\augonnet_et_al_starpu.pdf;Dropbox/library/undefined/Augonnet et al/augonnet_et_al_starpu.pdf;/Users/rwb/Zotero/storage/4Q9A7GRP/cpe.html},
  journal = {Concurrency and Computation: Practice and Experience},
  keywords = {accelerator,GPU,multicore,runtime system,scheduling,Unread},
  language = {en},
  number = {2}
}

@article{aziza2018,
  ids = {aziza2018a},
  title = {Bi-Objective Decision Support System for Task-Scheduling Based on Genetic Algorithm in Cloud Computing},
  author = {Aziza, Hatem and Krichen, Saoussen},
  year = {2018},
  month = feb,
  volume = {100},
  pages = {65--91},
  issn = {1436-5057},
  doi = {10.1007/s00607-017-0566-5},
  abstract = {We address in this paper the task-scheduling in cloud computing. This problem is known to be NP\{\textbackslash mathcal \{NP\}\}-hard due to its combinatorial aspect. The main role of our model is to estimate the time needed to run a set of tasks in cloud and in turn reduces the processing cost. We propose a genetic approach for modelling and optimizing a task-scheduling problem in cloud computing. The experimental results demonstrate that our solution successfully competes with previous task-scheduling algorithms. For this, we develop a decision support system based on the core of CloudSim. In terms of processing cost, the obtained results show that our approach outperforms previous scheduling methods by a significant margin. In terms of makespan, the obtained schedules are also shorter than those of other algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/aziza_krichen_2018_bi-objective_decision_support_system_for_task-scheduling_based_on_genetic.pdf},
  journal = {Computing},
  keywords = {68M14 (Distributed Systems),68M20 (Performance evaluation; queueing; scheduling),Cloud computing,Decision support system,Genetic algorithm,Task-scheduling},
  language = {en},
  number = {2}
}

@article{bahadori2014,
  title = {Using {{Queuing Theory}} and {{Simulation Model}} to {{Optimize Hospital Pharmacy Performance}}},
  author = {Bahadori, Mohammadkarim and Mohammadnejhad, Seyed Mohsen and Ravangard, Ramin and Teymourzadeh, Ehsan},
  year = {2014},
  month = mar,
  volume = {16},
  issn = {2074-1804},
  doi = {10.5812/ircmj.16807},
  abstract = {Background:
Hospital pharmacy is responsible for controlling and monitoring the medication use process and ensures the timely access to safe, effective and economical use of drugs and medicines for patients and hospital staff.

Objectives:
This study aimed to optimize the management of studied outpatient pharmacy by developing suitable queuing theory and simulation technique.

Patients and Methods:
A descriptive-analytical study conducted in a military hospital in Iran, Tehran in 2013. A sample of 220 patients referred to the outpatient pharmacy of the hospital in two shifts, morning and evening, was selected to collect the necessary data to determine the arrival rate, service rate, and other data needed to calculate the patients flow and queuing network performance variables. After the initial analysis of collected data using the software SPSS 18, the pharmacy queuing network performance indicators were calculated for both shifts. Then, based on collected data and to provide appropriate solutions, the queuing system of current situation for both shifts was modeled and simulated using the software ARENA 12 and 4 scenarios were explored.

Results:
Results showed that the queue characteristics of the studied pharmacy during the situation analysis were very undesirable in both morning and evening shifts. The average numbers of patients in the pharmacy were 19.21 and 14.66 in the morning and evening, respectively. The average times spent in the system by clients were 39 minutes in the morning and 35 minutes in the evening. The system utilization in the morning and evening were, respectively, 25\% and 21\%. The simulation results showed that reducing the staff in the morning from 2 to 1 in the receiving prescriptions stage didn't change the queue performance indicators. Increasing one staff in filling prescription drugs could cause a decrease of 10 persons in the average queue length and 18 minutes and 14 seconds in the average waiting time. On the other hand, simulation results showed that in the evening, decreasing the staff from 2 to 1 in the delivery of prescription drugs, changed the queue performance indicators very little. Increasing a staff to fill prescription drugs could cause a decrease of 5 persons in the average queue length and 8 minutes and 44 seconds in the average waiting time.

Conclusions:
The patients' waiting times and the number of patients waiting to receive services in both shifts could be reduced by using multitasking persons and reallocating them to the time-consuming stage of filling prescriptions, using queuing theory and simulation techniques.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/bahadori_et_al_2014_using_queuing_theory_and_simulation_model_to_optimize_hospital_pharmacy.pdf},
  journal = {Iranian Red Crescent Medical Journal},
  number = {3},
  pmcid = {PMC4005453},
  pmid = {24829791}
}

@article{bahrami2018,
  title = {An {{Online Learning Algorithm}} for {{Demand Response}} in {{Smart Grid}}},
  author = {Bahrami, S. and Wong, V. W. S. and Huang, J.},
  year = {2018},
  month = sep,
  volume = {9},
  pages = {4712--4725},
  issn = {1949-3053},
  doi = {10.1109/TSG.2017.2667599},
  abstract = {Demand response program with real-time pricing can encourage electricity users toward scheduling their energy usage to off-peak hours. A user needs to schedule the energy usage of his appliances in an online manner since he may not know the energy prices and the demand of his appliances ahead of time. In this paper, we study the users' long-term load scheduling problem and model the changes of the price information and load demand as a Markov decision process, which enables us to capture the interactions among users as a partially observable stochastic game. To make the problem tractable, we approximate the users' optimal scheduling policy by the Markov perfect equilibrium (MPE) of a fully observable stochastic game with incomplete information. We develop an online load scheduling learning (LSL) algorithm based on the actor-critic method to determine the users' MPE policy. When compared with the benchmark of not performing demand response, simulation results show that the LSL algorithm can reduce the expected cost of users and the peak-to-average ratio in the aggregate load by 28\% and 13\%, respectively. When compared with the short-term scheduling policies, the users with the long-term policies can reduce their expected cost by 17\%.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/bahrami_et_al_2018_an_online_learning_algorithm_for_demand_response_in_smart_grid.pdf;/Users/rwb/Zotero/storage/FWL2ZYYH/7849144.html},
  journal = {IEEE Transactions on Smart Grid},
  keywords = {actor-critic method,Approximation algorithms,Companies,Demand response,Games,Heuristic algorithms,Home appliances,Load management,online learning,partially observable stochastic game,real-time pricing,Stochastic processes},
  number = {5}
}

@article{bandi2015,
  title = {Robust {{Queueing Theory}}},
  author = {Bandi, Chaithanya and Bertsimas, Dimitris and Youssef, Nataly},
  year = {2015},
  month = jun,
  volume = {63},
  pages = {676--700},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2015.1367},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/bandi_et_al_2015_robust_queueing_theory.pdf},
  journal = {Operations Research},
  language = {en},
  number = {3}
}

@article{bao2018,
  title = {Online {{Job Scheduling}} in {{Distributed Machine Learning Clusters}}},
  author = {Bao, Yixin and Peng, Yanghua and Wu, Chuan and Li, Zongpeng},
  year = {2018},
  month = jan,
  abstract = {Nowadays large-scale distributed machine learning systems have been deployed to support various analytics and intelligence services in IT firms. To train a large dataset and derive the prediction/inference model, e.g., a deep neural network, multiple workers are run in parallel to train partitions of the input dataset, and update shared model parameters. In a shared cluster handling multiple training jobs, a fundamental issue is how to efficiently schedule jobs and set the number of concurrent workers to run for each job, such that server resources are maximally utilized and model training can be completed in time. Targeting a distributed machine learning system using the parameter server framework, we design an online algorithm for scheduling the arriving jobs and deciding the adjusted numbers of concurrent workers and parameter servers for each job over its course, to maximize overall utility of all jobs, contingent on their completion times. Our online algorithm design utilizes a primal-dual framework coupled with efficient dual subroutines, achieving good long-term performance guarantees with polynomial time complexity. Practical effectiveness of the online algorithm is evaluated using trace-driven simulation and testbed experiments, which demonstrate its outperformance as compared to commonly adopted scheduling algorithms in today's cloud systems.},
  archivePrefix = {arXiv},
  eprint = {1801.00936},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/Bao et al_2018_Online Job Scheduling in Distributed Machine Learning Clusters.pdf},
  journal = {arXiv:1801.00936 [cs]},
  keywords = {_tablet,Computer Science - Distributed; Parallel; and Cluster Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{barba2006,
  title = {Planning and Scheduling the {{Spitzer Space Telescope}}},
  booktitle = {{{SPIE Astronomical Telescopes}} + {{Instrumentation}}},
  author = {Barba, Stephen J. and Garcia, Lisa J. and McElroy, Douglas B. and Mittman, David S. and O'Linger, JoAnn C. and Tyler, Steven R.},
  editor = {Silva, David R. and Doxsey, Rodger E.},
  year = {2006},
  month = jun,
  pages = {62700Z},
  address = {{Orlando, Florida , USA}},
  doi = {10.1117/12.660146}
}

@inproceedings{barba2011,
  title = {Planning and {{Scheduling}} of {{Business Processes}} in {{Run}}-{{Time}}: {{A Repair Planning Example}}},
  shorttitle = {Planning and {{Scheduling}} of {{Business Processes}} in {{Run}}-{{Time}}},
  booktitle = {Information {{Systems Development}}},
  author = {Barba, Irene and Del Valle, Carmelo},
  editor = {Pokorny, Jaroslav and Repa, Vaclav and Richta, Karel and Wojtkowski, Wita and Linger, Henry and Barry, Chris and Lang, Michael},
  year = {2011},
  pages = {75--87},
  publisher = {{Springer New York}},
  abstract = {Over the last decade, the efficient and flexible management of business processes has become one of the most critical success aspects. Furthermore, there exists a growing interest in the application of Artificial Intelligence Planning and Scheduling techniques to automate the production and execution of models of organization. However, from our point of view, several connections between both disciplines remains to be exploited. The current work presents a proposal for modelling and enacting business processes that involve the selection and order of the activities to be executed (planning), besides the resource allocation (scheduling), considering the optimization of several functions and the reach of some objectives. The main novelty is that all decisions (even the activities selection) are taken in run-time considering the actual parameters of the execution, so the business process is managed in an efficient and flexible way. As an example, a complex and representative problem, the repair planning problem, is managed through the proposed approach.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/barba_del_valle_2011_planning_and_scheduling_of_business_processes_in_run-time.pdf},
  isbn = {978-1-4419-9790-6},
  keywords = {Artificial Intelligence Technique,Business Process,Business Process Model,Enactment Phase,Execution Plan},
  language = {en}
}

@incollection{barbosa2008,
  title = {A {{List Scheduling Algorithm}} for {{Scheduling Multi}}-User {{Jobs}} on {{Clusters}}},
  booktitle = {High {{Performance Computing}} for {{Computational Science}} - {{VECPAR}} 2008},
  author = {Barbosa, Jorge and Monteiro, Ant{\'o}nio P.},
  editor = {Palma, Jos{\'e} M. Laginha M. and Amestoy, Patrick R. and Dayd{\'e}, Michel and Mattoso, Marta and Lopes, Jo{\~a}o Correia},
  year = {2008},
  volume = {5336},
  pages = {123--136},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-92859-1_13},
  abstract = {This paper addresses the problem of scheduling multi-user jobs on clusters, both homogeneous and heterogeneous. A user job is composed by a set of dependent tasks and it is described by a direct acyclic graph (DAG). The aim is to maximize the resource usage by allowing a floating mapping of processors to a given job, instead of the common mapping approach that assigns a fixed set of processors to a user for a period of time. The simulation results show a better cluster usage. The scheduling algorithm minimizes the total length of the schedule (makespan) of a given set of parallel jobs, whose priorities are represented in a DAG. The algorithm is presented as producing static schedules although it can be adapted to a dynamic behavior as discussed in the paper.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/barbosa_monteiro_2008_a_list_scheduling_algorithm_for_scheduling_multi-user_jobs_on_clusters.pdf},
  isbn = {978-3-540-92858-4 978-3-540-92859-1},
  language = {en}
}

@inproceedings{barrett2011,
  ids = {barrett2011a,barrett2011b},
  title = {A {{Learning Architecture}} for {{Scheduling Workflow Applications}} in the {{Cloud}}},
  booktitle = {2011 {{IEEE Ninth European Conference}} on {{Web Services}}},
  author = {Barrett, E. and Howley, E. and Duggan, J.},
  year = {2011},
  month = sep,
  pages = {83--90},
  doi = {10.1109/ECOWS.2011.27},
  abstract = {The scheduling of workflow applications involves the mapping of individual workflow tasks to computational resources, based on a range of functional and non-functional quality of service requirements. Workflow applications such as scientific workflows often require extensive computational processing and generate significant amounts of experimental data. The emergence of cloud computing has introduced a utility-type market model, where computational resources of varying capacities can be procured on demand, in a pay-per-use fashion. In workflow based applications dependencies exist amongst tasks which requires the generation of schedules in accordance with defined precedence constraints. These constraints pose a difficult planning problem, where tasks must be scheduled for execution only once all their parent tasks have completed. In general the two most important objectives of workflow schedulers are the minimisation of both cost and make span. The cost of workflow execution consists of both computational costs incurred from processing individual tasks, and data transmission costs. With scientific workflows potentially large amounts of data must be transferred between compute and storage sites. This paper proposes a novel cloud workflow scheduling approach which employs a Markov Decision Process to optimally guide the workflow execution process depending on environmental state. In addition the system employs a genetic algorithm to evolve workflow schedules. The overall architecture is presented, and initial results indicate the potential of this approach for developing viable workflow schedules on the Cloud.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/Barrett et al_2011_A Learning Architecture for Scheduling Workflow Applications in the Cloud.pdf;/Users/rwb/Dropbox/PhD/zotero/2011/barrett_et_al_2011_a_learning_architecture_for_scheduling_workflow_applications_in_the_cloud.pdf;/Users/rwb/Dropbox/PhD/zotero/2011/barrett_et_al_2011_a_learning_architecture_for_scheduling_workflow_applications_in_the_cloud2.pdf;/Users/rwb/Zotero/storage/FG7YCBLE/6061080.html;/Users/rwb/Zotero/storage/Q4JS5XLA/6061080.html;/Users/rwb/Zotero/storage/RMUU7YYP/6061080.html},
  keywords = {_tablet,Bayesian Model Learning,Biological cells,cloud computing,cloud workflow scheduling,computational costs,computational processing,computational resources,data transmission costs,environmental state,genetic algorithm,Genetic Algorithm,genetic algorithms,Genetic algorithms,learning architecture,Markov decision process,Markov Decision Process,Markov processes,minimisation,Optimal scheduling,pay-per-use fashion,precedence constraints,Processor scheduling,quality of service,quality of service requirements,schedules,Schedules,scheduling,Scheduling,scheduling workflow applications,scientific workflows,software architecture,storage management,storage sites,utility-type market model,workflow based applications dependency,workflow execution process,workflow management software,workflow schedulers,Workflow Scheduling,workflow tasks}
}

@inproceedings{behzad2013,
  title = {Taming Parallel {{I}}/{{O}} Complexity with Auto-Tuning},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} on - {{SC}} '13},
  author = {Behzad, Babak and Luu, Huong Vu Thanh and Huchette, Joseph and Byna, Surendra and {Prabhat} and Aydt, Ruth and Koziol, Quincey and Snir, Marc},
  year = {2013},
  pages = {1--12},
  publisher = {{ACM Press}},
  address = {{Denver, Colorado}},
  doi = {10.1145/2503210.2503278},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/behzad_et_al_2013_taming_parallel_i-o_complexity_with_auto-tuning.pdf},
  isbn = {978-1-4503-2378-9},
  language = {en}
}

@article{benini2011,
  title = {Optimal Resource Allocation and Scheduling for~the~{{CELL BE}} Platform},
  author = {Benini, Luca and Lombardi, Michele and Milano, Michela and Ruggiero, Martino},
  year = {2011},
  month = apr,
  volume = {184},
  pages = {51--77},
  issn = {1572-9338},
  doi = {10.1007/s10479-010-0718-x},
  abstract = {Resource allocation and scheduling for multicore platforms is one of the most critical challenges in today's embedded computing. In this paper we focus on a well-known multicore platform, namely the Cell BE processor, and we address the problem of allocating and scheduling its processors, communication channels and memories, with the goal of minimizing execution time for complex data streaming applications.We propose three complete approaches that optimally solve the problem and prove optimality. The first is based on the recursive application of the Logic Based Benders decomposition, resulting in a three stage algorithm. The second is a pure CP approach while the third is a hybrid approach integrating the first two.Extensive experimental evaluation shows the features of each approach and its effectiveness on a specific instance structure.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/benini_et_al_2011_optimal_resource_allocation_and_scheduling_for_the_cell_be_platform.pdf},
  journal = {Annals of Operations Research},
  keywords = {Constraint based scheduling,Constraint Programming,Hybrid solvers,Integer Linear Programming,Multicore platforms,Resource allocation},
  language = {en},
  number = {1}
}

@article{bennett2013,
  title = {Artificial Intelligence Framework for Simulating Clinical Decision-Making: {{A Markov}} Decision Process Approach},
  shorttitle = {Artificial Intelligence Framework for Simulating Clinical Decision-Making},
  author = {Bennett, Casey C. and Hauser, Kris},
  year = {2013},
  month = jan,
  volume = {57},
  pages = {9--19},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2012.12.003},
  abstract = {In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that o\ldots},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/2013_artificial_intelligence_framework_for_simulating_clinical_decision-making.pdf;/Users/rwb/Zotero/storage/YAUIKQXD/S0933365712001510.html},
  journal = {Artificial Intelligence in Medicine},
  keywords = {_tablet},
  language = {en},
  number = {1}
}

@article{benoit2013,
  title = {A {{Survey}} of {{Pipelined Workflow Scheduling}}: {{Models}} and {{Algorithms}}},
  shorttitle = {A {{Survey}} of {{Pipelined Workflow Scheduling}}},
  author = {Benoit, Anne and {\c C}ataly{\"u}rek, {\"U}mit V. and Robert, Yves and Saule, Erik},
  year = {2013},
  month = aug,
  volume = {45},
  pages = {50:1--50:36},
  issn = {0360-0300},
  doi = {10.1145/2501654.2501664},
  abstract = {A large class of applications need to execute the same workflow on different datasets of identical size. Efficient execution of such applications necessitates intelligent distribution of the application components and tasks on a parallel machine, and the execution can be orchestrated by utilizing task, data, pipelined, and/or replicated parallelism. The scheduling problem that encompasses all of these techniques is called pipelined workflow scheduling, and it has been widely studied in the last decade. Multiple models and algorithms have flourished to tackle various programming paradigms, constraints, machine behaviors, or optimization goals. This article surveys the field by summing up and structuring known results and approaches.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/benoit_et_al_2013_a_survey_of_pipelined_workflow_scheduling.pdf},
  journal = {ACM Comput. Surv.},
  keywords = {algorithms,distributed systems,filter-stream programming,latency,models,parallel systems,pipeline,scheduling,throughput,Workflow programming},
  number = {4}
}

@article{berrada1996,
  title = {A Multi-Objective Approach to Nurse Scheduling with Both Hard and Soft Constraints},
  author = {Berrada, Ilham and Ferland, Jacques A. and Michelon, Philippe},
  year = {1996},
  month = sep,
  volume = {30},
  pages = {183--193},
  issn = {00380121},
  doi = {10.1016/0038-0121(96)00010-9},
  file = {/Users/rwb/Dropbox/PhD/zotero/1996/berrada_et_al_1996_a_multi-objective_approach_to_nurse_scheduling_with_both_hard_and_soft.pdf},
  journal = {Socio-Economic Planning Sciences},
  language = {en},
  number = {3}
}

@inproceedings{bharathi2008,
  title = {Characterization of Scientific Workflows},
  booktitle = {2008 {{Third Workshop}} on {{Workflows}} in {{Support}} of {{Large}}-{{Scale Science}}},
  author = {Bharathi, Shishir and Chervenak, Ann and Deelman, Ewa and Mehta, Gaurang and Su, Mei-Hui and Vahi, Karan},
  year = {2008},
  month = nov,
  pages = {1--10},
  issn = {2151-1381},
  doi = {10.1109/WORKS.2008.4723958},
  abstract = {Researchers working on the planning, scheduling and execution of scientific workflows need access to a wide variety of scientific workflows to evaluate the performance of their implementations. We describe basic workflow structures that are composed into complex workflows by scientific communities. We provide a characterization of workflows from five diverse scientific applications, describing their composition and data and computational requirements. We also describe the effect of the size of the input datasets on the structure and execution profiles of these workflows. Finally, we describe a workflow generator that produces synthetic, parameterizable workflows that closely resemble the workflows that we characterize. We make these workflows available to the community to be used as benchmarks for evaluating various workflow systems and scheduling algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/bharathi_et_al_2008_characterization_of_scientific_workflows.pdf;/Users/rwb/Zotero/storage/KQTG5UIE/4723958.html},
  keywords = {Astronomy,Biology,Character generation,computational requirement,data analysis,data requirement,data structures,Earthquakes,Geometry,Libraries,natural sciences computing,Performance analysis,Physics,scheduling,Scheduling algorithm,scientific workflow characterization,scientific workflow planning,scientific workflow scheduling,workflow execution profile,workflow management software,Workflow management software}
}

@inproceedings{bicer2010,
  title = {Supporting Fault Tolerance in a Data-Intensive Computing Middleware},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel Distributed Processing}} ({{IPDPS}})},
  author = {Bicer, T. and Jiang, W. and Agrawal, G.},
  year = {2010},
  month = apr,
  pages = {1--12},
  doi = {10.1109/IPDPS.2010.5470462},
  abstract = {Over the last 2-3 years, the importance of data-intensive computing has increasingly been recognized, closely coupled with the emergence and popularity of map-reduce for developing this class of applications. Besides programmability and ease of parallelization, fault tolerance is clearly important for data-intensive applications, because of their long running nature, and because of the potential for using a large number of nodes for processing massive amounts of data. Fault-tolerance has been an important attribute of map-reduce as well in its Hadoop implementation, where it is based on replication of data in the file system. Two important goals in supporting fault-tolerance are low overheads and efficient recovery. With these goals, this paper describes a different approach for enabling data-intensive computing with fault-tolerance. Our approach is based on an API for developing data-intensive computations that is a variation of map-reduce, and it involves an explicit programmer-declared reduction object. We show how more efficient fault-tolerance support can be developed using this API. Particularly, as the reduction object represents the state of the computation on a node, we can periodically cache the reduction object from every node at another location and use it to support failure-recovery. We have extensively evaluated our approach using two data-intensive applications. Our results show that the overheads of our scheme are extremely low, and our system outperforms Hadoop both in absence and presence of failures.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/bicer_et_al_2010_supporting_fault_tolerance_in_a_data-intensive_computing_middleware.pdf;/Users/rwb/Zotero/storage/WLJ62TFA/5470462.html},
  keywords = {API,Cloud computing,Computer science,Data analysis,Data engineering,data-intensive computations,Data-intensive computing,data-intensive computing middleware,failure-recovery,fault tolerance,Fault tolerance,fault tolerant computing,File systems,Hadoop implementation,High performance computing,Image analysis,Large-scale systems,map-reduce,Map-Reduce,middleware,Middleware,programmability,programmer-declared reduction object,system recovery}
}

@article{bidot2008,
  title = {A Theoretic and Practical Framework for Scheduling in a Stochastic Environment},
  author = {Bidot, Julien and Vidal, Thierry and Laborie, Philippe and Beck, J. Christopher},
  year = {2008},
  month = aug,
  volume = {12},
  pages = {315},
  issn = {1099-1425},
  doi = {10.1007/s10951-008-0080-x},
  abstract = {There are many systems and techniques that address stochastic planning and scheduling problems, based on distinct and sometimes opposite approaches, especially in terms of how generation and execution of the plan, or the schedule, are combined, and if and when knowledge about the uncertainties is taken into account. In many real-life problems, it appears that many of these approaches are needed and should be combined, which to our knowledge has never been done. In this paper, we propose a typology that distinguishes between proactive, progressive, and revision approaches. Then, focusing on scheduling and schedule execution, a theoretic model integrating those three approaches is defined. This model serves as a general template to implement a system that will fit specific application needs: we introduce and discuss our experimental prototypes which validate our model in part, and suggest how this framework could be extended to more general planning systems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/bidot_et_al_2008_a_theoretic_and_practical_framework_for_scheduling_in_a_stochastic_environment.pdf},
  journal = {Journal of Scheduling},
  keywords = {_tablet,Combinatorial optimization,Constraint programming,Flexibility,Planning,Robustness,Scheduling,Simulation,Stability,Uncertainty},
  language = {en},
  number = {3}
}

@inproceedings{bidot2011,
  title = {Using {{AI}} Planning and Late Binding for Managing Service Workflows in Intelligent Environments},
  booktitle = {2011 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}} ({{PerCom}})},
  author = {Bidot, J. and Goumopoulos, C. and Calemis, I.},
  year = {2011},
  month = mar,
  pages = {156--163},
  doi = {10.1109/PERCOM.2011.5767580},
  abstract = {In this paper, we present an approach to aggregating and using devices that support the everyday life of human users in ambient intelligence environments. These execution environments are complex and changing over time, since the devices of the environments are numerous and heterogeneous, and they may appear or disappear at any time. In order to appropriately adapt the ambient system to a user's needs, we adopt a service-oriented approach; i.e., devices provide services that reflect their capabilities. The orchestration of the devices is actually realized with the help of Artificial Intelligence planning techniques and dynamic service binding. At design time, (i) a planning problem is created that consists of the user's goal to be achieved and the services currently offered by the intelligent environment, (ii) the planning problem is then solved using Hierarchical Task Network and Partial-Order Causal-Link planning techniques, (iii) and from the planning decisions taken to find solution plans, abstract service workflows are automatically generated. At run time, the abstract services are dynamically bound to devices that are actually present in the environment. Adaptation of the workflow instantiation is possible due to the late binding mechanism employed. The paper depicts the architecture of our system. It also describes the modeling and the life cycle of the workflows. We discuss the advantages and the limit of our approach with respect to related work and give specific details about implementation. We present some experimental results that validate our system in a real-world application scenario.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/bidot_et_al_2011_using_ai_planning_and_late_binding_for_managing_service_workflows_in.pdf;/Users/rwb/Zotero/storage/NYG6GNTV/5767580.html},
  keywords = {Adaptation model,Adaptive systems,adaptive workflows,AI planning,ambient intelligence,ambient intelligence environment,artificial intelligence,dynamic service binding,Hierarchical Task Network planning,hierarchical task network planning technique,late binding,Ontologies,ontology,Partial-Order Causal-Link planning,partial-order causal-link planning technique,Planning,planning (artificial intelligence),Search engines,Search problems,service workflow management,service-oriented approach,services composition framework,user goal,user interfaces,workflow instantiation,workflow management software}
}

@inproceedings{bierwirth2010,
  ids = {bierwirth2010a},
  title = {New Observing Concepts for {{ESO}} Survey Telescopes},
  booktitle = {{{SPIE Astronomical Telescopes}} + {{Instrumentation}}},
  author = {Bierwirth, T. and Szeifert, T. and Dorigo, D. and Nunes, P. and Rejkuba, M. and Baugh, K. and Klein Gebbinck, M. and Manning, A. and Muravov, D. and Vera, I.},
  editor = {Silva, David R. and Peck, Alison B. and Soifer, B. Thomas},
  year = {2010},
  month = jul,
  pages = {77370W},
  address = {{San Diego, California, USA}},
  doi = {10.1117/12.858815},
  abstract = {The start of operations of the VISTA survey telescope will not only offer a new facility to the ESO community, but also a new way of observing. Survey observation programs typically observe large areas of the sky and might span several years, corresponding to the execution of hundreds of observations blocks (OBs) in service mode. However, the execution time of an individual survey OB will often be rather short. We expect that up to twelve OBs may be executed per hour, as opposed to about one OB per hour on ESO's Very Large Telescope (VLT). OBs of different programs are competing for observation time and must be executed with adequate priority. For these reasons, the scheduling of survey OBs is required to be almost fully automated. Two new key concepts are introduced to address these challenges: ESO's phase 2 proposal preparation tool P2PP allows PIs of survey programs to express advanced mid-term observing strategies using scheduling containers of OBs (groups, timelinks, concatenations). Telescope operators are provided with effective shortterm decision support based on ranking observable OBs. The ranking takes into account both empirical probability distributions of various constraints and the observing strategy described by the scheduling containers. We introduce the three scheduling container types and describe how survey OBs are ranked. We demonstrate how the new concepts are implemented in the preparation and observing tools and give an overview of the end-to-end workflow.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/bierwirth_et_al_2010_new_observing_concepts_for_eso_survey_telescopes.pdf;/Users/rwb/Dropbox/PhD/zotero/2010/bierwirth_et_al_2010_new_observing_concepts_for_eso_survey_telescopes2.pdf},
  language = {en}
}

@article{bittencourt2010,
  title = {Towards the {{Scheduling}} of {{Multiple Workflows}} on {{Computational Grids}}},
  author = {Bittencourt, Luiz Fernando and Madeira, Edmundo R. M.},
  year = {2010},
  month = sep,
  volume = {8},
  pages = {419--441},
  issn = {1572-9184},
  doi = {10.1007/s10723-009-9144-1},
  abstract = {The workflow paradigm has become the standard to represent processes and their execution flows. With the evolution of e-Science, workflows are becoming larger and more computational demanding. Such e-Science necessities match with what computational Grids have to offer. Grids are shared distributed platforms which will eventually receive multiple requisitions to execute workflows. With this, there is a demand for a scheduler which deals with multiple workflows in the same set of resources, thus the development of multiple workflow scheduling algorithms is necessary. In this paper we describe four different initial strategies for scheduling multiple workflows on Grids and evaluate them in terms of schedule length and fairness. We present results for the initial schedule and for the makespan after the execution with external load. From the results we conclude that interleaving the workflows on the Grid leads to good average makespan and provides fairness when multiple workflows share the same set of resources.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/bittencourt_madeira_2010_towards_the_scheduling_of_multiple_workflows_on_computational_grids.pdf},
  journal = {Journal of Grid Computing},
  keywords = {_tablet,Grid computing,Scheduling,Workflow},
  language = {en},
  number = {3}
}

@article{bittencourt2012,
  title = {Scheduling in Hybrid Clouds},
  author = {Bittencourt, L. F. and Madeira, E. R. M. and Fonseca, N. L. S. Da},
  year = {2012},
  month = sep,
  volume = {50},
  pages = {42--47},
  issn = {0163-6804},
  doi = {10.1109/MCOM.2012.6295710},
  abstract = {Schedulers for cloud computing determine on which processing resource jobs of a workflow should be allocated. In hybrid clouds, jobs can be allocated on either a private cloud or a public cloud on a pay per use basis. The capacity of the communication channels connecting these two types of resources impacts the makespan and the cost of workflow execution. This article introduces the scheduling problem in hybrid clouds presenting the main characteristics to be considered when scheduling workflows, as well as a brief survey of some of the scheduling algorithms used in these systems. To assess the influence of communication channels on job allocation, we compare and evaluate the impact of the available bandwidth on the performance of some of the scheduling algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/bittencourt_et_al_2012_scheduling_in_hybrid_clouds.pdf;/Users/rwb/Zotero/storage/YLYNXRR7/6295710.html},
  journal = {IEEE Communications Magazine},
  keywords = {Bandwidth,cloud computing,Cloud computing,communication channel,hybrid clouds,job allocation,private cloud,public cloud,Resource allocation,scheduling,scheduling algorithm,Scheduling algorithms},
  number = {9}
}

@article{bittencourt2018,
  title = {Scheduling in Distributed Systems: {{A}} Cloud Computing Perspective},
  shorttitle = {Scheduling in Distributed Systems},
  author = {Bittencourt, Luiz F. and Goldman, Alfredo and Madeira, Edmundo R. M. and {da Fonseca}, Nelson L. S. and Sakellariou, Rizos},
  year = {2018},
  month = nov,
  volume = {30},
  pages = {31--54},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2018.08.002},
  abstract = {Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/bittencourt_et_al_2018_scheduling_in_distributed_systems.pdf;/Users/rwb/Zotero/storage/GMUHRE7K/S1574013718301187.html},
  journal = {Computer Science Review},
  keywords = {Cloud computing,Distributed systems,Scheduling}
}

@article{biundo2011,
  title = {Planning in the {{Real World}}},
  author = {Biundo, Susanne and Bidot, Julien and Schattenberg, Bernd},
  year = {2011},
  month = oct,
  volume = {34},
  pages = {443--454},
  issn = {0170-6012, 1432-122X},
  doi = {10.1007/s00287-011-0562-7},
  abstract = {In this article, we describe how real world planning problems can be solved by employing Artificial Intelligence planning techniques. We introduce the paradigm of hybrid planning, which is particularly suited for applications where plans are not intended to be automatically executed by systems, but are made for humans. Hybrid planning combines hierarchical planning \textendash{} the stepwise refinement of complex tasks \textendash{} with explicit reasoning about causal dependencies between actions, thereby reflecting exactly the kinds of reasoning humans perform when developing plans. We show how plans are generated and how failed plans are repaired in a way that guarantees stability. Our illustrating examples are taken from a domain model for disaster relief missions enforced upon extensive floods. Finally, we present a tool to support the challenging task of constructing planning domain models.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/biundo_et_al_2011_planning_in_the_real_world.pdf},
  journal = {Informatik-Spektrum},
  language = {en},
  number = {5}
}

@incollection{blazewicz2007,
  title = {Scheduling under {{Resource Constraints}}},
  booktitle = {Handbook on {{Scheduling}}: {{From Theory}} to {{Applications}}},
  editor = {B\l a{\.z}ewicz, Jacek and Ecker, Klaus H. and Pesch, Erwin and Schmidt, G{\"u}nter and W{\k{e}}glarz, Jan},
  year = {2007},
  pages = {425--475},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-32220-7_12},
  abstract = {The scheduling model we consider now is more complicated than the previous ones, because any task, besides processors, may require for its processing some additional scarce resources. Resources, depending on their nature, may be classified into types and categories. The classification into types takes into account only the functions resources fulfill: resources of the same type are assumed to fulfill the same functions. The classification into categories will concern two points of view. First, we differentiate three categories of resources from the viewpoint of resource constraints. We will call a resource renewable, if only its total usage, i.e. temporary availability at every moment, is constrained. A resource is called non-renewable, if only its total consumption, i.e. integral availability up to any given moment, is constrained (in other words this resource once used by some task cannot be assigned to any other task). A resource is called doubly constrained, if both total usage and total consumption are constrained. Secondly, we distinguish two resource categories from the viewpoint of resource divisibility: discrete (i.e. discretely-divisible) and continuous (i.e. continuously-divisible) resources. In other words, by a discrete resource we will understand a resource which can be allocated to tasks in discrete amounts from a given finite set of possible allocations, which in particular may consist of one element only. Continuous resources, on the other hand, can be allocated in arbitrary, a priori unknown, amounts from given intervals.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/2007_scheduling_under_resource_constraints.pdf},
  isbn = {978-3-540-32220-7},
  keywords = {Project Schedule,Resource Constraint,Resource Requirement,Schedule Problem,Single Machine},
  language = {en},
  series = {International {{Handbook}} on {{Information Systems}}}
}

@article{blythe,
  ids = {blythea,blytheb},
  title = {The {{Role}} of {{Planning}} in {{Grid Computing}}},
  author = {Blythe, Jim and Deelman, Ewa and Gil, Yolanda and Kesselman, Carl and Agarwal, Amit and Mehta, Gaurang and Vahi, Karan},
  pages = {10},
  abstract = {Grid computing gives users access to widely distributed networks of computing resources to solve large-scale tasks such as scientific computation. These tasks are defined as standalone components that can be combined to process the data in various ways. We have implemented a planning system to generate task workflows for the Grid automatically, allowing the user to specify the desired data products in simple terms. The planner uses heuristic control rules and searches a number of alternative complete plans in order to find a high-quality solution. We describe an implemented test case in gravitational wave interferometry and show how the planner is integrated in the Grid environment. We discuss promising future directions of this work. We believe AI planning will play a crucial role in developing complex application workflows for the Grid.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/blythe_et_al_the_role_of_planning_in_grid_computing.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/blythe_et_al_the_role_of_planning_in_grid_computing2.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/blythe_et_al_the_role_of_planning_in_grid_computing3.pdf},
  language = {en}
}

@article{blythe2003,
  title = {Planning for Workflow Construction and Maintenance on the {{Grid}}},
  author = {Blythe, Jim and Deelman, Ewa and Gil, Yolanda},
  year = {2003},
  pages = {7},
  abstract = {We describe an implemented grid planner that has been used to compose workflows and schedule tasks on a computational Grid to solve scientific problems. We then discuss two issues that will demand further attention to make Grid and web service planners a reality. First, the planner must interact not only with external services that are to be composed in the final workflow, but also with external reasoners or knowledge bases containing information that is needed for the planning task, for example resource constraints and policies. Second, the planning system must provide for monitoring and replanning strategies in order to manage the execution of a workflow in a dynamic environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2003/blythe_et_al_2003_planning_for_workflow_construction_and_maintenance_on_the_grid.pdf},
  language = {en}
}

@inproceedings{blythe2005,
  title = {Task Scheduling Strategies for Workflow-Based Applications in Grids},
  booktitle = {{{CCGrid}} 2005. {{IEEE International Symposium}} on {{Cluster Computing}} and the {{Grid}}, 2005.},
  author = {Blythe, J. and Jain, S. and Deelman, E. and Gil, Y. and Vahi, K. and Mandal, A. and Kennedy, K.},
  year = {2005},
  month = may,
  volume = {2},
  pages = {759-767 Vol. 2},
  doi = {10.1109/CCGRID.2005.1558639},
  abstract = {Grid applications require allocating a large number of heterogeneous tasks to distributed resources. A good allocation is critical for efficient execution. However, many existing grid toolkits use matchmaking strategies that do not consider overall efficiency for the set of tasks to be run. We identify two families of resource allocation algorithms: task-based algorithms, that greedily allocate tasks to resources, and workflow-based algorithms, that search for an efficient allocation for the entire workflow. We compare the behavior of workflow-based algorithms and task-based algorithms, using simulations of workflows drawn from a real application and with varying ratios of computation cost to data transfer cost. We observe that workflow-based approaches have a potential to work better for data-intensive applications even when estimates about future tasks are inaccurate.},
  file = {/Users/rwb/Zotero/storage/6NBELJ5W/1558639.html},
  keywords = {Astronomy,Computational efficiency,Computational modeling,distributed resource allocation,Gas insulated transmission lines,Geophysics computing,grid computing,Grid computing,Physics,resource allocation,Resource management,Runtime,scheduling,task scheduling strategy,task-based algorithm,Testing,Unread,workflow management software,workflow-based algorithm}
}

@article{boito2018,
  title = {A {{Checkpoint}} of {{Research}} on {{Parallel I}}/{{O}} for {{High}}-{{Performance Computing}}},
  author = {Boito, Francieli Zanon and Inacio, Eduardo C. and Bez, Jean Luca and Navaux, Philippe O. A. and Dantas, Mario A. R. and Denneulin, Yves},
  year = {2018},
  month = mar,
  volume = {51},
  pages = {23:1--23:35},
  issn = {0360-0300},
  doi = {10.1145/3152891},
  abstract = {We present a comprehensive survey on parallel I/O in the high-performance computing (HPC) context. This is an important field for HPC because of the historic gap between processing power and storage latency, which causes application performance to be impaired when accessing or generating large amounts of data. As the available processing power and amount of data increase, I/O remains a central issue for the scientific community. In this survey article, we focus on a traditional I/O stack, with a POSIX parallel file system. We present background concepts everyone could benefit from. Moreover, through the comprehensive study of publications from the most important conferences and journals in a 5-year time window, we discuss the state of the art in I/O optimization approaches, access pattern extraction techniques, and performance modeling, in addition to general aspects of parallel I/O research. With this approach, we aim at identifying the general characteristics of the field and the main current and future research topics.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/boito_et_al_2018_a_checkpoint_of_research_on_parallel_i-o_for_high-performance_computing.pdf},
  journal = {ACM Comput. Surv.},
  keywords = {high-performance computing,Parallel file systems,storage systems},
  number = {2}
}

@article{bolton2019,
  title = {Parametric Models of {{SDP}} Compute Requirements},
  author = {Bolton, R and Broekema, P C and Cornwell, T J and {van Diepen}, G and Holli, C and {Johnston-Holli}, M and Preston, L Levin and Mika, A and Nijboer, R and Nikolic, B and Salvini, S and Rampadarath, H and Scaife, A and Stappers, B and Wortmann, P},
  year = {2019},
  pages = {52},
  abstract = {Introduc on . Purpose of the Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scope of the Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/bolton_et_al_2019_parametric_models_of_sdp_compute_requirements.pdf},
  language = {en}
}

@article{braun2001,
  ids = {braun2001a,braun2001b},
  title = {A {{Comparison}} of {{Eleven Static Heuristics}} for {{Mapping}} a {{Class}} of {{Independent Tasks}} onto {{Heterogeneous Distributed Computing Systems}}},
  author = {Braun, Tracy D and Siegel, Howard Jay and Beck, Noah and B{\"o}l{\"o}ni, Ladislau L and Maheswaran, Muthucumaru and Reuther, Albert I and Robertson, James P and Theys, Mitchell D and Yao, Bin and Hensgen, Debra and Freund, Richard F},
  year = {2001},
  month = jun,
  volume = {61},
  pages = {810--837},
  issn = {0743-7315},
  doi = {10.1006/jpdc.2000.1714},
  abstract = {Mixed-machine heterogeneous computing (HC) environments utilize a distributed suite of different high-performance machines, interconnected with high-speed links, to perform different computationally intensive applications that have diverse computational requirements. HC environments are well suited to meet the computational demands of large, diverse groups of tasks. The problem of optimally mapping (defined as matching and scheduling) these tasks onto the machines of a distributed HC environment has been shown, in general, to be NP-complete, requiring the development of heuristic techniques. Selecting the best heuristic to use in a given environment, however, remains a difficult problem, because comparisons are often clouded by different underlying assumptions in the original study of each heuristic. Therefore, a collection of 11 heuristics from the literature has been selected, adapted, implemented, and analyzed under one set of common assumptions. It is assumed that the heuristics derive a mapping statically (i.e., off-line). It is also assumed that a metatask (i.e., a set of independent, noncommunicating tasks) is being mapped and that the goal is to minimize the total execution time of the metatask. The 11 heuristics examined are Opportunistic Load Balancing, Minimum Execution Time, Minimum Completion Time, Min\textendash min, Max\textendash min, Duplex, Genetic Algorithm, Simulated Annealing, Genetic Simulated Annealing, Tabu, and A*. This study provides one even basis for comparison and insights into circumstances where one technique will out-perform another. The evaluation procedure is specified, the heuristics are defined, and then comparison results are discussed. It is shown that for the cases studied here, the relatively simple Min\textendash min heuristic performs well in comparison to the other techniques.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2001/braun_et_al_2001_a_comparison_of_eleven_static_heuristics_for_mapping_a_class_of_independent.pdf;/Users/rwb/Zotero/storage/2HHBBK4N/S0743731500917143.html;/Users/rwb/Zotero/storage/NCLS33Y5/S0743731500917143.html;/Users/rwb/Zotero/storage/RYWP5Q3H/S0743731500917143.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {A*,Genetic Algorithm,heterogeneous computing,mapping heuristics,metatasks,simulated annealing,static matching,Tabu search,Unread},
  number = {6}
}

@article{bridi2016,
  title = {A {{Constraint Programming Scheduler}} for {{Heterogeneous High}}-{{Performance Computing Machines}}},
  author = {Bridi, T. and Bartolini, A. and Lombardi, M. and Milano, M. and Benini, L.},
  year = {2016},
  month = oct,
  volume = {27},
  pages = {2781--2794},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2016.2516997},
  abstract = {Scheduling and dispatching tools for high-performance computing (HPC) machines have the key role of mapping jobs to the available resources, trying to maximize performance and quality-of-service (QoS). Allocation and Scheduling in the general case are well-known NP-hard problems, forcing commercial schedulers to adopt greedy approaches to improve performance and QoS. Search-based approaches featuring the exploration of the solution space have seldom been employed in this setting, but mostly applied in off-line scenarios. In this paper, we present the first search-based approach to job allocation and scheduling for HPC machines, working in a production environment. The scheduler is based on Constraint Programming, an effective programming technique for optimization problems. The resulting scheduler is flexible, as it can be easily customized for dealing with heterogeneous resources, user-defined constraints and different metrics. We evaluate our solution both on virtual machines using synthetic workloads, and on the Eurora HPC with production workloads. Tests on a wide range of operating conditions show significant improvements in waitings and QoS in mid-tier HPC machines w.r.t state-of-the-art commercial rule-based dispatchers. Furthermore, we analyze the conditions under which our approach outperforms commercial approaches, to create a portfolio of scheduling algorithms that ensures robustness, flexibility and scalability.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/bridi_et_al_2016_a_constraint_programming_scheduler_for_heterogeneous_high-performance_computing.pdf;/Users/rwb/Zotero/storage/2R8WKHVQ/7378987.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {commercial rule-based dispatchers,constraint handling,Constraint programming,constraint programming scheduler,Eurora HPC machines,flexible scheduler,heterogeneous high-performance computing machines,heterogeneous resources,HPC,job allocation,job scheduling,mid-tier HPC machines,NP-hard problems,operating conditions,optimisation,optimization,Optimization,optimization problems,parallel processing,processor scheduling,Processor scheduling,production environment,production workloads,Programming,QoS,Quality of service,quality-of-service,resource allocation,Resource management,scheduling,Scheduling,scheduling algorithms,search problems,search-based approach,supercomputer,synthetic workloads,user-defined constraints,virtual machines},
  number = {10}
}

@article{broekema2015,
  title = {The {{Square Kilometre Array Science Data Processor}}. {{Preliminary}} Compute Platform Design},
  author = {Broekema, P.C. and van Nieuwpoort, R.V. and Bal, H.E.},
  year = {2015},
  month = jul,
  volume = {10},
  pages = {C07004-C07004},
  issn = {1748-0221},
  doi = {10.1088/1748-0221/10/07/C07004},
  abstract = {The Square Kilometre Array is a next-generation radio-telescope, to be built in South Africa and Western Australia. It is currently in its detailed design phase, with procurement and construction scheduled to start in 2017. The SKA Science Data Processor is the high-performance computing element of the instrument, responsible for producing science-ready data. This is a major IT project, with the Science Data Processor expected to challenge the computing state-of-the art even in 2020. In this paper we introduce the preliminary Science Data Processor design and the principles that guide the design process, as well as the constraints to the design. We introduce a highly scalable and flexible system architecture capable of handling the SDP workload.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/broekema_et_al_2015_the_square_kilometre_array_science_data_processor.pdf},
  journal = {Journal of Instrumentation},
  language = {en},
  number = {07}
}

@book{brucker2007,
  title = {Scheduling Algorithms},
  author = {Brucker, Peter},
  year = {2007},
  edition = {5th ed},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  file = {Dropbox/library/2007/Brucker/brucker_2007_scheduling_algorithms.pdf},
  isbn = {978-3-540-69515-8},
  keywords = {Computer algorithms,Computer capacity,Planning,Production scheduling},
  language = {en},
  lccn = {QA76.9.C63 B78 2007}
}

@article{bruno1974,
  title = {Scheduling {{Independent Tasks}} to {{Reduce Mean Finishing Time}}},
  author = {Bruno, J. and Coffman, Jr., E. G. and Sethi, R.},
  year = {1974},
  month = jul,
  volume = {17},
  pages = {382--387},
  issn = {0001-0782},
  doi = {10.1145/361011.361064},
  abstract = {Sequencing to minimize mean finishing time (or mean time in system) is not only desirable to the user, but it also tends to minimize at each point in time the storage required to hold incomplete tasks. In this paper a deterministic model of independent tasks is introduced and new results are derived which extend and generalize the algorithms known for minimizing mean finishing time. In addition to presenting and analyzing new algorithms it is shown that the most general mean-finishing-time problem for independent tasks is polynomial complete, and hence unlikely to admit of a non-enumerative solution.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1974/bruno_et_al_1974_scheduling_independent_tasks_to_reduce_mean_finishing_time.pdf},
  journal = {Commun. ACM},
  keywords = {deterministic scheduling models,minimizing mean finishing time,minimizing mean flow time,optimal scheduling algorithms,sequencing algorithms},
  number = {7}
}

@article{buchner,
  title = {Dynamic {{Scheduling}}  and {{Planning}} Parallel Observations  on Large {{Radio Telescope Arrays}} with the {{Square Kilometre Array}} in Mind},
  author = {Buchner, Johannes},
  pages = {111},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/buchner_dynamic_scheduling_and_planning_parallel_observations_on_large_radio.pdf},
  language = {en}
}

@article{burke2010,
  title = {A Hybrid Model of Integer Programming and Variable Neighbourhood Search for Highly-Constrained Nurse Rostering Problems},
  author = {Burke, Edmund K. and Li, Jingpeng and Qu, Rong},
  year = {2010},
  month = jun,
  volume = {203},
  pages = {484--493},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2009.07.036},
  abstract = {This paper presents a hybrid multi-objective model that combines integer programming (IP) and variable neighbourhood search (VNS) to deal with highly-\ldots},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/burke_et_al_2010_a_hybrid_model_of_integer_programming_and_variable_neighbourhood_search_for.pdf;/Users/rwb/Zotero/storage/4DTIXAKW/S0377221709005396.html},
  journal = {European Journal of Operational Research},
  language = {en},
  number = {2}
}

@article{burkimsher,
  title = {Fair, {{Responsive Scheduling}} of {{Engineering Workflows}} on {{Computing Grids}}},
  author = {Burkimsher, Andrew Marc},
  pages = {238},
  abstract = {This thesis considers scheduling in the context of a grid computing system used in engineering design. Users desire responsiveness and fairness in the treatment of the workflows they submit. Submissions outstrip the available computing capacity during the work day, and the queue is only caught up on overnight and at weekends. The execution times observed span a wide range of 100 to 107 core-minutes.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/burkimsher_fair,_responsive_scheduling_of_engineering_workows_on_computing_grids.pdf},
  language = {en}
}

@incollection{busoniu2008,
  title = {Continuous-{{State Reinforcement Learning}} with {{Fuzzy Approximation}}},
  booktitle = {Adaptive {{Agents}} and {{Multi}}-{{Agent Systems III}}. {{Adaptation}} and {{Multi}}-{{Agent Learning}}},
  author = {Bu{\c s}oniu, Lucian and Ernst, Damien and De Schutter, Bart and Babu{\v s}ka, Robert},
  editor = {Tuyls, Karl and Nowe, Ann and Guessoum, Zahia and Kudenko, Daniel},
  year = {2008},
  volume = {4865},
  pages = {27--43},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-77949-0_3},
  abstract = {Reinforcement Learning (RL) is a widely used learning paradigm for adaptive agents. There exist several convergent and consistent RL algorithms which have been intensively studied. In their original form, these algorithms require that the environment states and agent actions take values in a relatively small discrete set. Fuzzy representations for approximate, model-free RL have been proposed in the literature for the more difficult case where the state-action space is continuous. In this work, we propose a fuzzy approximation architecture similar to those previously used for Q-learning, but we combine it with the model-based Q-value iteration algorithm. We prove that the resulting algorithm converges. We also give a modified, asynchronous variant of the algorithm that converges at least as fast as the original version. An illustrative simulation example is provided.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/buoniu_et_al_2008_continuous-state_reinforcement_learning_with_fuzzy_approximation.pdf},
  isbn = {978-3-540-77947-6 978-3-540-77949-0},
  language = {en}
}

@article{bux2013,
  title = {Parallelization in {{Scientific Workflow Management Systems}}},
  author = {Bux, Marc and Leser, Ulf},
  year = {2013},
  month = mar,
  abstract = {Over the last two decades, scientific workflow management systems (SWfMS) have emerged as a means to facilitate the design, execution, and monitoring of reusable scientific data processing pipelines. At the same time, the amounts of data generated in various areas of science outpaced enhancements in computational power and storage capabilities. This is especially true for the life sciences, where new technologies increased the sequencing throughput from kilobytes to terabytes per day. This trend requires current SWfMS to adapt: Native support for parallel workflow execution must be provided to increase performance; dynamically scalable ``pay-per-use'' compute infrastructures have to be integrated to diminish hardware costs; adaptive scheduling of workflows in distributed compute environments is required to optimize resource utilization. In this survey we give an overview of parallelization techniques for SWfMS, both in theory and in their realization in concrete systems. We find that current systems leave considerable room for improvement and we propose key advancements to the landscape of SWfMS.},
  archivePrefix = {arXiv},
  eprint = {1303.7195},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/bux_leser_2013_parallelization_in_scientific_workflow_management_systems.pdf},
  journal = {arXiv:1303.7195 [cs]},
  keywords = {68N19,C.1.4,Computer Science - Distributed; Parallel; and Cluster Computing,D.1.3,D.3.2,J.3},
  language = {en},
  primaryClass = {cs}
}

@article{buyya2002,
  title = {{{GridSim}}: A Toolkit for the Modeling and Simulation of Distributed Resource Management and Scheduling for {{Grid}} Computing},
  shorttitle = {{{GridSim}}},
  author = {Buyya, Rajkumar and Murshed, Manzur},
  year = {2002},
  month = nov,
  volume = {14},
  pages = {1175--1220},
  issn = {1532-0626, 1532-0634},
  doi = {10.1002/cpe.710},
  abstract = {Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular paradigms for next generation parallel and distributed computing. They enable aggregation of distributed resources for solving large-scale problems in science, engineering, and commerce. In grid and P2P computing environments, the resources are usually geographically distributed in multiple administrative domains, managed and owned by different organizations with different policies, and interconnected by wide-area networks or the Internet. This introduces a number of resource management and application scheduling challenges in the domain of security, resource and policy heterogeneity, fault tolerance, continuously changing resource conditions, and policies. The resource management and scheduling systems for grid computing need to manage resources and application execution depending on either resource consumers' or owners' requirements, and continuously adapt to changes in resource availability.},
  file = {/Users/rwb/Zotero/storage/DHNZKDLI/Buyya and Murshed - 2002 - GridSim a toolkit for the modeling and simulation.pdf},
  journal = {Concurrency and Computation: Practice and Experience},
  language = {en},
  number = {13-15}
}

@article{caamano,
  title = {Fast and {{Flexible Compilation Techniques}} for {{Effective Speculative Polyhedral Parallelization}}},
  author = {Caama{\~n}o, Juan Manuel Martinez},
  pages = {130},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/caamao_fast_and_flexible_compilation_techniques_for_effective_speculative_polyhedral.pdf},
  language = {en}
}

@article{calheiros2011,
  title = {{{CloudSim}}: A Toolkit for Modeling and Simulation of Cloud Computing Environments and Evaluation of Resource Provisioning Algorithms},
  shorttitle = {{{CloudSim}}},
  author = {Calheiros, Rodrigo N. and Ranjan, Rajiv and Beloglazov, Anton and De Rose, C{\'e}sar A. F. and Buyya, Rajkumar},
  year = {2011},
  month = jan,
  volume = {41},
  pages = {23--50},
  issn = {00380644},
  doi = {10.1002/spe.995},
  abstract = {Cloud computing is a recent advancement wherein IT infrastructure and applications are provided as `services' to end-users under a usage-based payment model. It can leverage virtualized services even on the fly based on requirements (workload patterns and QoS) varying with time. The application services hosted under Cloud computing model have complex provisioning, composition, configuration, and deployment requirements. Evaluating the performance of Cloud provisioning policies, application workload models, and resources performance models in a repeatable manner under varying system and user configurations and requirements is difficult to achieve. To overcome this challenge, we propose CloudSim: an extensible simulation toolkit that enables modeling and simulation of Cloud computing systems and application provisioning environments. The CloudSim toolkit supports both system and behavior modeling of Cloud system components such as data centers, virtual machines (VMs) and resource provisioning policies. It implements generic application provisioning techniques that can be extended with ease and limited effort. Currently, it supports modeling and simulation of Cloud computing environments consisting of both single and inter-networked clouds (federation of clouds). Moreover, it exposes custom interfaces for implementing policies and provisioning techniques for allocation of VMs under inter-networked Cloud computing scenarios. Several researchers from organizations, such as HP Labs in U.S.A., are using CloudSim in their investigation on Cloud resource provisioning and energy-efficient management of data center resources. The usefulness of CloudSim is demonstrated by a case study involving dynamic provisioning of application services in the hybrid federated clouds environment. The result of this case study proves that the federated Cloud computing model significantly improves the application QoS requirements under fluctuating resource and service demand patterns. Copyright q 2010 John Wiley \& Sons, Ltd.},
  file = {/Users/rwb/Zotero/storage/MDW8PPUD/Calheiros et al. - 2011 - CloudSim a toolkit for modeling and simulation of.pdf},
  journal = {Software: Practice and Experience},
  language = {en},
  number = {1}
}

@article{cameron2019,
  title = {{{MOANA}}: {{Modeling}} and {{Analyzing I}}/{{O Variability}} in {{Parallel System Experimental Design}}},
  shorttitle = {{{MOANA}}},
  author = {Cameron, K. W. and Anwar, A. and Cheng, Y. and Xu, L. and Li, B. and Ananth, U. and Bernard, J. and Jearls, C. and Lux, T. and Hong, Y. and Watson, L. T. and Butt, A. R.},
  year = {2019},
  month = aug,
  volume = {30},
  pages = {1843--1856},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2019.2892129},
  abstract = {Exponential increases in complexity and scale make variability a growing threat to sustaining HPC performance at exascale. Performance variability in HPC I/O is common, acute, and formidable. We take the first step towards comprehensively studying linear and nonlinear approaches to modeling HPC I/O system variability in an effort to demonstrate that variability is often a predictable artifact of system design. Using over 8 months of data collection on 6 identical systems, we propose and validate a modeling and analysis approach (MOANA) that predicts HPC I/O variability for thousands of software and hardware configurations on highly parallel shared-memory systems. Our findings indicate nonlinear approaches to I/O variability prediction are an order of magnitude more accurate than linear regression techniques. We demonstrate the use of MOANA to accurately predict the confidence intervals of unmeasured I/O system configurations for a given number of repeat runs \textendash{} enabling users to quantitatively balance experiment duration with statistical confidence.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/cameron_et_al_2019_moana.pdf;/Users/rwb/Zotero/storage/NA5925PI/8631172.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Analytical models,Benchmark testing,Force,Hardware,Jitter,machine learning,performance modeling,Reactive power,Throughput,Variability},
  number = {8}
}

@inproceedings{caniou2018,
  title = {Budget-{{Aware Scheduling Algorithms}} for {{Scientific Workflows}} with {{Stochastic Task Weights}} on {{Heterogeneous IaaS Cloud Platforms}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Caniou, Y. and Caron, E. and Chang, A. K. W. and Robert, Y.},
  year = {2018},
  month = may,
  pages = {15--26},
  doi = {10.1109/IPDPSW.2018.00014},
  abstract = {This paper introduces several budget-aware algorithms to deploy scientific workflows on IaaS cloud platforms, where users can request Virtual Machines (VMs) of different types, each with specific cost and speed parameters. We use a realistic application/platform model with stochastic task weights, and VMs communicating through a datacenter. We extend two well-known algorithms, MinMin and HEFT, and make scheduling decisions based upon machine availability and available budget. During the mapping process, the budget-aware algorithms make conservative assumptions to avoid exceeding the initial budget; we further improve our results with refined versions that aim at re-scheduling some tasks onto faster VMs, thereby spending any budget fraction leftover by the first allocation. These refined variants are much more time-consuming than the former algorithms, so there is a trade-off to find in terms of scalability. We report an extensive set of simulations with workflows from the Pegasus benchmark suite. Most of the time our budget-aware algorithms succeed in achieving efficient makespans while enforcing the given budget, despite (i) the uncertainty in task weights and (ii) the heterogeneity of VMs in both cost and speed values.},
  file = {/Users/rwb/Zotero/storage/FFJ2YVNK/8425321.html},
  keywords = {budget aware algorithm,budget-aware algorithms,budget-aware scheduling algorithms,cloud computing,Cloud computing,heterogeneous IaaS cloud platforms,multi criteria scheduling,realistic application-platform model,Resource management,scheduling,Scheduling,Scheduling algorithms,scientific workflows,speed parameters,Stochastic processes,stochastic task weights,Task analysis,virtual machines,Virtual machining,VM,workflow}
}

@article{cardoen2010,
  title = {Operating Room Planning and Scheduling: {{A}} Literature Review},
  shorttitle = {Operating Room Planning and Scheduling},
  author = {Cardoen, Brecht and Demeulemeester, Erik and Beli{\"e}n, Jeroen},
  year = {2010},
  month = mar,
  volume = {201},
  pages = {921--932},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2009.04.011},
  abstract = {This paper provides a review of recent operational research on operating room planning and scheduling. We evaluate the literature on multiple fields that are related to either the problem setting (e.g., performance measures or patient classes) or the technical features (e.g., solution technique or uncertainty incorporation). Since papers are pooled and evaluated in various ways, a diversified and detailed overview is obtained that facilitates the identification of manuscripts related to the reader's specific interests. Throughout the literature review, we summarize the significant trends in research on operating room planning and scheduling, and we identify areas that need to be addressed in the future.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/cardoen_et_al_2010_operating_room_planning_and_scheduling.pdf;/Users/rwb/Zotero/storage/RY7RKJDY/S0377221709002616.html},
  journal = {European Journal of Operational Research},
  keywords = {_tablet,Literature review,Operating room,OR in health services,Planning,Scheduling},
  number = {3}
}

@article{casanova,
  title = {{{SimGrid}}: A {{Generic Framework}} for {{Large}}-{{Scale Distributed Experiments}}},
  author = {Casanova, Henri and Legrand, Arnaud and Quinson, Martin},
  pages = {7},
  abstract = {Distributed computing is a very broad and active research area comprising fields such as cluster computing, computational grids, desktop grids and peer-to-peer (P2P) systems. Unfortunately, it is often impossible to obtain theoretical or analytical results to compare the performance of algorithms targeting such systems. One possibility is to conduct large numbers of back-to-back experiments on real platforms. While this is possible on tightlycoupled platforms, it is infeasible on modern distributed platforms as experiments are labor-intensive and results typically not reproducible. Consequently, one must resort to simulations, which enable reproducible results and also make it possible to explore wide ranges of platform and application scenarios.},
  file = {/Users/rwb/Zotero/storage/4UN9RJ52/Casanova et al. - SimGrid a Generic Framework for Large-Scale Distr.pdf},
  language = {en}
}

@inproceedings{casanova2018,
  title = {{{WRENCH}}: {{A Framework}} for {{Simulating Workflow Management Systems}}},
  shorttitle = {{{WRENCH}}},
  booktitle = {2018 {{IEEE}}/{{ACM Workflows}} in {{Support}} of {{Large}}-{{Scale Science}} ({{WORKS}})},
  author = {Casanova, Henri and Pandey, Suraj and Oeth, James and Tanaka, Ryan and Suter, Fr{\'e}d{\'e}ric and {Ferreira da Silva}, Rafael},
  year = {2018},
  month = nov,
  pages = {74--85},
  doi = {10.1109/WORKS.2018.00013},
  abstract = {Scientific workflows are used routinely in numerous scientific domains, and Workflow Management Systems (WMSs) have been developed to orchestrate and optimize workflow executions on distributed platforms. WMSs are complex software systems that interact with complex software infrastructures. Most WMS research and development activities rely on empirical experiments conducted with full-fledged software stacks on actual hardware platforms. Such experiments, however, are limited to hardware and software infrastructures at hand and can be labor- and/or time-intensive. As a result, relying solely on real-world experiments impedes WMS research and development. An alternative is to conduct experiments in simulation. In this work we present WRENCH, a WMS simulation framework, whose objectives are (i) accurate and scalable simulations; and (ii) easy simulation software development. WRENCH achieves its first objective by building on the SimGrid framework. While SimGrid is recognized for the accuracy and scalability of its simulation models, it only provides low-level simulation abstractions and thus large software development efforts are required when implementing simulators of complex systems. WRENCH thus achieves its second objective by providing high- level and directly re-usable simulation abstractions on top of SimGrid. After describing and giving rationales for WRENCH's software architecture and APIs, we present a case study in which we apply WRENCH to simulate the Pegasus production WMS. We report on ease of implementation, simulation accuracy, and simulation scalability so as to determine to which extent WRENCH achieves its two above objectives. We also draw both qualitative and quantitative comparisons with a previously proposed workflow simulator.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/casanova_et_al_2018_wrench.pdf;/Users/rwb/Zotero/storage/RLI2AHTF/8638379.html},
  keywords = {accurate simulations,actual hardware platforms,application program interfaces,complex software infrastructures,complex systems,Computational modeling,development activities,distributed platforms,Distributed-Computing,easy simulation software development,empirical experiments,extent WRENCH,full-fledged software stacks,grid computing,Hardware,low-level simulation abstractions,natural sciences computing,Pegasus production WMS,Production,re-usable simulation abstractions,real-world experiments,Research and development,Scalability,scalable simulations,scientific workflows,Scientific-Workflows,SimGrid framework,simulating workflow management systems,Simulation,simulation accuracy,simulation models,simulation scalability,Software,software architecture,software development efforts,WMS simulation framework,WMSs,workflow executions,workflow management software,Workflow management software,workflow simulator,Workflow-Management-Systems,WRENCH software architecture}
}

@article{casas2017,
  title = {A Balanced Scheduler with Data Reuse and Replication for Scientific Workflows in Cloud Computing Systems},
  author = {Casas, Israel and Taheri, Javid and Ranjan, Rajiv and Wang, Lizhe and Zomaya, Albert Y.},
  year = {2017},
  month = sep,
  volume = {74},
  pages = {168--178},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.12.005},
  abstract = {Cloud computing provides substantial opportunities to researchers who demand pay-as-you-go computing systems. Although cloud provider (e.g., Amazon Web Services) and application provider (e.g., biologists, physicists, and online gaming companies) both have specific performance requirements (e.g. application response time), it is the cloud scheduler's responsibility to map the application to underlying cloud resources. This article presents a Balanced and file Reuse\textendash Replication Scheduling (BaRRS) algorithm for cloud computing environments to optimally schedule scientific application workflows. BaRRS splits scientific workflows into multiple sub-workflows to balance system utilization via parallelization. It also exploits data reuse and replication techniques to optimize the amount of data that needs to be transferred among tasks at run-time. BaRRS analyzes the key application features (e.g., task execution times, dependency patterns and file sizes) of scientific workflows for adapting existing data reuse and replication techniques to cloud systems. Further, BaRRS performs a trade-off analysis to select the optimal solution based on two optimization constraints: execution time and monetary cost of running scientific workflows. BaRRS is compared with a state-of-the-art scheduling approach; experiments prove its superior performance. Experiments include four well known scientific workflows with different dependency patterns and data file sizes. Results were promising and also highlighted most critical factors affecting execution of scientific applications on clouds.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/casas_et_al_2017_a_balanced_scheduler_with_data_reuse_and_replication_for_scientific_workflows.pdf;/Users/rwb/Zotero/storage/AGE4FPUI/S0167739X1500388X.html},
  journal = {Future Generation Computer Systems},
  keywords = {Big data,Cloud computing,Data-intensive computing,Scheduling,Scientific workflow,Virtual machine}
}

@article{casavant1988,
  title = {A Taxonomy of Scheduling in General-Purpose Distributed Computing Systems},
  author = {Casavant, T. L. and Kuhl, J. G.},
  year = {1988},
  month = feb,
  volume = {14},
  pages = {141--154},
  issn = {0098-5589},
  doi = {10.1109/32.4634},
  abstract = {One measure of the usefulness of a general-purpose distributed computing system is the system's ability to provide a level of performance commensurate to the degree of multiplicity of resources present in the system. A taxonomy of approaches to the resource management problem is presented in an attempt to provide a common terminology and classification mechanism necessary in addressing this problem. The taxonomy, while presented and discussed in terms of distributed scheduling, is also applicable to most types of resource management},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\IEEE Transactions on Software Engineering\\1988\\casavant_kuhl_1988_a_taxonomy_of_scheduling_in_general-purpose_distributed_computing_systems.pdf;Dropbox/library/1988/Casavant_Kuhl/casavant_kuhl_1988_a_taxonomy_of_scheduling_in_general-purpose_distributed_computing_systems.pdf;/Users/rwb/Zotero/storage/SC3UGBL5/4634.html},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {Cities and towns,Control theory,distributed computing,Distributed computing,distributed processing,distributed scheduling,Energy management,operating systems,operating systems (computers),Operations research,Power system management,Processor scheduling,resource management,Resource management,scheduling,Taxonomy,Terminology,Unread},
  number = {2}
}

@article{cesta2007,
  title = {Mexar2: {{AI Solves Mission Planner Problems}}},
  shorttitle = {Mexar2},
  author = {Cesta, A. and Cortellessa, G. and Denis, M. and Donati, A. and Fratini, S. and Oddi, A. and Policella, N. and Rabenau, E. and Schulster, J.},
  year = {2007},
  month = jul,
  volume = {22},
  pages = {12--19},
  issn = {1541-1672},
  doi = {10.1109/MIS.2007.75},
  abstract = {Deep-space missions carry an ever larger set of different and complementary onboard payloads. Each payload generates data, and synthesizing it for optimized downlinking is one way to reduce the ratio of mission costs to science return. This is the main role of the Mars-Express scheduling architecture (Mexar2), an Al-based tool in daily use on the Mars-Express mission since February 2005. Mexar2 supports space mission planners continuously as they plan data downlinks from the spacecraft to Earth. The tool lets planners work at a higher abstraction level while it performs low-level, often-repetitive tasks. It also helps them produce a plan rapidly, explore alternative solutions, and choose the most robust plan for execution. Additionally, planners can analyze any problems over multiple days and identify payload overcommitments that cause resource bottlenecks and increase the risk of data losses. Mexar2 has significantly increased the data return over the whole Mars-Express mission duration. It's effectively become a work companion for mission planners at the European Space Agency's European Space Operations Center (ESOC) in Darmstadt, Germany.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/cesta_et_al_2007_mexar2.pdf;/Users/rwb/Zotero/storage/Z7PKSX45/4287268.html},
  journal = {IEEE Intelligent Systems},
  keywords = {aerospace computing,AI,artificial intelligence,Artificial intelligence,Communication channels,Councils,deep-space missions,European Space Agency,European Space Operations Center,extraterrestrial exploration,Geoscience,Intelligent systems,Mars,Mars-Express scheduling architecture,Mexar2,mission planner problems,Payloads,Production,Safety,scheduling,Space missions,space vehicle communication},
  number = {4}
}

@inproceedings{chaari2014,
  title = {Scheduling under Uncertainty: {{Survey}} and Research Directions},
  shorttitle = {Scheduling under Uncertainty},
  booktitle = {2014 {{International Conference}} on {{Advanced Logistics}} and {{Transport}} ({{ICALT}})},
  author = {Chaari, Tarek and Chaabane, Sondes and Aissani, Nassima and Trentesaux, Damien},
  year = {2014},
  month = may,
  pages = {229--234},
  doi = {10.1109/ICAdLT.2014.6866316},
  abstract = {In real-world scheduling problems, several kinds of hard-to-predict risk must be considered. Scheduling under uncertainty allows these kinds of risks to be taken into account. This paper provides an overview of the state of the art in scheduling under uncertainty, including a survey on modeling techniques of uncertainty and a survey of the existing positioning typologies and contributions. A new classification scheme for the different approaches to scheduling under uncertainty is proposed and discussed. Several areas for future research are suggested.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/chaari_et_al_2014_scheduling_under_uncertainty.pdf;/Users/rwb/Zotero/storage/6MKZLV5C/6866316.html},
  keywords = {classification scheme,hard-to-predict risk,Job shop scheduling,pattern classification,positioning typologies,predictive-reactive scheduling,proactive scheduling,proactive-reactive scheduling,Processor scheduling,reactive scheduling,real-world scheduling problems,risk analysis,Robustness,Schedules,scheduling,uncertainty,Uncertainty,uncertainty modeling techniques}
}

@article{chandrashekar,
  title = {Robust and {{Fault}}-{{Tolerant Scheduling}} for {{Scientific Workflows}} in {{Cloud Computing Environments}}},
  author = {Chandrashekar, Deepak Poola},
  pages = {191},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/chandrashekar_robust_and_fault-tolerant_scheduling_for_scientic_workows_in_cloud_computing.pdf},
  language = {en}
}

@misc{chapman2013,
  title = {{{CSIRO ASKAP Science Data Archive Requirements}}: {{ASKAP}}-{{SW}}-0017},
  author = {Chapman, Jess and Humphreys, Ben and Whiting, Matthew and Miller, Dan and Norris, Ray},
  year = {2013},
  publisher = {{CSIRO}},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/chapman_et_al_2013_csiro_askap_science_data_archive_requirements.pdf}
}

@book{chapman2016,
  title = {Common Workflow Language, v1.0},
  author = {Chapman, Brad and Chilton, John and Heuer, Michael and Kartashov, Andrey and Leehr, Dan and M{\'e}nager, Herv{\'e} and Nedeljkovich, Maya and Scales, Matt and {Soiland-Reyes}, Stian and Stojanovic, Luka},
  editor = {Amstutz, Peter and Crusoe, Michael R. and Tijani{\'c}, Neboj{\v s}a},
  year = {2016},
  month = jul,
  publisher = {{figshare}},
  address = {{United States}},
  doi = {10.6084/m9.figshare.3115156.v2},
  abstract = {The Common Workflow Language (CWL) is an informal, multi-vendor working group consisting of various organizations and individuals that have an interest in portability of data analysis workflows. Our goal is to create specifications that enable data scientists to describe analysis tools and workflows that are powerful, easy to use, portable, and support reproducibility.CWL builds on technologies such as JSON-LD and Avro for data modeling and Docker for portable runtime environments. CWL is designed to express workflows for data-intensive science, such as Bioinformatics, Medical Imaging, Chemistry, Physics, and Astronomy.This is v1.0 of the CWL tool and workflow specification, released on 2016-07-08},
  keywords = {cwl,specification,workflow},
  language = {English}
}

@article{chaudhary1993,
  title = {A Generalized Scheme for Mapping Parallel Algorithms},
  author = {Chaudhary, V. and Aggarwal, J. K.},
  year = {1993},
  month = mar,
  volume = {4},
  pages = {328--346},
  issn = {1045-9219},
  doi = {10.1109/71.210815},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  number = {3}
}

@article{chen,
  title = {Deep {{Reinforcement Learning}} for {{Multi}}-{{Resource Multi}}-{{Machine Job Scheduling}}},
  author = {Chen, Weijia and Xu, Yuedong and Wu, Xiaofeng},
  pages = {2},
  abstract = {Minimizing job scheduling time is a fundamental issue in data center networks that has been extensively studied in recent years. The incoming jobs require different CPU and memory units, and span different number of time slots. The traditional solution is to design efficient heuristic algorithms with performance guarantee under certain assumptions. In this paper, we improve a recently proposed job scheduling algorithm using deep reinforcement learning and extend it to multiple server clusters. Our study reveals that deep reinforcement learning method has the potential to outperform traditional resource allocation algorithms in a variety of complicated environments.},
  file = {/Users/rwb/Zotero/storage/YBTYJDWK/Chen et al. - Deep Reinforcement Learning for Multi-Resource Mul.pdf},
  language = {en}
}

@inproceedings{chen2012,
  title = {Integration of {{Workflow Partitioning}} and {{Resource Provisioning}}},
  booktitle = {2012 12th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} (Ccgrid 2012)},
  author = {Chen, W. and Deelman, E.},
  year = {2012},
  month = may,
  pages = {764--768},
  doi = {10.1109/CCGrid.2012.57},
  abstract = {The recent increased use of workflow management systems by large scientific collaborations presents the challenge of scheduling large-scale workflows onto distributed resources. This work aims to partition large-scale scientific workflows in conjunction with resources provisioning to reduce the workflow make span and resource cost.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/chen_deelman_2012_integration_of_workflow_partitioning_and_resource_provisioning.pdf;/Users/rwb/Zotero/storage/ESJLAVHS/6217508.html},
  keywords = {Biological cells,Distributed computing,distributed resources,Dynamic scheduling,Heuristic algorithms,large-scale scientific workflow partitioning,large-scale workflow scheduling,Partitioning algorithms,Processor scheduling,resource allocation,resource cost,resource provisioning,scientific collaborations,scientific information systems,scientific workflows,workflow makespan,workflow management software,workflow management systems,workflow partitioning}
}

@inproceedings{chen2012a,
  title = {{{WorkflowSim}}: {{A}} Toolkit for Simulating Scientific Workflows in Distributed Environments},
  shorttitle = {{{WorkflowSim}}},
  booktitle = {2012 {{IEEE}} 8th {{International Conference}} on {{E}}-{{Science}}},
  author = {Chen, Weiwei and Deelman, Ewa},
  year = {2012},
  month = oct,
  pages = {1--8},
  doi = {10.1109/eScience.2012.6404430},
  abstract = {Simulation is one of the most popular evaluation methods in scientific workflow studies. However, existing workflow simulators fail to provide a framework that takes into consideration heterogeneous system overheads and failures. They also lack the support for widely used workflow optimization techniques such as task clustering. In this paper, we introduce WorkflowSim, which extends the existing CloudSim simulator by providing a higher layer of workflow management. We also indicate that to ignore system overheads and failures in simulating scientific workflows could cause significant inaccuracies in the predicted workflow runtime. To further validate its value in promoting other research work, we introduce two promising research areas for which WorkflowSim provides a unique and effective evaluation platform.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/chen_deelman_2012_workflowsim.pdf;/Users/rwb/Zotero/storage/3LBC83ST/6404430.html},
  keywords = {cloud computing,CloudSim simulator,clustering,Computational modeling,Delay,distributed environments,distributed processing,Engines,failure,Heuristic algorithms,optimisation,overhead,Processor scheduling,Runtime,simulating scientific workflow toolkit,simulation,Training,workflow,workflow management,workflow optimization techniques,WorkflowSim}
}

@article{chen2015,
  title = {Adaptive Multiple-Workflow Scheduling with Task Rearrangement},
  author = {Chen, Wei and Lee, Young Choon and Fekete, Alan and Zomaya, Albert Y.},
  year = {2015},
  month = apr,
  volume = {71},
  pages = {1297--1317},
  issn = {1573-0484},
  doi = {10.1007/s11227-014-1361-0},
  abstract = {Large-scale distributed computing systems like grids and more recently clouds are a platform of choice for many resource-intensive applications. Workflow applications account for the majority of these applications, particularly in science and engineering. A workflow application consists of multiple precedence-constrained tasks with data dependencies. Since resources in those systems are shared by many users and applications deployed there are very diverse, scheduling is complicated. Often, the actual execution of applications differs from the original schedule following delays such as those caused by resource contention and other issues in performance prediction. These delays have further impact when running multiple workflow applications due to inter-task dependencies. In this paper, we investigate the problem of scheduling multiple workflow applications concurrently, explicitly taking into account scheduling robustness. We present a dynamic task rearrangement and rescheduling algorithm that exploits the scheduling flexibility from precedence constraints among tasks. The algorithm optimizes resource allocation among multiple workflows, and it often stops the influence of delayed execution passing to subsequent tasks. The experimental results demonstrate that our approach can significantly improve performance in multiple-workflow scheduling.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/chen_et_al_2015_adaptive_multiple-workflow_scheduling_with_task_rearrangement.pdf},
  journal = {The Journal of Supercomputing},
  keywords = {Rescheduling,Scheduling,Workflow applications,Workflow scheduling},
  language = {en},
  number = {4}
}

@article{chen2017,
  title = {Efficient Task Scheduling for Budget Constrained Parallel Applications on Heterogeneous Cloud Computing Systems},
  author = {Chen, Weihong and Xie, Guoqi and Li, Renfa and Bai, Yang and Fan, Chunnian and Li, Keqin},
  year = {2017},
  month = sep,
  volume = {74},
  pages = {1--11},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.03.008},
  abstract = {As the cost-driven public cloud services emerge, budget constraint is one of the primary design issues in large-scale scientific applications executed on heterogeneous cloud computing systems. Minimizing the schedule length while satisfying the budget constraint of an application is one of the most important quality of service requirements for cloud providers. A directed acyclic graph (DAG) can be used to describe an application consisted of multiple tasks with precedence constrains. Previous DAG scheduling methods tried to presuppose the minimum cost assignment for each task to minimize the schedule length of budget constrained applications on heterogeneous cloud computing systems. However, our analysis revealed that the preassignment of tasks with the minimum cost does not necessarily lead to the minimization of the schedule length. In this study, we propose an efficient algorithm of minimizing the schedule length using the budget level (MSLBL) to select processors for satisfying the budget constraint and minimizing the schedule length of an application. Such problem is decomposed into two sub-problems, namely, satisfying the budget constraint and minimizing the schedule length. The first sub-problem is solved by transferring the budget constraint of the application to that of each task, and the second sub-problem is solved by heuristically scheduling each task with low-time complexity. Experimental results on several real parallel applications validate that the proposed MSLBL algorithm can obtain shorter schedule lengths while satisfying the budget constraint of an application than existing methods in various situations.},
  file = {/Users/rwb/Zotero/storage/DGNHXKSU/S0167739X16304411.html},
  journal = {Future Generation Computer Systems},
  keywords = {Budget constraint,Heterogeneous clouds,Parallel application,Schedule length}
}

@inproceedings{cheng2018,
  title = {{{DRL}}-Cloud: {{Deep Reinforcement Learning}}-Based {{Resource Provisioning}} and {{Task Scheduling}} for {{Cloud Service Providers}}},
  shorttitle = {{{DRL}}-Cloud},
  booktitle = {Proceedings of the 23rd {{Asia}} and {{South Pacific Design Automation Conference}}},
  author = {Cheng, Mingxi and Li, Ji and Nazarian, Shahin},
  year = {2018},
  pages = {129--134},
  publisher = {{IEEE Press}},
  address = {{Piscataway, NJ, USA}},
  abstract = {Cloud computing has become an attractive computing paradigm in both academia and industry. Through virtualization technology, Cloud Service Providers (CSPs) that own data centers can structure physical servers into Virtual Machines (VMs) to provide services, resources, and infrastructures to users. Profit-driven CSPs charge users for service access and VM rental, and reduce power consumption and electric bills so as to increase profit margin. The key challenge faced by CSPs is data center energy cost minimization. Prior works proposed various algorithms to reduce energy cost through Resource Provisioning (RP) and/or Task Scheduling (TS). However, they have scalability issues or do not consider TS with task dependencies, which is a crucial factor that ensures correct parallel execution of tasks. This paper presents DRL-Cloud, a novel Deep Reinforcement Learning (DRL)-based RP and TS system, to minimize energy cost for large-scale CSPs with very large number of servers that receive enormous numbers of user requests per day. A deep Q-learning-based two-stage RP-TS processor is designed to automatically generate the best long-term decisions by learning from the changing environment such as user request patterns and realistic electric price. With training techniques such as target network, experience replay, and exploration and exploitation, the proposed DRL-Cloud achieves remarkably high energy cost efficiency, low reject rate as well as low runtime with fast convergence. Compared with one of the state-of-the-art energy efficient algorithms, the proposed DRL-Cloud achieves up to 320\% energy cost efficiency improvement while maintaining lower reject rate on average. For an example CSP setup with 5,000 servers and 200,000 tasks, compared to a fast round-robin baseline, the proposed DRL-Cloud achieves up to 144\% runtime reduction.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/Cheng et al_2018_DRL-cloud.pdf},
  keywords = {_tablet,cloud computing,cloud resource management,deep Q-learning,deep reinforcement learning,resource provisioning,task scheduling},
  series = {{{ASPDAC}} '18}
}

@article{chopra1993,
  title = {The Partition Problem},
  author = {Chopra, Sunil and Rao, M. R.},
  year = {1993},
  month = mar,
  volume = {59},
  pages = {87--115},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/BF01581239},
  abstract = {In this paper we describe several forms of thek-partition problem and give integer programming formulations of each case. The dimension of the associated polytopes and some basic facets are identified. We also give several valid and facet defining inequalities for each of the polytopes.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1993/chopra_rao_1993_the_partition_problem.pdf;/Users/rwb/Zotero/storage/DG5LY23T/BF01581239.html},
  journal = {Mathematical Programming},
  keywords = {Unread},
  language = {en},
  number = {1-3}
}

@inproceedings{chowdhury2015,
  title = {Efficient {{Coflow Scheduling Without Prior Knowledge}}},
  booktitle = {Proceedings of the 2015 {{ACM Conference}} on {{Special Interest Group}} on {{Data Communication}}},
  author = {Chowdhury, Mosharaf and Stoica, Ion},
  year = {2015},
  pages = {393--406},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2785956.2787480},
  abstract = {Inter-coflow scheduling improves application-level communication performance in data-parallel clusters. However, existing efficient schedulers require a priori coflow information and ignore cluster dynamics like pipelining, task failures, and speculative executions, which limit their applicability. Schedulers without prior knowledge compromise on performance to avoid head-of-line blocking. In this paper, we present Aalo that strikes a balance and efficiently schedules coflows without prior knowledge. Aalo employs Discretized Coflow-Aware Least-Attained Service (D-CLAS) to separate coflows into a small number of priority queues based on how much they have already sent across the cluster. By performing prioritization across queues and by scheduling coflows in the FIFO order within each queue, Aalo's non-clairvoyant scheduler reduces coflow completion times while guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete 1.93X faster on average and 3.59X faster at the 95th percentile using Aalo in comparison to per-flow mechanisms. Aalo's performance is comparable to that of solutions using prior knowledge, and Aalo outperforms them in presence of cluster dynamics.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/chowdhury_stoica_2015_efficient_coflow_scheduling_without_prior_knowledge.pdf},
  isbn = {978-1-4503-3542-3},
  keywords = {coflow,data-intensive applications,datacenter networks},
  series = {{{SIGCOMM}} '15}
}

@inproceedings{chronaki2015,
  title = {Criticality-{{Aware Dynamic Task Scheduling}} for {{Heterogeneous Architectures}}},
  booktitle = {Proceedings of the 29th {{ACM}} on {{International Conference}} on {{Supercomputing}}},
  author = {Chronaki, Kallia and Rico, Alejandro and Badia, Rosa M. and Ayguad{\'e}, Eduard and Labarta, Jes{\'u}s and Valero, Mateo},
  year = {2015},
  pages = {329--338},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2751205.2751235},
  abstract = {Current and future parallel programming models need to be portable and efficient when moving to heterogeneous multi-core systems. OmpSs is a task-based programming model with dependency tracking and dynamic scheduling. This paper describes the OmpSs approach on scheduling dependent tasks onto the asymmetric cores of a heterogeneous system. The proposed scheduling policy improves performance by prioritizing the newly-created tasks at runtime, detecting the longest path of the dynamic task dependency graph, and assigning critical tasks to fast cores. While previous works use profiling information and are static, this dynamic scheduling approach uses information that is discoverable at runtime which makes it implementable and functional without the need of an oracle or profiling. The evaluation results show that our proposal outperforms a dynamic implementation of Heterogeneous Earliest Finish Time by up to 1.15x, and the default breadth-first OmpSs scheduler by up to 1.3x in an 8-core heterogeneous platform and up to 2.7x in a simulated 128-core chip.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/chronaki_et_al_2015_criticality-aware_dynamic_task_scheduling_for_heterogeneous_architectures.pdf},
  isbn = {978-1-4503-3559-1},
  keywords = {_tablet,heterogeneous systems,high performance computing,scheduling,task-based programming models},
  series = {{{ICS}} '15}
}

@inproceedings{civeit2017,
  title = {{{SOFIA}} Observatory Automated Scheduling after 5 Years of Operations},
  booktitle = {2017 {{IEEE Aerospace Conference}}},
  author = {Civeit, T. and Andersson, B. and Moore, E. and Buizer, J. De},
  year = {2017},
  month = mar,
  pages = {1--14},
  doi = {10.1109/AERO.2017.7943919},
  abstract = {This paper describes a new framework for scheduling that has been developed for the NASA Stratospheric Observatory for Infrared Astronomy (SOFIA). Key to successful and cost-efficient operations of the SOFIA airborne observatory is the optimized scheduling of operational activities. These include instrument, observation and maintenance schedules, as well as Southern Hemisphere deployments. The most distinctive aspect of the SOFIA flight scheduling problem is the interdependency of the targets than can be observed in a same flight, which makes automated scheduling techniques available for ground-based and space-based telescopes unsuitable. SOFIA began early science operations in 2011 and is currently completing its fourth annual cycle of operations, which consists of about 550 hours of observer time carried out during 100 science flights. Although early conceptual studies on the SOFIA scheduling problem were previously conducted, flights still had to be manually created when operations started. Here, we introduce the new automated scheduling system based on a tree search algorithm that is used to generate long-term and short-term operational schedules. We provide a formulation of the SOFIA scheduling problem, as defined after 5 years of operations, including all constraints that a valid schedule must satisfy. We list the flight operational tasks that must be efficiently simulated while building the global search tree. We discuss the foundations of the scheduler and describe the constraint representation, algorithm and heuristics that guide the search. Finally, we report on the integration of the automated system in mission operations and its current and future expected performance.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/civeit_et_al_2017_sofia_observatory_automated_scheduling_after_5_years_of_operations.pdf;/Users/rwb/Zotero/storage/LCHUYMQT/7943919.html},
  keywords = {Aircraft,astronomical observatories,automated scheduling system,global search tree,infrared astronomy,Legged locomotion,long-term operational scheduling,maintenance engineering,maintenance scheduling,NASA stratospheric observatory for infrared astronomy,Observatories,Proposals,Schedules,scheduling,short-term operational scheduling,SOFIA airborne observatory,SOFIA flight scheduling problem,SOFIA observatory automated scheduling,Southern Hemisphere deployments,Telescopes,tree search algorithm,trees (mathematics)}
}

@article{coffman1972,
  title = {Optimal Scheduling for Two-Processor Systems},
  author = {Coffman, E. G. and Graham, R. L.},
  year = {1972},
  month = sep,
  volume = {1},
  pages = {200--213},
  issn = {0001-5903, 1432-0525},
  doi = {10.1007/BF00288685},
  abstract = {SummaryDespite the recognized potential of multiprocessing little is known concerning the general problem of finding efficient algorithms which compute minimallength schedules for given computations and m{$\geqq$}2 processors. In this paper we formulate a general model of computation structures and exhibit an efficient algorithm for finding optimal nonpreemptive schedules for these structures on two-processor systems. We prove that the algorithm gives optimal solutions and discuss its application to preemptive scheduling disciplines.},
  file = {/Users/rwb/Zotero/storage/JSZMUR8Q/BF00288685.html},
  journal = {Acta Informatica},
  keywords = {Unread},
  language = {en},
  number = {3}
}

@article{coll2006,
  title = {Multiprocessor Scheduling under Precedence Constraints: {{Polyhedral}} Results},
  shorttitle = {Multiprocessor Scheduling under Precedence Constraints},
  author = {Coll, Pablo E. and Ribeiro, Celso C. and {de Souza}, Cid C.},
  year = {2006},
  month = apr,
  volume = {154},
  pages = {770--801},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2004.07.009},
  abstract = {We consider the problem of scheduling a set of tasks related by precedence constraints to a set of processors, so as to minimize their makespan. Each task has to be assigned to a unique processor and no preemption is allowed. A new integer programming formulation of the problem is given and strong valid inequalities are derived. A subset of the inequalities in this formulation has a strong combinatorial structure, which we use to define the polytope of partitions into linear orders. The facial structure of this polytope is investigated and facet defining inequalities are presented which may be helpful to tighten the integer programming formulation of other variants of multiprocessor scheduling problems. Numerical results on real-life problems are presented.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/coll_et_al_2006_multiprocessor_scheduling_under_precedence_constraints.pdf;/Users/rwb/Zotero/storage/XUI7E77N/S0166218X05003069.html},
  journal = {Discrete Applied Mathematics},
  keywords = {Multiprocessors,Order polytopes,Polyhedral combinatorics,Precedence constraints,Scheduling,Valid inequalities},
  number = {5},
  series = {{{IV ALIO}}/{{EURO Workshop}} on {{Applied Combinatorial Optimization}}}
}

@article{collberg,
  title = {Repeatability and {{Benefaction}} in {{Computer Systems Research}} \textemdash{} {{A Study}} and a {{Modest Proposal}}},
  author = {Collberg, Christian and Proebsting, Todd and Warren, Alex M},
  pages = {68},
  abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data and the extent to which such code builds. Starting with 601 papers from ACM conferences and journals, we examine 402 papers whose results were backed by code. For 32.3\% of these papers we were able to obtain the code and build it within 30 minutes; for 48.3\% of the papers we managed to build the code, but it may have required extra effort; for 54.0\% of the papers either we managed to build the code or the authors stated the code would build with reasonable effort. We also propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
  file = {/Users/rwb/Zotero/storage/CZ2T6EQE/Collberg et al. - Repeatability and Benefaction in Computer Systems .pdf},
  language = {en}
}

@article{colome,
  ids = {colomea},
  title = {Telescope and Space Mission Scheduling towards a Multi-Observatory Framework},
  author = {Colom{\'e}, J and {Garc{\'i}a-Piquer}, {\'A} and Wilhelmi, E de Ona and Torres, D F and Bridger, A and Lightfoot, J and D{\'i}ez, E and Morales, J C and Vilardell, F},
  pages = {23},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/colom_et_al_telescope_and_space_mission_scheduling_towards_a_multi-observatory_framework.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/colom_et_al_telescope_and_space_mission_scheduling_towards_a_multi-observatory_framework2.pdf},
  language = {en}
}

@article{crick2014,
  title = {"{{Can I Implement Your Algorithm}}?": {{A Model}} for {{Reproducible Research Software}}},
  shorttitle = {"{{Can I Implement Your Algorithm}}?},
  author = {Crick, Tom and Hall, Benjamin A. and Ishtiaq, Samin},
  year = {2014},
  month = sep,
  abstract = {The reproduction and replication of novel results has become a major issue for a number of scientific disciplines. In computer science and related computational disciplines such as systems biology, the issues closely revolve around the ability to implement novel algorithms and approaches. Taking an approach from the literature and applying it to a new codebase frequently requires local knowledge missing from the published manuscripts and project websites. Alongside this issue, benchmarking, and the development of fair \textemdash{} and widely available \textemdash{} benchmark sets present another barrier.},
  archivePrefix = {arXiv},
  eprint = {1407.5981},
  eprinttype = {arxiv},
  file = {/Users/rwb/Zotero/storage/BVGNXFU2/Crick et al. - 2014 - Can I Implement Your Algorithm A Model for Rep.pdf},
  journal = {arXiv:1407.5981 [cs]},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Software Engineering},
  language = {en},
  primaryClass = {cs}
}

@misc{cruz2014,
  title = {Finite {{Queueing Modeling}} and {{Optimization}}: {{A Selected Review}}},
  shorttitle = {Finite {{Queueing Modeling}} and {{Optimization}}},
  author = {Cruz, F. R. B. and {van Woensel}, T.},
  year = {2014},
  doi = {10.1155/2014/374962},
  abstract = {This review provides an overview of the queueing modeling issues and the related performance evaluation and optimization approaches framed in a joined manufacturing and product engineering. Such networks are represented as queueing networks. The performance of the queueing networks is evaluated using an advanced queueing network analyzer: the generalized expansion method. Secondly, different model approaches are described and optimized with regard to the key parameters in the network (e.g., buffer and server sizes, service rates, and so on).},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/cruz_van_woensel_2014_finite_queueing_modeling_and_optimization.pdf;/Users/rwb/Zotero/storage/TC73EHB8/374962.html},
  howpublished = {https://www.hindawi.com/journals/jam/2014/374962/},
  journal = {Journal of Applied Mathematics},
  keywords = {_tablet},
  language = {en},
  type = {Research Article}
}

@article{dao2017,
  title = {Constrained Clustering by Constraint Programming},
  author = {Dao, Thi-Bich-Hanh and Duong, Khanh-Chuong and Vrain, Christel},
  year = {2017},
  month = mar,
  volume = {244},
  pages = {70--94},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2015.05.006},
  abstract = {Constrained Clustering allows to make the clustering task more accurate by integrating user constraints, which can be instance-level or cluster-level constraints. Few works consider the integration of different kinds of constraints, they are usually based on declarative frameworks and they are often exact methods, which either enumerate all the solutions satisfying the user constraints, or find a global optimum when an optimization criterion is specified. In a previous work, we have proposed a model for Constrained Clustering based on a Constraint Programming framework. It is declarative, allowing a user to integrate user constraints and to choose an optimization criterion among several ones. In this article we present a new and substantially improved model for Constrained Clustering, still based on a Constraint Programming framework. It differs from our earlier model in the way partitions are represented by means of variables and constraints. It is also more flexible since the number of clusters does not need to be set beforehand; only a lower and an upper bound on the number of clusters have to be provided. In order to make the model-based approach more efficient, we propose new global optimization constraints with dedicated filtering algorithms. We show that such a framework can easily be embedded in a more general process and we illustrate this on the problem of finding the optimal Pareto front of a bi-criterion constrained clustering task. We compare our approach with existing exact approaches, based either on a branch-and-bound approach or on graph coloring on twelve datasets. Experiments show that the model outperforms exact approaches in most cases.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/dao_et_al_2017_constrained_clustering_by_constraint_programming.pdf;/Users/rwb/Zotero/storage/GI8CXXQN/S0004370215000806.html},
  journal = {Artificial Intelligence},
  keywords = {Bi-criterion clustering,Constrained clustering,Constraint programming,Filtering algorithm,Global optimization constraint,Modeling},
  series = {Combining {{Constraint Solving}} with {{Mining}} and {{Learning}}}
}

@article{das2018,
  title = {Taxonomy and Analysis of Security Protocols for {{Internet}} of {{Things}}},
  author = {Das, Ashok Kumar and Zeadally, Sherali and He, Debiao},
  year = {2018},
  month = dec,
  volume = {89},
  pages = {110--125},
  issn = {0167-739X},
  doi = {10.1016/j.future.2018.06.027},
  abstract = {The Internet of Things (IoT) is a system of physical as well as virtual objects (each with networking capabilities incorporated) that are interconnected to exchange and collect information locally or remotely over the Internet. Since the communication often takes place over the Internet, it is vulnerable to various security threats in an IoT environment. We first discuss essential security requirements that are needed to secure IoT environment. We also discuss the threat model and various attacks related to the IoT environment. We then present a taxonomy of security protocols for the IoT environment which includes important security services such as key management, user and device authentication, access control, privacy preservation, and identity management. We also present a comparative study of recently proposed IoT-related state-of-art security protocols in terms of various security and functionality features they support. Finally, we discuss some future challenges for IoT security protocols that need to be addressed in the future.},
  file = {Dropbox/library/2018/Das et al/das_et_al_2018_taxonomy_and_analysis_of_security_protocols_for_internet_of_things.pdf;/Users/rwb/Zotero/storage/IFPXGZE8/S0167739X18308112.html},
  journal = {Future Generation Computer Systems},
  keywords = {Access control,Authentication,Identity management,IoT,Key management,Privacy,Security,Sensing devices}
}

@book{daskdevelopmentteam2016,
  title = {Dask: {{Library}} for Dynamic Task Scheduling},
  author = {{Dask Development Team}},
  year = {2016}
}

@article{davare2006,
  title = {Classification, {{Customization}}, and {{Characterization}}: {{Using MILP}} for {{Task Allocation}} and {{Scheduling}}},
  author = {Davare, Abhijit and Chong, Jike and Zhu, Qi and Densmore, Douglas Michael and {Sangiovanni-Vincentelli}, Alberto L},
  year = {2006},
  pages = {8},
  abstract = {Task allocation and scheduling for heterogeneous multicore platforms must be automated for such platforms to be successful. Techniques such as Mixed Integer Linear Programming (MILP) provide the ability to easily customize the allocation and scheduling problem to application or platform-specific peculiarities. The representation of the core problem in a MILP form has a large impact on the solution time required. In this paper, we investigate a variety of such representations and propose a taxonomy for them. A promising representation is chosen with extensive computational characterization. The MILP formulation is customized for a multimedia case study involving the deployment of a Motion JPEG encoder application onto a Xilinx Virtex II Pro FPGA platform. We demonstrate that our approach can produce solutions that are competitive with manual designs.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/davare_et_al_2006_classification,_customization,_and_characterization.pdf},
  language = {en}
}

@article{davidsson2005,
  title = {An Analysis of Agent-Based Approaches to Transport Logistics},
  author = {Davidsson, Paul and Henesey, Lawrence and Ramstedt, Linda and T{\"o}rnquist, Johanna and Wernstedt, Fredrik},
  year = {2005},
  month = aug,
  volume = {13},
  pages = {255--271},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2005.07.002},
  abstract = {This paper provides a survey of existing research on agent-based approaches to transportation and traffic management. A framework for describing and assessing this work will be presented and systematically applied. We are mainly adopting a logistical perspective, thus focusing on freight transportation. However, when relevant, work of traffic and transport of people will be considered. A general conclusion from our study is that agent-based approaches seem very suitable for this domain, but that this still needs to be verified by more deployed system.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/davidsson_et_al_2005_an_analysis_of_agent-based_approaches_to_transport_logistics.pdf;/Users/rwb/Zotero/storage/FB85IPL6/S0968090X05000318.html},
  journal = {Transportation Research Part C: Emerging Technologies},
  keywords = {Decentralized systems,Multi-agent systems,Survey,Traffic and transportation management},
  number = {4},
  series = {Agents in {{Traffic}} and {{Transportation}}: {{Exploring Autonomy}} in {{Logistics}}, {{Management}}, {{Simulation}}, and {{Cooperative Driving}}}
}

@article{deb,
  title = {Multi-{{Objective Optimization Using Evolutionary Algorithms}}: {{An Introduction}}},
  author = {Deb, Kalyanmoy},
  pages = {24},
  abstract = {As the name suggests, multi-objective optimization involves optimizing a number of objectives simultaneously. The problem becomes challenging when the objectives are of conflict to each other, that is, the optimal solution of an objective function is different from that of the other. In solving such problems, with or without the presence of constraints, these problems give rise to a set of trade-off optimal solutions, popularly known as Pareto-optimal solutions. Due to the multiplicity in solutions, these problems were proposed to be solved suitably using evolutionary algorithms which use a population approach in its search procedure. Starting with parameterized procedures in early nineties, the so-called evolutionary multi-objective optimization (EMO) algorithms is now an established field of research and application with many dedicated texts and edited books, commercial softwares and numerous freely downloadable codes, a biannual conference series running successfully since 2001, special sessions and workshops held at all major evolutionary computing conferences, and full-time researchers from universities and industries from all around the globe. In this chapter, we provide a brief introduction to its operating principles and outline the current research and application studies of EMO.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/deb_multi-objective_optimization_using_evolutionary_algorithms.pdf},
  language = {en}
}

@article{deb2002,
  title = {A Fast and Elitist Multiobjective Genetic Algorithm: {{NSGA}}-{{II}}},
  shorttitle = {A Fast and Elitist Multiobjective Genetic Algorithm},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  volume = {6},
  pages = {182--197},
  issn = {1089778X},
  doi = {10.1109/4235.996017},
  file = {/Users/rwb/Dropbox/PhD/zotero/2002/deb_et_al_2002_a_fast_and_elitist_multiobjective_genetic_algorithm.pdf},
  journal = {IEEE Transactions on Evolutionary Computation},
  language = {en},
  number = {2}
}

@article{deelman,
  ids = {deelmana},
  title = {Mapping {{Abstract Complex Workflows}} onto {{Grid Environments}}},
  author = {Deelman, Ewa and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Blackburn, Kent and Lazzarini, Albert and Arbree, Adam and Cavanaugh, Richard and Koranda, Scott},
  pages = {15},
  abstract = {In this paper we address the problem of automatically generating job workflows for the Grid. These workflows describe the execution of a complex application built from individual application components. In our work we have developed two workflow generators: the first (the Concrete Workflow Generator CWG) maps an abstract workflow defined in terms of application-level components to the set of available Grid resources. The second generator (Abstract and Concrete Workflow Generator, ACWG) takes a wider perspective and not only performs the abstract to concrete mapping but also enables the construction of the abstract workflow based on the available components. This system operates in the application domain and chooses application components based on the application metadata attributes. We describe our current ACWG based on AI planning technologies and outline how these technologies can play a crucial role in developing complex application workflows in Grid environments. Although our work is preliminary, CWG has already been used to map high energy physics applications onto the Grid. In one particular experiment, a set of production runs lasted 7 days and resulted in the generation of 167,500 events by 678 jobs. Additionally, ACWG was used to map gravitational physics workflows, with hundreds of nodes onto the available resources, resulting in 975 tasks, 1365 data transfers and 975 output files produced.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/deelman_et_al_mapping_abstract_complex_workflows_onto_grid_environments.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/deelman_et_al_mapping_abstract_complex_workflows_onto_grid_environments2.pdf},
  language = {en}
}

@inproceedings{deelman2004,
  title = {Pegasus: {{Mapping Scientific Workflows}} onto the {{Grid}}},
  shorttitle = {Pegasus},
  booktitle = {Grid {{Computing}}},
  author = {Deelman, Ewa and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Patil, Sonal and Su, Mei-Hui and Vahi, Karan and Livny, Miron},
  editor = {Dikaiakos, Marios D.},
  year = {2004},
  pages = {11--20},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In this paper we describe the Pegasus system that can map complex workflows onto the Grid. Pegasus takes an abstract description of a workflow and finds the appropriate data and Grid resources to execute the workflow. Pegasus is being released as part of the GriPhyN Virtual Data Toolkit and has been used in a variety of applications ranging from astronomy, biology, gravitational-wave science, and high-energy physics. A deferred planning mode of Pegasus is also introduced.},
  isbn = {978-3-540-28642-4},
  keywords = {Gravitational Wave,Grid Environment,Grid Resource,High Performance Computing Application,Virtual Data},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@misc{deelman2005,
  title = {Pegasus: {{A Framework}} for {{Mapping Complex Scientific Workflows}} onto {{Distributed Systems}}},
  shorttitle = {Pegasus},
  author = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-Hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G. Bruce and Good, John and Laity, Anastasia and Jacob, Joseph C. and Katz, Daniel S.},
  year = {2005},
  doi = {10.1155/2005/128026},
  abstract = {This paper describes the Pegasus framework that can be used to map complex scientific workflows onto distributed resources. Pegasus enables users to represent the workflows at an abstract level without needing to worry about the particulars of the target execution systems. The paper describes general issues in mapping applications and the functionality of Pegasus. We present the results of improving application performance through workflow restructuring which clusters multiple tasks in a workflow into single entities. A real-life astronomy application is used as the basis for the study.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/deelman_et_al_2005_pegasus.pdf;/Users/rwb/Zotero/storage/24ZHU9B3/abs.html},
  howpublished = {https://www.hindawi.com/journals/sp/2005/128026/abs/},
  journal = {Scientific Programming},
  language = {en},
  type = {Research Article}
}

@article{deelman2015,
  title = {Pegasus, a Workflow Management System for Science Automation},
  author = {Deelman, Ewa and Vahi, Karan and Juve, Gideon and Rynge, Mats and Callaghan, Scott and Maechling, Philip J. and Mayani, Rajiv and Chen, Weiwei and {Ferreira da Silva}, Rafael and Livny, Miron and Wenger, Kent},
  year = {2015},
  month = may,
  volume = {46},
  pages = {17--35},
  issn = {0167-739X},
  doi = {10.1016/j.future.2014.10.008},
  abstract = {Modern science often requires the execution of large-scale, multi-stage simulation and data analysis pipelines to enable the study of complex systems. The amount of computation and data involved in these pipelines requires scalable workflow management systems that are able to reliably and efficiently coordinate and automate data movement and task execution on distributed computational resources: campus clusters, national cyberinfrastructures, and commercial and academic clouds. This paper describes the design, development and evolution of the Pegasus Workflow Management System, which maps abstract workflow descriptions onto distributed computing infrastructures. Pegasus has been used for more than twelve years by scientists in a wide variety of domains, including astronomy, seismology, bioinformatics, physics and others. This paper provides an integrated view of the Pegasus system, showing its capabilities that have been developed over time in response to application needs and to the evolution of the scientific computing platforms. The paper describes how Pegasus achieves reliable, scalable workflow execution across a wide variety of computing infrastructures.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/deelman_et_al_2015_pegasus,_a_workflow_management_system_for_science_automation.pdf;C\:\\Users\\gerald\\Dropbox\\library\\Future Generation Computer Systems\\2015\\deelman_et_al_2015_pegasus,_a_workflow_management_system_for_science_automation.pdf;Dropbox/library/2015/Deelman et al/deelman_et_al_2015_pegasus,_a_workflow_management_system_for_science_automation.pdf;/Users/rwb/Zotero/storage/44WBLJYE/S0167739X14002015.html},
  journal = {Future Generation Computer Systems},
  keywords = {Pegasus,Scientific workflows,Unread,Workflow management system}
}

@article{deelman2019,
  title = {The {{Evolution}} of the {{Pegasus Workflow Management Software}}},
  author = {Deelman, Ewa and Vahi, Karan and Rynge, Mats and Mayani, Rajiv and {da Silva}, Rafael Ferreira and Papadimitriou, George and Livny, Miron},
  year = {2019},
  month = jul,
  volume = {21},
  pages = {22--36},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2019.2919690},
  abstract = {Since 2001, the Pegasus Workflow Management System has evolved into a robust and scalable system that automates the execution of a number of complex applications running on a variety of heterogeneous, distributed high-throughput, and high-performance computing environments. Pegasus was built on the principle of separation between the workflow description and workflow execution, providing the ability to port and adapt the workflow based on the target execution environment. Through its user-driven research and development, it has adapted to the needs of a number of scientific communities, utilizing and developing novel algorithms and software engineering solutions. This paper describes the evolution of Pegasus over time and provides motivations behind the design decisions. It concludes with selected lessons learned.},
  file = {/Users/rwb/Zotero/storage/X72TUJ2G/8725518.html},
  journal = {Computing in Science Engineering},
  keywords = {Biological system modeling,complex applications,Distributed databases,distributed high-throughput,high-performance computing environments,natural sciences computing,parallel processing,Pegasus Workflow Management software,robust system,scalable system,software engineering,software engineering solutions,target execution environment,Task analysis,user-driven research,workflow description,workflow management software,Workflow management software},
  number = {4}
}

@article{deelman2019a,
  title = {The Role of Machine Learning in Scientific Workflows},
  author = {Deelman, Ewa and Mandal, Anirban and Jiang, Ming and Sakellariou, Rizos},
  year = {2019},
  month = nov,
  volume = {33},
  pages = {1128--1139},
  issn = {1094-3420},
  doi = {10.1177/1094342019852127},
  abstract = {Machine learning (ML) is being applied in a number of everyday contexts from image recognition, to natural language processing, to autonomous vehicles, to product recommendation. In the science realm, ML is being used for medical diagnosis, new materials development, smart agriculture, DNA classification, and many others. In this article, we describe the opportunities of using ML in the area of scientific workflow management. Scientific workflows are key to today's computational science, enabling the definition and execution of complex applications in heterogeneous and often distributed environments. We describe the challenges of composing and executing scientific workflows and identify opportunities for applying ML techniques to meet these challenges by enhancing the current workflow management system capabilities. We foresee that as the ML field progresses, the automation provided by workflow management systems will greatly increase and result in significant improvements in scientific productivity.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/deelman_et_al_2019_the_role_of_machine_learning_in_scientific_workflows.pdf},
  journal = {The International Journal of High Performance Computing Applications},
  keywords = {anomaly detection,machine learning,Scientific workflows,workflow composition,workflow systems},
  language = {en},
  number = {6}
}

@article{deelman2019b,
  title = {The Role of Machine Learning in Scientific Workflows:},
  shorttitle = {The Role of Machine Learning in Scientific Workflows},
  author = {Deelman, Ewa and Mandal, Anirban and Jiang, Ming and Sakellariou, Rizos},
  year = {2019},
  month = may,
  doi = {10.1177/1094342019852127},
  abstract = {Machine learning (ML) is being applied in a number of everyday contexts from image recognition, to natural language processing, to autonomous vehicles, to produ...},
  file = {/Users/rwb/Zotero/storage/5Q9K8YYE/1094342019852127.html},
  journal = {The International Journal of High Performance Computing Applications},
  language = {en}
}

@inproceedings{demediuk2017,
  title = {Monte {{Carlo}} Tree Search Based Algorithms for Dynamic Difficulty Adjustment},
  booktitle = {2017 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Demediuk, Simon and Tamassia, Marco and Raffe, William L. and Zambetta, Fabio and Li, Xiaodong and Mueller, Florian},
  year = {2017},
  month = aug,
  pages = {53--59},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}},
  doi = {10.1109/CIG.2017.8080415},
  abstract = {Maintaining player immersion is a crucial step in making an enjoyable video game. One aspect of player immersion is the level of challenge the game presents to the player. To avoid a mismatch between a player's skill and the challenge of a game, which can result from traditional manual difficulty selection mechanisms (e.g. easy, medium, hard), Dynamic Difficulty Adjustment (DDA) has previously been proposed as a means of automatically detecting a player's skill and adjusting the level of challenge the game presents accordingly. This work contributes to the field of DDA by proposing a novel approach to artificially intelligent agents for opponent control. Specifically, we propose four new DDA Artificially Intelligent (AI) agents: Reactive Outcome Sensitive Action Selection (Reactive OSAS), Proactive OSAS, and their ``True'' variants. These agents provide the player with an level of difficulty tailored to their skill in realtime by altering the action selection policy and the heuristic playout evaluation of Monte Carlo Tree Search. The DDA AI agents are tested within the FightingICE engine, which has been used in the past as an environment for AI agent competitions. The results of the experiments against other AI agents and human players show that these novel DDA AI agents can adjust the level of difficulty in real-time, by targeting a zero health difference as the outcome of the fighting game. This work also demonstrates the trade-off existing between targeting the outcome exactly (Reactive OSAS) and introducing proactive behaviour (i.e., the DDA AI agent fights even if the health difference is zero) to increase the agents believability (Proactive OSAS).},
  file = {/Users/rwb/Zotero/storage/MMCNP2PT/demediuk_et_al_2017_monte_carlo_tree_search_based_algorithms_for_dynamic_difficulty_adjustment.pdf},
  isbn = {978-1-5386-3233-8},
  keywords = {_tablet},
  language = {en}
}

@article{demeulemeester1992,
  title = {A {{Branch}}-and-{{Bound Procedure}} for the {{Multiple Resource}}-{{Constrained Project Scheduling Problem}}},
  author = {Demeulemeester, Erik and Herroelen, Willy},
  year = {1992},
  month = dec,
  volume = {38},
  pages = {1803--1818},
  issn = {0025-1909},
  doi = {10.1287/mnsc.38.12.1803},
  abstract = {In this paper a branch-and-bound procedure is described for scheduling the activities of a project of the PERT/CPM variety subject to precedence and resource constraints where the objective is to minimize project duration. The procedure is based on a depth-first solution strategy in which nodes in the solution tree represent resource and precedence feasible partial schedules. Branches emanating from a parent node correspond to exhaustive and minimal combinations of activities, the delay of which resolves resource conflicts at each parent node. Precedence and resource-based bounds described in the paper are combined with new dominance pruning rules to rapidly fathom major portions of the solution tree. The procedure is programmed in the C language for use on both a mainframe and a personal computer. The procedure has been validated using a standard set of test problems with between 7 and 50 activities requiring up to three resource types each. Computational experience on a personal computer indicates that the procedure is 11.6 times faster than the most rapid solution procedure reported in the literature while requiring less computer storage. Moreover, problems requiring large amounts of computer time using existing approaches for solving this problem type are rapidly solved with our procedure using the dominance rules described, resulting in a significant reduction in the variability in solution times as well.},
  file = {/Users/rwb/Zotero/storage/6SP2DCRH/mnsc.38.12.html},
  journal = {Management Science},
  number = {12}
}

@inproceedings{demirci2018,
  title = {A {{Divide}} and {{Conquer Algorithm}} for {{DAG Scheduling Under Power Constraints}}},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}}, and {{Analysis}}},
  author = {Demirci, G{\"o}kalp and Marincic, Ivana and Hoffmann, Henry},
  year = {2018},
  pages = {36:1--36:12},
  publisher = {{IEEE Press}},
  address = {{Piscataway, NJ, USA}},
  abstract = {We consider the problem of scheduling a parallel computation---represented as a directed acyclic graph (DAG)---on a distributed parallel system with a global resource constraint---specifically a global power budget---and configurable resources, allowing a range of different power/performance tradeoffs. There is a rich body of literature on the independent problems of (1) scheduling DAGs and (2) scheduling independent applications under resource constraints. Very little, however, is known about the combined problem of scheduling DAGs under resource constraints. We present a novel approximation algorithm using a divide-and-conquer method for minimizing application execution time. We prove that the length of the schedule returned by our algorithm is always within O(log n)-factor of the optimum that can be achieved with any selection of configurations for the tasks. We implement and test our algorithm on simulations of real application DAGs. We find that our divide-and-conquer method improves performance by up to 75\% compared to greedy scheduling algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/demirci_et_al_2018_a_divide_and_conquer_algorithm_for_dag_scheduling_under_power_constraints.pdf},
  keywords = {configuration,DAG,power,precedence,resource,scheduling},
  series = {{{SC}} '18}
}

@inproceedings{derainville2012,
  title = {{{DEAP}}: A Python Framework for Evolutionary Algorithms},
  shorttitle = {{{DEAP}}},
  booktitle = {Proceedings of the Fourteenth International Conference on {{Genetic}} and Evolutionary Computation Conference Companion - {{GECCO Companion}} '12},
  author = {De Rainville, Fran{\c c}ois-Michel and Fortin, F{\'e}lix-Antoine and Gardner, Marc-Andr{\'e} and Parizeau, Marc and Gagn{\'e}, Christian},
  year = {2012},
  pages = {85},
  publisher = {{ACM Press}},
  address = {{Philadelphia, Pennsylvania, USA}},
  doi = {10.1145/2330784.2330799},
  abstract = {DEAP (Distributed Evolutionary Algorithms in Python) is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black box type of frameworks. It also incorporates easy parallelism where users need not concern themselves with gory implementation details like synchronization and load balancing, only functional decomposition. Several examples illustrate the multiple properties of DEAP.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/de_rainville_et_al_2012_deap.pdf},
  isbn = {978-1-4503-1178-6},
  language = {en}
}

@article{dewdney2015,
  ids = {dewdney2015a,dewdney2015b},
  title = {{{SKA1 System BaselineV2 Description}}},
  author = {Dewdney, P and Turner, W and Braun, R and {Santander-Vela}, J and Waterson, M and Tan, G-H},
  year = {2015},
  volume = {1},
  journal = {Document number SKA-TEL-SKO-0000308},
  number = {1}
}

@article{dong2006,
  title = {Scheduling {{Algorithms}} for {{Grid Computing}}: {{State}} of the {{Art}} and {{Open Problems}}},
  author = {Dong, Fangpeng and Akl, Selim G},
  year = {2006},
  pages = {55},
  abstract = {Thanks to advances in wide-area network technologies and the low cost of computing resources, Grid computing came into being and is currently an active research area. One motivation of Grid computing is to aggregate the power of widely distributed resources, and provide non-trivial services to users. To achieve this goal, an efficient Grid scheduling system is an essential part of the Grid. Rather than covering the whole Grid scheduling area, this survey provides a review of the subject mainly from the perspective of scheduling algorithms. In this review, the challenges for Grid scheduling are identified. First, the architecture of components involved in scheduling is briefly introduced to provide an intuitive image of the Grid scheduling process. Then various Grid scheduling algorithms are discussed from different points of view, such as static vs. dynamic policies, objective functions, applications models, adaptation, QoS constraints, strategies dealing with dynamic behavior of resources, and so on. Based on a comprehensive understanding of the challenges and the state of the art of current research, some general issues worthy of further exploration are proposed.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/dong_akl_2006_scheduling_algorithms_for_grid_computing.pdf},
  keywords = {Unread},
  language = {en}
}

@article{dong2019,
  title = {{{ECOS}}: {{An}} Efficient Task-Clustering Based Cost-Effective Aware Scheduling Algorithm for Scientific Workflows Execution on Heterogeneous Cloud Systems},
  shorttitle = {{{ECOS}}},
  author = {Dong, Minggang and Fan, Lili and Jing, Chao},
  year = {2019},
  month = dec,
  volume = {158},
  pages = {110405},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2019.110405},
  abstract = {Cloud Computing provides an attractive execution environment for scientific workflow execution. However, due to the increasingly high charge cost of using cloud service, cost minimization for workflows execution on cloud systems has become a crucial issue. Traditional work are adopting the sophisticated scheduling techniques to address such issue. Differently, this paper has proposed an efficient task-clustering based cost-effective aware scheduling algorithm (ECOS) to minimize the cost without comprising the deadline constraint. First, with respect to the characteristics of multi-type workflows, cloud heterogeneity and cost model, we have formulated the problem of task-clustering to simplify the structure of workflows and workflow scheduling to minimize cost within the deadline constraint. Then, we have devised ECOS with two key steps: (1) vertical clustering is with the time consideration that selectively merges the sequential tasks to reduce the transferring time within the workflow; (2) horizontal clustering and greedy allocation is to aggregate the parallel tasks and greedily allocate resources to that tasks with the aim of minimizing cost within deadline. Last, we have conducted the experiment that compare with well-known task-clustering based algorithms via WorkflowSim platform. The results have demonstrated that ECOS can efficiently merge tasks and minimize the total cost without comprising the deadline constraint both in small and large datasets. Moreover, we have discussed the ECOS in terms of various schedulers and number of tasks to validate the performance of ECOS.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/dong_et_al_2019_ecos.pdf;/Users/rwb/Zotero/storage/CV86I4HS/S0164121219301797.html},
  journal = {Journal of Systems and Software},
  keywords = {Cloud computing,Cost minimization,Greedy allocation,Heterogeneous cloud systems,Scientific workflows scheduling,Task clustering}
}

@article{dordaie2018,
  title = {A Hybrid Particle Swarm Optimization and Hill Climbing Algorithm for Task Scheduling in the Cloud Environments},
  author = {Dordaie, Negar and Navimipour, Nima Jafari},
  year = {2018},
  month = dec,
  volume = {4},
  pages = {199--202},
  issn = {2405-9595},
  doi = {10.1016/j.icte.2017.08.001},
  abstract = {Task scheduling is one of the most important issues in heterogeneous environments when high efficiency is required. Because task scheduling is a Nondeterministic Polynomial (NP)-hard problem, many evolutionary algorithms have been adopted to solve this problem. Since the convergence speed of solutions in population-based algorithms is low, they are integrated with local search algorithms. Thus, in this paper, to optimize the task scheduling makespan, a hybrid particle swarm optimization and hill climbing algorithm is proposed. The experimental results on random and scientific Directed Acyclic Graph (DAG) showed that the proposed algorithm performs effectively in terms of the makespan compared to the current well-known heuristic and particle swarm optimization algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/dordaie_navimipour_2018_a_hybrid_particle_swarm_optimization_and_hill_climbing_algorithm_for_task.pdf;/Users/rwb/Zotero/storage/QYBPP7N8/S2405959517300784.html},
  journal = {ICT Express},
  keywords = {Cloud computing,Directed acyclic graph,Hill climbing,PSO,Task scheduling},
  language = {en},
  number = {4}
}

@article{dr.b.rambedkarnationalinstituteoftechnologydepartmentofcsejalandhar144011india2018,
  title = {Time {{Effective Workflow Scheduling}} Using {{Genetic Algorithm}} in {{Cloud Computing}}},
  author = {{Dr. B.R Ambedkar National Institute of Technology, Department of CSE, Jalandhar, 144011, India} and Nagar, Rohit and Gupta, Deepak K. and Singh, Raj M.},
  year = {2018},
  month = jan,
  volume = {10},
  pages = {68--75},
  issn = {20749007, 20749015},
  doi = {10.5815/ijitcs.2018.01.08},
  abstract = {Cloud computing is service based technology on internet which facilitates users to access plenty of resources on demand from anywhere and anytime in a metered manner i.e. pay per usage without paying much heed to the maintenance and implementation details of application. As cloud technology is evolving day by day it is being confronted by numerous challenges, such as time and cost under deadline constraints. Research work done so far mainly focused on reducing cost as well as execution time. In order to minimize cost and execution time previously existing workflow scheduling model known as predict earliest finish time is used. In this research work we have proposed a new PEFT genetic algorithm approach to further reduce the execution time on this model. A strategy is developed to let GA focus on to optimize chromosomes objective to get best suitable mutated children. After obtaining a feasible solution, the genetic algorithm focuses on optimizing the execution time. Experimental results show that our algorithm can find better solution within lesser time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/dr._b.r_ambedkar_national_institute_of_technology,_department_of_cse,_jalandhar,_144011,_india_et_al_2018_time_effective_workflow_scheduling_using_genetic_algorithm_in_cloud_computing.pdf},
  journal = {International Journal of Information Technology and Computer Science},
  language = {en},
  number = {1}
}

@article{drummond,
  title = {Planning, {{Scheduling}}, and {{Control}} for {{Automatic Telescopes}}},
  author = {Drummond, Mark},
  abstract = {Planning, Scheduling, and Control for Automatic Telescopes},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/drummond_planning,_scheduling,_and_control_for_automatic_telescopes.pdf;/Users/rwb/Zotero/storage/3XLL7ESG/Planning_Scheduling_and_Control_for_Automatic_Telescopes.html},
  language = {en}
}

@article{duan2014,
  title = {Multi-{{Objective Game Theoretic Schedulingof Bag}}-of-{{Tasks Workflows}} on {{Hybrid Clouds}}},
  author = {Duan, R. and Prodan, R. and Li, X.},
  year = {2014},
  month = jan,
  volume = {2},
  pages = {29--42},
  issn = {2168-7161},
  doi = {10.1109/TCC.2014.2303077},
  abstract = {Scheduling multiple large-scale parallel workflow applications on heterogeneous computing systems like hybrid clouds is a fundamental NP-complete problem that is critical to meeting various types of QoS (Quality of Service) requirements. This paper addresses the scheduling problem of large-scale applications inspired from real-world, characterized by a huge number of homogeneous and concurrent bags-of-tasks that are the main sources of bottlenecks but open great potential for optimization. The scheduling problem is formulated as a new sequential cooperative game and propose a communication and storage-aware multi-objective algorithm that optimizes two user objectives (execution time and economic cost) while fulfilling two constraints (network bandwidth and storage requirements). We present comprehensive experiments using both simulation and real-world applications that demonstrate the efficiency and effectiveness of our approach in terms of algorithm complexity, makespan, cost, system-level efficiency, fairness, and other aspects compared with other related algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/duan_et_al_2014_multi-objective_game_theoretic_schedulingof_bag-of-tasks_workflows_on_hybrid.pdf;/Users/rwb/Zotero/storage/YYRX6WV7/6727390.html},
  journal = {IEEE Transactions on Cloud Computing},
  keywords = {bag-of-tasks workflows,bags-of-tasks,Bandwidth allocation,cloud computing,Cloud computing,fundamental NP-complete problem,game theory,Game theory,heterogeneous computing systems,hybrid clouds,large scale parallel workflow applications,Multi-objective scheduling,multiobjective game theoretic scheduling problem,parallel processing,Processor scheduling,QoS requirements,scheduling,Scheduling,sequential cooperative game,storage-aware multi objective algorithm},
  number = {1}
}

@inproceedings{durillo2012,
  ids = {durillo2012a},
  title = {{{MOHEFT}}: {{A}} Multi-Objective List-Based Method for Workflow Scheduling},
  shorttitle = {{{MOHEFT}}},
  booktitle = {4th {{IEEE International Conference}} on {{Cloud Computing Technology}} and {{Science Proceedings}}},
  author = {Durillo, J. J. and Fard, H. M. and Prodan, R.},
  year = {2012},
  month = dec,
  pages = {185--192},
  doi = {10.1109/CloudCom.2012.6427573},
  abstract = {Nowadays, scientists and companies are confronted with multiple competing goals such as makespan in high-performance computing and economic cost in Clouds that have to be simultaneously optimized. Multi-objective scheduling of scientific workflows in distributed systems is therefore receiving increasing research attention. Most existing approaches typically aggregate all objectives in a single function, defined a-priori without any knowledge about the problem being solved, which negatively impacts the quality of the solutions. In contrast, Pareto-based approaches having as outcome a set of several (nearly-) optimal solutions that represent a tradeoff among the different objectives, have been scarcely studied. In this paper, we propose a new Pareto-based list scheduling heuristic that provides the user with a set of tradeoff optimal solutions from where the one that better suits the user requirements can be manually selected. We demonstrate the potential of MOHEFT for a bi-objective scheduling problem that optimizes makespan and economic cost in a Cloud-based computing scenario. We compare MOHEFT with two state-of-the-art approaches using different synthetic and real-world workflows: the classical HEFT algorithm used in single-objective scheduling and the SPEA2* genetic algorithm used for multi-objective optimisation problems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/durillo_et_al_2012_moheft.pdf;/Users/rwb/Dropbox/PhD/zotero/2012/durillo_et_al_2012_moheft.pdf;/Users/rwb/Zotero/storage/G5LDCNWW/6427573.html;/Users/rwb/Zotero/storage/R6B7228U/6427573.html},
  keywords = {biobjective scheduling problem,cloud computing,Cloud computing,cloud-based computing scenario,Computational modeling,Conferences,Data models,distributed systems,economic cost,high-performance computing,MOHEFT,multiobjective heterogeneous earliest finish time,multiobjective list-based method,multiobjective scientific workflow scheduling,multiple competing goals,optimal solutions,Optimization,parallel processing,Pareto analysis,Pareto-based list scheduling heuristic,Processor scheduling,Schedules,workflow management software}
}

@incollection{dutot2017,
  title = {Batsim: {{A Realistic Language}}-{{Independent Resources}} and {{Jobs Management Systems Simulator}}},
  shorttitle = {Batsim},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Dutot, Pierre-Fran{\c c}ois and Mercier, Michael and Poquet, Millian and Richard, Olivier},
  editor = {Desai, Narayan and Cirne, Walfredo},
  year = {2017},
  volume = {10353},
  pages = {178--197},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-61756-5_10},
  abstract = {As large scale computation systems are growing to exascale, Resources and Jobs Management Systems (RJMS) need to evolve to manage this scale modification. However, their study is problematic since they are critical production systems, where experimenting is extremely costly due to downtime and energy costs. Meanwhile, many scheduling algorithms emerging from theoretical studies have not been transferred to production tools for lack of realistic experimental validation. To tackle these problems we propose Batsim, an extendable, language-independent and scalable RJMS simulator. It allows researchers and engineers to test and compare any scheduling algorithm, using a simple event-based communication interface, which allows different levels of realism. In this paper we show that Batsim's behaviour matches the one of the real RJMS OAR. Our evaluation process was made with reproducibility in mind and all the experiment material is freely available.},
  file = {/Users/rwb/Zotero/storage/3UCMVTDK/Dutot et al. - 2017 - Batsim A Realistic Language-Independent Resources.pdf},
  isbn = {978-3-319-61755-8 978-3-319-61756-5},
  language = {en}
}

@inproceedings{dutreilh2010,
  title = {From {{Data Center Resource Allocation}} to {{Control Theory}} and {{Back}}},
  booktitle = {2010 {{IEEE}} 3rd {{International Conference}} on {{Cloud Computing}}},
  author = {Dutreilh, X. and Moreau, A. and Malenfant, J. and Rivierre, N. and Truck, I.},
  year = {2010},
  month = jul,
  pages = {410--417},
  doi = {10.1109/CLOUD.2010.55},
  abstract = {Continuously adjusting the horizontal scaling of applications hosted by data centers appears as a good candidate to automatic control approaches allocating resources in closed-loop given their current workload. Despite several attempts, real applications of these techniques in cloud computing infrastructures face some difficulties. Some of them essentially turn back to the core concepts of automatic control: controllability, inertia of the controlled system, gain and stability. In this paper, considering our recent work to build a management framework dedicated to automatic resource allocation in virtualized applications, we attempt to identify from experiments the sources of instabilities in the controlled systems. As examples, we analyze two types of policies: threshold-based and reinforcement learning techniques to dynamically scale resources. The experiments show that both approaches are tricky and that trying to implement a controller without looking at the way the controlled system reacts to actions, both in time and in amplitude, is doomed to fail. We discuss both lessons learned from the experiments in terms of simple yet key points to build good resource management policies, and longer term issues on which we are currently working to manage contracts and reinforcement learning efficiently in cloud controllers.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/dutreilh_et_al_2010_from_data_center_resource_allocation_to_control_theory_and_back.pdf;/Users/rwb/Zotero/storage/IYMUSFTS/5557965.html},
  keywords = {Application hosting,Closed loop systems,Cloud computing,cloud computing infrastructures,Clouds,computer centres,Control systems,control theory,Controllability,data center resource allocation,Hysteresis,Internet,learning (artificial intelligence),Oscillators,reinforcement learning techniques,resource allocation,Resource allocation,Resource management,Servers,threshold-based,Time factors}
}

@article{dyer2018,
  title = {A Telescope Control and Scheduling System for the {{Gravitational}}-Wave {{Optical Transient Observer}} ({{GOTO}})},
  author = {Dyer, Martin and Dhillon, Vik and Littlefair, Stuart and Steeghs, Danny and Ulaczyk, Krzysztof and Chote, Paul and Galloway, Duncan and Rol, Evert},
  year = {2018},
  month = jul,
  pages = {14},
  doi = {10.1117/12.2311865},
  abstract = {The Gravitational-wave Optical Transient Observer (GOTO) is a wide-field telescope project aimed at detecting optical counterparts to gravitational wave sources. The prototype instrument was inaugurated in July 2017 on La Palma in the Canary Islands. We describe the GOTO Telescope Control System (G-TeCS), a custom robotic control system written in Python which autonomously manages the telescope hardware and nightly operations. The system comprises of multiple independent control daemons, which are supervised by a master control program known as the ``pilot''. Observations are decided by a ``just-in-time'' scheduler, which instructs the pilot what to observe in real time and provides quick follow-up of transient events.},
  archivePrefix = {arXiv},
  eprint = {1807.01614},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/018/dyer_et_al_2018_a_telescope_control_and_scheduling_system_for_the_gravitational-wave_optical.pdf},
  journal = {Observatory Operations: Strategies, Processes, and Systems VII},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  language = {en}
}

@article{dyer2018a,
  title = {A Telescope Control and Scheduling System for the {{Gravitational}}-Wave {{Optical Transient Observer}} ({{GOTO}})},
  author = {Dyer, Martin and Dhillon, Vik and Littlefair, Stuart and Steeghs, Danny and Ulaczyk, Krzysztof and Chote, Paul and Galloway, Duncan and Rol, Evert},
  year = {2018},
  month = jul,
  pages = {14},
  doi = {10.1117/12.2311865},
  abstract = {The Gravitational-wave Optical Transient Observer (GOTO) is a wide-field telescope project aimed at detecting optical counterparts to gravitational wave sources. The prototype instrument was inaugurated in July 2017 on La Palma in the Canary Islands. We describe the GOTO Telescope Control System (G-TeCS), a custom robotic control system written in Python which autonomously manages the telescope hardware and nightly operations. The system comprises of multiple independent control daemons, which are supervised by a master control program known as the "pilot". Observations are decided by a "just-in-time" scheduler, which instructs the pilot what to observe in real time and provides quick follow-up of transient events.},
  archivePrefix = {arXiv},
  eprint = {1807.01614},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/dyer_et_al_2018_a_telescope_control_and_scheduling_system_for_the_gravitational-wave_optical2.pdf;/Users/rwb/Zotero/storage/IAN95K4J/1807.html},
  journal = {Observatory Operations: Strategies, Processes, and Systems VII},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics}
}

@article{eisa2014,
  title = {Enhancing {{Cloud Computing Scheduling}} Based on {{Queuing Models}}},
  author = {Eisa, Mohamed and I. Esedimy, E. and Z. Rashad, M.},
  year = {2014},
  month = jan,
  volume = {85},
  pages = {17--23},
  issn = {09758887},
  doi = {10.5120/14813-3032},
  abstract = {This paper presented a proposed model for cloud computing scheduling based on multiple queuing models. This allowed us to improve the quality of service by minimize execution time per jobs, waiting time and the cost of resources to satisfy user's requirements. By taking advantage of some useful proprieties of queuing theory scheduling algorithm is proposed to improve scheduling process. Experimental results indicate that our model increases utilization of global scheduler and reduce waiting time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/eisa_et_al_2014_enhancing_cloud_computing_scheduling_based_on_queuing_models.pdf},
  journal = {International Journal of Computer Applications},
  language = {en},
  number = {2}
}

@inproceedings{emeretlis2015,
  title = {Mapping {{DAGs}} on {{Heterogeneous Platforms Using Logic}}-{{Based Benders Decompostion}}},
  booktitle = {2015 {{IEEE Computer Society Annual Symposium}} on {{VLSI}}},
  author = {Emeretlis, A. and Theodoridis, G. and Alefragis, P. and Voros, N.},
  year = {2015},
  month = jul,
  pages = {119--124},
  doi = {10.1109/ISVLSI.2015.98},
  abstract = {Efficient mapping of DAGs on heterogeneous multicore platforms is a key component for modern embedded applications. An approach based on the Benders decomposition principle that uses a heuristic pre-solver and Integer Linear and Constraint Programming methods to find proven-optimal solutions is introduced. We present multiple cuts generation schemes, that improve the performance of the solution process, and extensive experimental results, that show significant speedups compared to the pure ILP-based method.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/emeretlis_et_al_2015_mapping_dags_on_heterogeneous_platforms_using_logic-based_benders_decompostion.pdf;/Users/rwb/Zotero/storage/PL3UJ3RQ/7309549.html},
  keywords = {_tablet,Benders decomposition,benders decomposition principle,Complexity theory,constraint handling,constraint programming method,DAG,DAGs mapping,directed graphs,heterogeneous multicore platform,heterogeneous platform,heuristic pre-solver,ILP CP optimization,ILP-based method,integer linear programming,integer programming,linear programming,logic-based benders decompostion,Mathematical model,Multicore architectures,Multicore processing,multiprocessing systems,Optimization,Programming,Scheduling,Sequential analysis}
}

@inproceedings{emeretlis2016,
  title = {A Hybrid Approach for Mapping and Scheduling on Heterogeneous Multicore Systems},
  booktitle = {2016 {{International Conference}} on {{Embedded Computer Systems}}: {{Architectures}}, {{Modeling}} and {{Simulation}} ({{SAMOS}})},
  author = {Emeretlis, A. and Theodoridis, G. and Alefragis, P. and Voros, N.},
  year = {2016},
  month = jul,
  pages = {360--365},
  doi = {10.1109/SAMOS.2016.7818373},
  abstract = {A hybrid approach for mapping applications represented as Directed Acyclic Graphs (DAGs) is introduced in this work. It combines the Benders decomposition principle, which integrates Integer Linear and Constraint Programming (ILP and CP) methods, with a pure ILP model to find optimal solutions. The cuts that are generated during the iterative Benders solution process are later exploited by the ILP solver to prune the remaining search space. The proposed model succeeds to provide the optimal solution in cases where either method alone fails to do so, while it also reduces the total solution time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/emeretlis_et_al_2016_a_hybrid_approach_for_mapping_and_scheduling_on_heterogeneous_multicore_systems.pdf;/Users/rwb/Zotero/storage/25Q55KFX/7818373.html},
  keywords = {application mapping,Benders decomposition,Benders decomposition principle,Computational modeling,Computers,constraint programming,CP,DAG,DAGs mapping,directed acyclic graph,directed graphs,Heterogeneous,heterogeneous multicore system,hybrid approach,ILP,ILP  CP optimization,integer linear programming,integer programming,iterative Benders solution process,iterative methods,linear programming,Linear programming,Multicore architectures,Multicore processing,multiprocessing systems,Optimization,processor scheduling,Programming,scheduling,Scheduling,search problems,search space}
}

@article{emeretlis2016a,
  title = {A {{Logic}}-{{Based Benders Decomposition Approach}} for {{Mapping Applications}} on {{Heterogeneous Multicore Platforms}}},
  author = {Emeretlis, Andreas and Theodoridis, George and Alefragis, Panayiotis and Voros, Nikolaos},
  year = {2016},
  month = feb,
  volume = {15},
  pages = {19:1--19:28},
  issn = {1539-9087},
  doi = {10.1145/2838733},
  abstract = {The development of efficient methods for mapping applications on heterogeneous multicore platforms is a key issue in the field of embedded systems. In this article, a novel approach based on the Logic-Based Benders decomposition principle is introduced for mapping complex applications on these platforms, aiming at optimizing their execution time. To provide optimal solutions for this problem in a short time, a new hybrid model that combines Integer Linear Programming (ILP) and Constraint Programming (CP) models is introduced. Also, to reduce the complexity of the model and its solution time, a set of novel techniques for generating additional constraints called Benders cuts is proposed. An extensive set of experiments has been performed in which synthetic applications described by Directed Acyclic Graphs (DAGs) were mapped to a number of heterogeneous multicore platforms. Moreover, experiments with DAGs that correspond to two real-life applications have also been performed. Based on the experimental results, it is proven that the proposed approach outperforms the pure ILP model in terms of the solution time and quality of the solution. Specifically, the proposed approach is able to find an optimal solution within a time limit of 2 hours in the vast majority of performed experiments, while the pure ILP model fails. Also, for the cases where both methods fail to find an optimal solution within the time limit, the solution of the proposed approach is systematically better than the solution of the ILP model.},
  file = {/home/rwb/Dropbox/University/UWA/PhD/zotero/ACM Trans. Embed. Comput. Syst./emeretlis_et_al_2016_a_logic-based_benders_decomposition_approach_for_mapping_applications_on.pdf},
  journal = {ACM Trans. Embed. Comput. Syst.},
  keywords = {_tablet,Benders decomposition,constraint programming,integer linear programming,multicore embedded platforms,Task scheduling},
  number = {1}
}

@inproceedings{emeretlis2017,
  title = {Task Graph Mapping and Scheduling on Heterogeneous Architectures under Communication Constraints},
  booktitle = {2017 {{International Conference}} on {{Embedded Computer Systems}}: {{Architectures}}, {{Modeling}}, and {{Simulation}} ({{SAMOS}})},
  author = {Emeretlis, A. and Tsakoulis, T. and Theodoridis, G. and Alefragis, P. and Voros, N.},
  year = {2017},
  month = jul,
  pages = {239--244},
  doi = {10.1109/SAMOS.2017.8344634},
  abstract = {An approach for mapping applications represented as Directed Acyclic Graphs (DAGs) on platforms consisting of heterogeneous cores considering the communication overhead between the cores is introduced. The approach is based on the Benders decomposition principle and integrates Integer Linear and Constraint Programming formulations. Both formulations take into account the communication delay between dependent tasks that are assigned to different cores trying to optimize the application's execution time. The proposed approach succeeds to provide the optimal solution in all cases of synthetic and real-application DAGs, while the pure ILP model fails more than half of them. Also, the average solution time of the proposed method is about 1 minute, whereas for instances solved by both models, the speedup equals to 11\texttimes{} over the ILP model.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/emeretlis_et_al_2017_task_graph_mapping_and_scheduling_on_heterogeneous_architectures_under.pdf;/Users/rwb/Zotero/storage/7M2JCN72/8344634.html},
  keywords = {average solution time,Benders decomposition principle,communication constraints,communication overhead,Constraint programming,constraint programming formulations,DAG,Delays,directed acyclic graphs,directed graphs,heterogeneous architectures,heterogeneous cores,integer linear,Integer linear programming,integer programming,linear programming,mapping applications,Multicore processing,optimal solution,Optimization,Processor scheduling,Programming,pure ILP model,scheduling,Scheduling,Task analysis}
}

@article{esteves2016,
  title = {{{WaaS}}: {{Workflow}}-as-a-{{Service}} for the {{Cloud}} with {{Scheduling}} of {{Continuous}} and {{Data}}-{{Intensive Workflows}}},
  shorttitle = {{{WaaS}}},
  author = {Esteves, S{\'e}rgio and Veiga, Lu{\'i}s},
  year = {2016},
  month = mar,
  volume = {59},
  pages = {371--383},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/bxu158},
  file = {/Users/rwb/Zotero/storage/HA59Y2T5/esteves_veiga_2016_waas.pdf},
  journal = {The Computer Journal},
  keywords = {_tablet},
  language = {en},
  number = {3}
}

@inproceedings{fahringer2005,
  title = {{{ASKALON}}: A {{Grid}} Application Development and Computing Environment},
  shorttitle = {{{ASKALON}}},
  booktitle = {The 6th {{IEEE}}/{{ACM International Workshop}} on {{Grid Computing}}, 2005.},
  author = {Fahringer, T. and Prodan, R. and Rubing Duan and Nerieri, F. and Podlipnig, S. and Jun Qin and Siddiqui, M. and {Hong-Linh Truong} and Villazon, A. and Wieczorek, M.},
  year = {2005},
  month = nov,
  pages = {10 pp.-},
  issn = {2152-1093},
  doi = {10.1109/GRID.2005.1542733},
  abstract = {We present the ASKALON environment whose goal is to simplify the development and execution of workflow applications on the Grid. ASKALON is centered around a set of high-level services for transparent and effective Grid access, including a Scheduler for optimized mapping of workflows onto the Grid, an Enactment Engine for reliable application execution, a Resource Manager covering both computers and application components, and a Performance Prediction service based on training phase and statistical methods. A sophisticated XML-based programming interface that shields the user from the Grid middleware details allows the high-level composition of workflow applications. ASKALON is used to develop and port scientific applications as workflows in the Austrian Grid project. We present experimental results using two real-world scientific applications to demonstrate the effectiveness of our approach.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/fahringer_et_al_2005_askalon.pdf;/Users/rwb/Zotero/storage/T8SB5EYC/1542733.html},
  keywords = {application execution,Application software,ASKALON environment,Austrian Grid project,Computer applications,enactment engine,Engines,Grid access,Grid application development,grid computing,Grid computing,grid middleware,High performance computing,Management training,middleware,natural sciences computing,Optimization methods,optimized workflow mapping,performance prediction,Processor scheduling,programming environments,resource allocation,Resource management,resource manager,scheduler,scheduling,scientific applications,Statistical analysis,XML,XML-based programming interface}
}

@article{faragardi2020,
  title = {{{GRP}}-{{HEFT}}: {{A Budget}}-{{Constrained Resource Provisioning Scheme}} for {{Workflow Scheduling}} in {{IaaS Clouds}}},
  shorttitle = {{{GRP}}-{{HEFT}}},
  author = {Faragardi, Hamid Reza and Saleh Sedghpour, Mohammad Reza and Fazliahmadi, Saber and Fahringer, Thomas and Rasouli, Nayereh},
  year = {2020},
  month = jun,
  volume = {31},
  pages = {1239--1254},
  issn = {2161-9883},
  doi = {10.1109/TPDS.2019.2961098},
  abstract = {In Infrastructure as a Service (IaaS) Clouds, users are charged to utilize cloud services according to a pay-per-use model. If users intend to run their workflow applications on cloud resources within a specific budget, they have to adjust their demands for cloud resources with respect to this budget. Although several scheduling approaches have introduced solutions to optimize the makespan of workflows on a set of heterogeneous IaaS cloud resources within a certain budget, the hourly-based cost model of some well-known cloud providers (e.g., Amazon EC2 Cloud) can easily lead to a higher makespan and some schedulers may not find any feasible solution. In this article, we propose a novel resource provisioning mechanism and a workflow scheduling algorithm, named Greedy Resource Provisioning and modified HEFT (GRP-HEFT), for minimizing the makespan of a given workflow subject to a budget constraint for the hourly-based cost model of modern IaaS clouds. As a resource provisioning mechanism, we propose a greedy algorithm which lists the instance types according to their efficiency rate. For our scheduler, we modified the HEFT algorithm to consider a budget limit. GRP-HEFT is compared against state-of-the-art workflow scheduling techniques, including MOACS (MultiObjective Ant Colony System), PSO (Particle Swarm Optimization), and GA (Genetic Algorithm). The experimental results demonstrate that GRP-HEFT outperforms GA, PSO, and MOACS for several well-known scientific workflow applications for different problem sizes on average by 13.64, 19.77, and 11.69 percent, respectively. Also in terms of time complexity, GRP-HEFT outperforms GA, PSO and MOACS.},
  file = {/Users/rwb/Zotero/storage/59Q79TVC/8937813.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Amazon EC2 Cloud,budget constraint,budget limit,budget-constrained resource provisioning scheme,budget-constrained scheduling,budgeting,cloud computing,Cloud computing,cloud providers,cloud services,genetic algorithms,Genetic algorithms,greedy algorithms,greedy resource provisioning,GRP-HEFT,HEFT algorithm,heterogeneous IaaS cloud resources,hourly-based cost model,modern IaaS clouds,particle swarm optimisation,pay-per-use model,resource allocation,resource provisioning,resource provisioning mechanism,Schedules,scheduling,Scheduling,Scheduling algorithms,scheduling approaches,scientific workflow applications,state-of-the-art workflow,Task analysis,workflow management software,workflow scheduling,workflow scheduling algorithm,workflow subject},
  number = {6}
}

@inproceedings{fard2012,
  ids = {fard2012a,fard2012b},
  title = {A {{Multi}}-Objective {{Approach}} for {{Workflow Scheduling}} in {{Heterogeneous Environments}}},
  booktitle = {2012 12th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} (Ccgrid 2012)},
  author = {Fard, H. M. and Prodan, R. and Barrionuevo, J. J. D. and Fahringer, T.},
  year = {2012},
  month = may,
  pages = {300--309},
  doi = {10.1109/CCGrid.2012.114},
  abstract = {Traditional scheduling research usually targets make span as the only optimization goal, while several isolated efforts addressed the problem by considering at most two objectives. In this paper we propose a general framework and heuristic algorithm for multi-objective static scheduling of scientific workflows in heterogeneous computing environments. The algorithm uses constraints specified by the user for each objective and approximates the optimal solution by applying a double strategy: maximizing the distance to the constraint vector for dominant solutions and minimizing it otherwise. We analyze and classify different objectives with respect to their impact on the optimization process and present a four-objective case study comprising make span, economic cost, energy consumption, and reliability. We implemented the algorithm as part of the ASKALON environment for Grid and Cloud computing. Results for two real-world applications demonstrate that the solutions generated by our algorithm are superior to user-defined constraints most of the time. Moreover, the algorithm outperforms a related bi-criteria heuristic and a bi-criteria genetic algorithm.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/fard_et_al_2012_a_multi-objective_approach_for_workflow_scheduling_in_heterogeneous_environments.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/fard_et_al_2012_a_multi-objective_approach_for_workflow_scheduling_in_heterogeneous_environments.pdf;/Users/rwb/Zotero/storage/267P5F8Z/references.html;/Users/rwb/Zotero/storage/4ZX4NVQ9/6217435.html;/Users/rwb/Zotero/storage/AYCYBWYJ/6217435.html},
  keywords = {ASKALON environment,bi-criteria genetic algorithm,bi-criteria heuristic algorithm,cloud computing,Complexity theory,Computational modeling,computing systems,constraint vector distance maximization,constraint vector distance minimization,economic cost objective,Energy consumption,energy consumption objective,genetic algorithms,grid computing,Grids and Clouds,heterogeneous computing environments,make span objective,minimisation,multi-objective optimization,multiobjective static scheduling,natural sciences computing,Optimization,optimization goal,optimization process,Processor scheduling,Reliability,reliability objective,scheduling,scientific workflows,Vectors,workflow management software,workflow scheduling}
}

@article{fard2014,
  title = {Multi-Objective List Scheduling of Workflow Applications in Distributed Computing Infrastructures},
  author = {Fard, Hamid Mohammadi and Prodan, Radu and Fahringer, Thomas},
  year = {2014},
  month = mar,
  volume = {74},
  pages = {2152--2165},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2013.12.004},
  abstract = {Executing large-scale applications in distributed computing infrastructures (DCI), for example modern Cloud environments, involves optimization of several conflicting objectives such as makespan, reliability, energy, or economic cost. Despite this trend, scheduling in heterogeneous DCIs has been traditionally approached as a single or bi-criteria optimization problem. In this paper, we propose a generic multi-objective optimization framework supported by a list scheduling heuristic for scientific workflows in heterogeneous DCIs. The algorithm approximates the optimal solution by considering user-specified constraints on objectives in a dual strategy: maximizing the distance to the user's constraints for dominant solutions and minimizing it otherwise. We instantiate the framework and algorithm for a four-objective case study comprising makespan, economic cost, energy consumption, and reliability as optimization goals. We implemented our method as part of the ASKALON environment (Fahringer et~al., 2007) for Grid and Cloud computing and demonstrate through extensive real and synthetic simulation experiments that our algorithm outperforms related bi-criteria heuristics while meeting the user constraints most of the time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/fard_et_al_2014_multi-objective_list_scheduling_of_workflow_applications_in_distributed.pdf;/Users/rwb/Zotero/storage/BEZSG6WC/S0743731513002384.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {Distributed computing infrastructures,Multi-objective scheduling,Scientific workflows},
  number = {3}
}

@article{farzin2017,
  title = {A {{Stochastic Multi}}-{{Objective Framework}} for {{Optimal Scheduling}} of {{Energy Storage Systems}} in {{Microgrids}}},
  author = {Farzin, H. and {Fotuhi-Firuzabad}, M. and {Moeini-Aghtaie}, M.},
  year = {2017},
  month = jan,
  volume = {8},
  pages = {117--127},
  issn = {1949-3053},
  doi = {10.1109/TSG.2016.2598678},
  abstract = {This paper presents a stochastic framework for day-ahead scheduling of microgrid energy storage systems in the context of multi-objective (MO) optimization. Operation cost of microgrid in normal conditions and load curtailment index in case of unscheduled islanding events (initiated by disturbances in the main grid) are chosen as main criteria of the proposed scheme. In practice, duration of disconnection from the upstream network is unknown in unscheduled islanding incidents and cannot be predicted with certainty. To properly handle the uncertainties associated with time and duration of such events as well as microgrid load and renewable power generation, stochastic models are involved in the MO scheduling framework and they are formulated as mixed integer linear programming problems. The non-dominated sorting genetic algorithm II is employed to effectively cope with the MO optimization problem and a fuzzy decision making approach is employed for appropriate representation of microgrid operator's preferences in compromising between the two objectives. The proposed scheme is implemented on a test microgrid and the obtained results demonstrate the applicability and efficiency of this framework in dealing with conflicting requirements of microgrid security and economic operation.},
  file = {/Users/rwb/Zotero/storage/524C7UTY/7553493.html},
  journal = {IEEE Transactions on Smart Grid},
  keywords = {day-ahead scheduling,distributed power generation,energy storage,energy storage system optimal scheduling,Energy storage systems (ESSs),fuzzy decision making approach,genetic algorithms,Indexes,Islanding,load curtailment index,Load modeling,microgrid,microgrid economic operation,microgrid energy storage systems,microgrid security,Microgrids,mixed integer linear programming problems,MO optimization,multiobjective optimization,non-dominated sorting genetic algorithm II (NSGAII),nondominated sorting genetic algorithm II,Optimal scheduling,scheduling,stochastic multiobjective framework,stochastic optimization,stochastic processes,Stochastic processes,Uncertainty,unscheduled islanding,unscheduled islanding events},
  number = {1}
}

@article{ferreiradasilva2017,
  title = {A Characterization of Workflow Management Systems for Extreme-Scale Applications},
  author = {{Ferreira da Silva}, Rafael and Filgueira, Rosa and Pietri, Ilia and Jiang, Ming and Sakellariou, Rizos and Deelman, Ewa},
  year = {2017},
  month = oct,
  volume = {75},
  pages = {228--238},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.02.026},
  abstract = {Automation of the execution of computational tasks is at the heart of improving scientific productivity. Over the last years, scientific workflows have been established as an important abstraction that captures data processing and computation of large and complex scientific applications. By allowing scientists to model and express entire data processing steps and their dependencies, workflow management systems relieve scientists from the details of an application and manage its execution on a computational infrastructure. As the resource requirements of today's computational and data science applications that process vast amounts of data keep increasing, there is a compelling case for a new generation of advances in high-performance computing, commonly termed as extreme-scale computing, which will bring forth multiple challenges for the design of workflow applications and management systems. This paper presents a novel characterization of workflow management systems using features commonly associated with extreme-scale computing applications. We classify 15 popular workflow management systems in terms of workflow execution models, heterogeneous computing environments, and data access methods. The paper also surveys workflow applications and identifies gaps for future research on the road to extreme-scale workflows and management systems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/ferreira_da_silva_et_al_2017_a_characterization_of_workflow_management_systems_for_extreme-scale_applications.pdf;/Users/rwb/Zotero/storage/7VHZPGHZ/S0167739X17302510.html},
  journal = {Future Generation Computer Systems},
  keywords = {Extreme-scale computing,processing,Scientific workflows,Workflow management systems}
}

@article{fischetti2018,
  title = {Deep Neural Networks and Mixed Integer Linear Optimization},
  author = {Fischetti, Matteo and Jo, Jason},
  year = {2018},
  month = jul,
  volume = {23},
  pages = {296--309},
  issn = {1383-7133, 1572-9354},
  doi = {10.1007/s10601-018-9285-6},
  abstract = {Deep Neural Networks (DNNs) are very popular these days, and are the subject of a very intense investigation. A DNN is made up of layers of internal units (or neurons), each of which computes an affine combination of the output of the units in the previous layer, applies a nonlinear operator, and outputs the corresponding value (also known as activation). A commonly-used nonlinear operator is the so-called rectified linear unit (ReLU), whose output is just the maximum between its input value and zero. In this (and other similar cases like max pooling, where the max operation involves more than one input value), for fixed parameters one can model the DNN as a 0-1 Mixed Integer Linear Program (0-1 MILP) where the continuous variables correspond to the output values of each unit, and a binary variable is associated with each ReLU to model its yes/no nature. In this paper we discuss the peculiarity of this kind of 0-1 MILP models, and describe an effective boundtightening technique intended to ease its solution. We also present possible applications of the 0-1 MILP model arising in feature visualization and in the construction of adversarial examples. Computational results are reported, aimed at investigating (on small DNNs) the computational performance of a state-of-the-art MILP solver when applied to a known test case, namely, hand-written digit recognition.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/fischetti_jo_2018_deep_neural_networks_and_mixed_integer_linear_optimization.pdf},
  journal = {Constraints},
  language = {en},
  number = {3}
}

@article{floudas2005,
  ids = {floudas2005},
  title = {Mixed {{Integer Linear Programming}} in {{Process Scheduling}}: {{Modeling}}, {{Algorithms}}, and {{Applications}}},
  shorttitle = {Mixed {{Integer Linear Programming}} in {{Process Scheduling}}},
  author = {Floudas, Christodoulos A. and Lin, Xiaoxia},
  year = {2005},
  month = oct,
  volume = {139},
  pages = {131--162},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/s10479-005-3446-x},
  abstract = {This paper reviews the advances of mixed-integer linear programming (MILP) based approaches for the scheduling of chemical processing systems. We focus on the short-term scheduling of general network represented processes. First, the various mathematical models that have been proposed in the literature are classified mainly based on the time representation. Discrete-time and continuous-time models are presented along with their strengths and limitations. Several classes of approaches for improving the computational efficiency in the solution of MILP problems are discussed. Furthermore, a summary of computational experiences and applications is provided. The paper concludes with perspectives on future research directions for MILP based process scheduling technologies.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/floudas_lin_2005_mixed_integer_linear_programming_in_process_scheduling.pdf;/Users/rwb/Dropbox/PhD/zotero/2005/floudas_lin_2005_mixed_integer_linear_programming_in_process_scheduling.pdf;/Users/rwb/Zotero/storage/8TLUJAMV/s10479-005-3446-x.html},
  journal = {Annals of Operations Research},
  language = {en},
  number = {1}
}

@inproceedings{foster1999,
  title = {A Distributed Resource Management Architecture That Supports Advance Reservations and Co-Allocation},
  booktitle = {1999 {{Seventh International Workshop}} on {{Quality}} of {{Service}}. {{IWQoS}}'99. ({{Cat}}. {{No}}.{{98EX354}})},
  author = {Foster, I. and Kesselman, C. and Lee, C. and Lindell, B. and Nahrstedt, K. and Roy, A.},
  year = {1999},
  pages = {27--36},
  publisher = {{IEEE}},
  address = {{London, UK}},
  doi = {10.1109/IWQOS.1999.766475},
  abstract = {The realization of end-to-end quality of service (QoS) guarantees in emerging network-based applications requires mechanisms that support first dynamic discovery and then advance or immediate reservation of resources that will often be heterogeneous in type and implementation and independently controlled and administered. We propose the Globus Architecture for Reservation and Allocation (GARA) to address these four issues. GARA treats both reservations and computational elements such as processes, network flows, and memory blocks as first class entities, allowing them to be created, monitored, and managed independently and uniformly. It simplifies management of heterogeneous resource types by defining uniform mechanisms for computers, networks, disk, memory, and other resources. Layering on these standard mechanisms, GARA enables the construction of application-level co-reservation and coallocation libraries that applications can use to dynamically assemble collections of resources, guided by both application QoS requirements and the local administration policy of individual resources. We describe a prototype GARA implementation that supports three different resource types\textemdash parallel computers, individual CPUs under control of the Dynamic Soft Real-Time scheduler, and Integrated Services networks\textemdash and provide performance results that quantify the costs of our techniques.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1999/foster_et_al_1999_a_distributed_resource_management_architecture_that_supports_advance.pdf},
  isbn = {978-0-7803-5671-9},
  language = {en}
}

@inproceedings{foster2008,
  title = {Cloud {{Computing}} and {{Grid Computing}} 360-{{Degree Compared}}},
  booktitle = {2008 {{Grid Computing Environments Workshop}}},
  author = {Foster, I. and Zhao, Y. and Raicu, I. and Lu, S.},
  year = {2008},
  month = nov,
  pages = {1--10},
  doi = {10.1109/GCE.2008.4738445},
  abstract = {Cloud computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for cloud computing and there seems to be no consensus on what a cloud is. On the other hand, cloud computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established grid computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast cloud computing with grid computing from various angles and give insights into the essential characteristics of both.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/foster_et_al_2008_cloud_computing_and_grid_computing_360-degree_compared.pdf;/Users/rwb/Zotero/storage/C8GBK2T7/4738445.html},
  keywords = {cloud computing,Cloud computing,cluster computing,Computer science,Computer vision,Costs,Distributed computing,distributed system,Economies of scale,grid computing,Grid computing,Laboratories,Large-scale systems,Standards organizations,utility computing}
}

@article{fowler2003,
  title = {Branching {{Constraint Satisfaction Problems}} and {{Markov Decision Problems Compared}}},
  author = {Fowler, David W. and Brown, Kenneth N.},
  year = {2003},
  month = feb,
  volume = {118},
  pages = {85--100},
  issn = {1572-9338},
  doi = {10.1023/A:1021853506616},
  abstract = {Branching Constraint Satisfaction Problems (BCSPs) model a class of uncertain dynamic resource allocation problems. We describe the features of BCSPs, and show that the associated decision problem is NP-complete. Markov Decision Problems could be used in place of BCSPs, but we show analytically and empirically that, for the class of problems in question, the BCSP algorithms are more efficient than the related MDP algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2003/fowler_brown_2003_branching_constraint_satisfaction_problems_and_markov_decision_problems_compared.pdf},
  journal = {Annals of Operations Research},
  keywords = {_tablet,constraint satisfaction,Markov decision problems,uncertainty},
  language = {en},
  number = {1}
}

@article{galstyan2005,
  title = {Resource {{Allocation}} in the {{Grid}} with {{Learning Agents}}},
  author = {Galstyan, Aram and Czajkowski, Karl and Lerman, Kristina},
  year = {2005},
  month = jun,
  volume = {3},
  pages = {91--100},
  issn = {1572-9184},
  doi = {10.1007/s10723-005-9003-7},
  abstract = {One of the main challenges in Grid computing is efficient allocation of resources (CPU \textendash{} hours, network bandwidth, etc.) to the tasks submitted by users. Due to the lack of centralized control and the dynamic/stochastic nature of resource availability, any successful allocation mechanism should be highly distributed and robust to the changes in the Grid environment. Moreover, it is desirable to have an allocation mechanism that does not rely on the availability of coherent global information. In this paper we examine a simple algorithm for distributed resource allocation in a simplified Grid-like environment that meets the above requirements. Our system consists of a large number of heterogenous reinforcement learning agents that share common resources for their computational needs. There is no explicit communication or interaction between the agents: the only information that agents receive is the expected response time of a job it submitted to a particular resource, which serves as a reinforcement signal for the agent. The results of our experiments suggest that even simple reinforcement learning can indeed be used to achieve load balanced resource allocation in large scale heterogenous system.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/galstyan_et_al_2005_resource_allocation_in_the_grid_with_learning_agents.pdf},
  journal = {Journal of Grid Computing},
  keywords = {Grid,multi-agent system,reinforcement learning,resource allocation},
  language = {en},
  number = {1}
}

@article{garcia-piquer2017,
  ids = {garcia-piquer2017a,garcia-piquer2017b},
  title = {Efficient Scheduling of Astronomical Observations: {{Application}} to the {{CARMENES}} Radial-Velocity Survey},
  shorttitle = {Efficient Scheduling of Astronomical Observations},
  author = {{Garcia-Piquer}, A. and Morales, J. C. and Ribas, I. and Colom{\'e}, J. and Gu{\`a}rdia, J. and Perger, M. and Caballero, J. A. and {Cort{\'e}s-Contreras}, M. and Jeffers, S. V. and Reiners, A. and Amado, P. J. and Quirrenbach, A. and Seifert, W.},
  year = {2017},
  month = aug,
  volume = {604},
  pages = {A87},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/201628577},
  abstract = {Methods. We used evolutionary computation techniques to create an automatic scheduler that minimizes the idle periods of the telescope and distributes the observations among all the targets using configurable criteria. We simulated the case of the CARMENES survey with a realistic sample of targets, and we estimated the efficiency of the planning tool both in terms of telescope operations and planet detection.
Results. Our scheduling simulations produce plans that use about 99\% of the available telescope time (including overheads) and optimally distribute the observations among the different targets. Under such conditions, and using current planet statistics, the optimized plan using this tool should allow the CARMENES survey to discover about 65\% of the planets with radial-velocity semi-amplitudes greater than 1 m s-1 when considering only photon noise.
Conclusions. The simulations using our scheduling tool show that it is possible to optimize the survey planning by minimizing idle instrument periods and fulfilling the science objectives in an efficient manner to maximize the scientific return.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/garcia-piquer_et_al_2017_efficient_scheduling_of_astronomical_observations.pdf;/Users/rwb/Dropbox/PhD/zotero/2017/garcia-piquer_et_al_2017_efficient_scheduling_of_astronomical_observations2.pdf;/Users/rwb/Dropbox/PhD/zotero/2017/garcia-piquer_et_al_2017_efficient_scheduling_of_astronomical_observations3.pdf},
  journal = {Astronomy \& Astrophysics},
  language = {en}
}

@article{garcia-valls2018,
  title = {Pragmatic Cyber-Physical Systems Design Based on Parametric Models},
  author = {{Garc{\'i}a-Valls}, Marisol and {Perez-Palacin}, Diego and Mirandola, Raffaela},
  year = {2018},
  month = jun,
  issn = {0164-1212},
  doi = {10.1016/j.jss.2018.06.044},
  abstract = {The adaptive nature of cyber-physical systems (CPS) comes from the fact that they are deeply immersed in the physical environments that is inherently dynamic. CPS also have stringent requirements on real-time operation and safety that are fulfilled by rigorous model design and verification. In the real-time literature, adaptation is mostly limited to off-line modeling of predicted system transitions. In the adaptive systems literature, adaptation solutions are silent about timely execution and about the underlying hardware possibilities that can potentially speed up execution. This paper presents a solution for designing adaptive cyber-physical systems by using parametric models that are verified during the system execution (i.e., online), so that adaptation decisions are made based on the timing requirements of each particular adaptation event. Our approach allows the system to undergo timely adaptations that exploit the potential parallelism of the software and its execution over multicore processors. We exemplify the approach on a specific use case with autonomous vehicles communication, showing its applicability for situations that require time-bounded online adaptations.},
  file = {Dropbox/library/2018/Garca-Valls et al/garca-valls_et_al_2018_pragmatic_cyber-physical_systems_design_based_on_parametric_models.pdf;/Users/rwb/Zotero/storage/E49NJFRI/S0164121218301298.html},
  journal = {Journal of Systems and Software},
  keywords = {adaptive systems,autonomous systems,CPS,verification}
}

@article{garg2012,
  title = {Reference {{Point Based Evolutionary Approach forWorkflow Grid Scheduling}}},
  author = {Garg, R.},
  year = {2012},
  issn = {20103719},
  doi = {10.7763/IJIEE.2012.V2.147},
  abstract = {Grid computing facilitates the users to consume the services over the network. In order to optimize the workflow execution, multi-objective scheduling algorithm is required. In this paper, we considered two conflicting objectives of execution time (makespan) and total cost. We propose a scheduling algorithm, using Reference Point Based multi-objective evolutionary algorithm (R-NSGA-II), which provides the optimal scheduling solutions near the regions of user preference within the given quality of service constraints. The simulation results show the multiple solutions are obtained near each user specified regions of interest.},
  file = {/Users/rwb/Zotero/storage/S8L9P3KB/Garg - 2012 - Reference Point Based Evolutionary Approach forWor.pdf},
  journal = {International Journal of Information and Electronics Engineering},
  language = {en}
}

@article{gedik2018,
  title = {A Constraint Programming Approach for Solving Unrelated Parallel Machine Scheduling Problem},
  author = {Gedik, Ridvan and Kalathia, Darshan and Egilmez, Gokhan and Kirac, Emre},
  year = {2018},
  month = jul,
  volume = {121},
  pages = {139--149},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2018.05.014},
  abstract = {This paper addresses the non-preemptive unrelated parallel machine scheduling problem (PMSP) with job sequence and machine dependent setup times. This is a widely seen NP-hard (non-deterministic polynomial-time) problem with the objective to minimize the makespan. This study provides a noval constraint programming (CP) model with two customized branching strategies that utilize CP's global constraints, interval decision variables, and domain filtering algorithms. The performance of the CP model is evaluated against the state-of-art algorithms. In addition, we compare the performance of the default branching method in the CP solver against the two customized variants. In terms of average solution quality, the computational results indicate that the CP model slightly outperforms all of the state-of-art algorithms in solving small problem instances, is able to prove the optimality of 283 currently best-known solutions. It is also effective in finding good quality feasible solutions for the larger problem instances.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/gedik_et_al_2018_a_constraint_programming_approach_for_solving_unrelated_parallel_machine.pdf;/Users/rwb/Zotero/storage/AWFYHNC4/S0360835218302158.html},
  journal = {Computers \& Industrial Engineering},
  keywords = {Constraint programming,Interval variables,Machine dependent setup times,Sequence dependent setup times,Unrelated parallel machine scheduling}
}

@inproceedings{geiger2018,
  ids = {geiger2018a},
  title = {Long {{Term Planning}} for the {{ExoMars Trace Gas Orbiter Mission}}: {{Opportunity Analysis}} and {{Observation Scheduling}}},
  shorttitle = {Long {{Term Planning}} for the {{ExoMars Trace Gas Orbiter Mission}}},
  booktitle = {15th {{International Conference}} on {{Space Operations}}},
  author = {Geiger, Bernhard and Cardesin Moinelo, Alejandro and Frew, David and Ashman, Mike and Garcia Beteta, Juan Jose and Mu{\~n}oz, Michela and Costa, Marc and Metcalfe, Leo and Svedhem, Hakan},
  year = {2018},
  month = may,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {{Marseille, France}},
  doi = {10.2514/6.2018-2608},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/geiger_et_al_2018_long_term_planning_for_the_exomars_trace_gas_orbiter_mission.pdf;/Users/rwb/Dropbox/PhD/zotero/2018/geiger_et_al_2018_long_term_planning_for_the_exomars_trace_gas_orbiter_mission2.pdf},
  isbn = {978-1-62410-562-3},
  language = {en}
}

@inproceedings{genez2012,
  ids = {genez2012a},
  title = {Workflow Scheduling for {{SaaS}} / {{PaaS}} Cloud Providers Considering Two {{SLA}} Levels},
  booktitle = {2012 {{IEEE Network Operations}} and {{Management Symposium}}},
  author = {Genez, T. A. L. and Bittencourt, L. F. and Madeira, E. R. M.},
  year = {2012},
  month = apr,
  pages = {906--912},
  doi = {10.1109/NOMS.2012.6212007},
  abstract = {Cloud computing is being used to avoid maintenance costs and upfront investment, while providing elasticity to the available computational power in a pay-per-use basis. Customers can make use of the cloud as a software (SaaS), platform (PaaS), or infrastructure (IaaS) provider. When one customer utilizes an environment provided by a SaaS cloud, she is unaware of any details about the computational infrastructure where her requests are being processed. Therefore, such infrastructure can be composed of computational resources from a datacenter owned by the SaaS or its resources can be leased from a cloud infrastructure provider. In this paper we present an integer linear program (ILP) formulation for the problem of scheduling SaaS customer's workflows into multiple IaaS providers where SLA exists at two levels. In addition, we present heuristics to solve the relaxed version of the presented ILP. Simulation results show that the proposed ILP is able to find low-cost solutions for short deadlines, while the proposed heuristics are effective when deadlines are larger.},
  file = {/Users/rwb/Zotero/storage/N2WZDSAW/6212007.html;/Users/rwb/Zotero/storage/N8SCDGZD/6212007.html},
  keywords = {cloud computing,Cloud computing,computational infrastructure,Computational modeling,computational power,computational resources,Contracts,IaaS,ILP,infrastructure provider,integer linear program formulation,integer programming,linear programming,maintenance cost avoidance,PaaS cloud providers,pay-per-use basis,platform provider,Quality of service,resource allocation,SaaS cloud providers,scheduling,Scheduling,Scheduling algorithms,SLA levels,software provider,upfront investment,workflow management software,workflow scheduling}
}

@article{gershwin1989,
  title = {Hierarchical Flow Control: A Framework for Scheduling and Planning Discrete Events in Manufacturing Systems},
  shorttitle = {Hierarchical Flow Control},
  author = {Gershwin, S. B.},
  year = {1989},
  month = jan,
  volume = {77},
  pages = {195--209},
  issn = {0018-9219},
  doi = {10.1109/5.21079},
  abstract = {The synthesis of operating policies for manufacturing systems is discussed. These are feedback laws that respond to potentially disruptive events. Laws are developed that are based on realistic dynamic programming models that account for the discrete nature of manufacturing and are computationally tractable. These scheduling and planning policies have a hierarchical structure which is systematically based on the production process. The levels of the hierarchy correspond to classes of events that occur with distinct frequencies. At each level, feedback laws select times for the controllable events whose frequency class is treated at that level and frequency targets for much higher-frequency controllable events.{$<$}{$>$}},
  file = {/Users/rwb/Dropbox/PhD/zotero/1989/gershwin_1989_hierarchical_flow_control.pdf;/Users/rwb/Zotero/storage/NGE82BUN/21079.html},
  journal = {Proceedings of the IEEE},
  keywords = {Computer aided manufacturing,Control system synthesis,discrete events,dynamic programming,Dynamic programming,dynamic programming models,feedback,Feedback,Frequency,hierarchical flow control,hierarchical systems,Job shop scheduling,manufacturing systems,Manufacturing systems,operations research,planning,Process planning,Processor scheduling,production control,scheduling,Virtual manufacturing},
  number = {1}
}

@article{ghaemi2017,
  title = {Railway Disruption Management Challenges and Possible Solution Directions},
  author = {Ghaemi, Nadjla and Cats, Oded and Goverde, Rob M. P.},
  year = {2017},
  month = jul,
  volume = {9},
  pages = {343--364},
  issn = {1866-749X, 1613-7159},
  doi = {10.1007/s12469-017-0157-z},
  abstract = {This paper investigates the challenges of railway traffic controllers in dealing with big disruptions and the kind of support tools that could help to improve their task in terms of performance, lead time and workload. The disruption handling process can be partitioned into three phases resembling a bathtub. For each phase the essential decision making process has been identified. Currently, the support to rail traffic controllers in case of severe disruptions is limited to predefined contingency plans that are not always feasible or applicable. In the literature, models and algorithms have been identified that could be used in the different parts of the three phases of the disruption handling process. This paper investigates the processes of disruption management in practice and the challenges that traffic controllers are facing during a disruption. The literature of models applicable to disruption management is reviewed and classified based on the three phases of the traffic state during disruptions. Finally, a rescheduling optimization model is applied to a case of complete blockage on a corridor of the Dutch railway network. The case study shows how a microscopic model could support the traffic controllers by providing real-time solutions for different phases of a disruption.},
  file = {/Users/rwb/Zotero/storage/I44S9W2Q/Ghaemi et al. - 2017 - Railway disruption management challenges and possi.pdf},
  journal = {Public Transport},
  language = {en},
  number = {1-2}
}

@article{ghobaei-arani2018,
  title = {An Autonomic Resource Provisioning Approach for Service-Based Cloud Applications: {{A}} Hybrid Approach},
  shorttitle = {An Autonomic Resource Provisioning Approach for Service-Based Cloud Applications},
  author = {{Ghobaei-Arani}, Mostafa and Jabbehdari, Sam and Pourmina, Mohammad Ali},
  year = {2018},
  month = jan,
  volume = {78},
  pages = {191--210},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.02.022},
  abstract = {In cloud computing environment, resources can be dynamically provisioned on deman for cloud services The amount of the resources to be provisioned is determined during runtime according to the workload changes. Deciding the right amount of resources required to run the cloud services is not trivial, and it depends on the current workload of the cloud services. Therefore, it is necessary to predict the future demands to automatically provision resources in order to deal with fluctuating demands of the cloud services. In this paper, we propose a hybrid resource provisioning approach for cloud services that is based on a combination of the concept of the autonomic computing and the reinforcement learning (RL). Also, we present a framework for autonomic resource provisioning which is inspired by the cloud layer model. Finally, we evaluate the effectiveness of our approach under two real world workload traces. The experimental results show that the proposed approach reduces the total cost by up to 50\%, and increases the resource utilization by up to 12\% compared with the other approaches.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/ghobaei-arani_et_al_2018_an_autonomic_resource_provisioning_approach_for_service-based_cloud_applications.pdf;/Users/rwb/Zotero/storage/D2796KKC/S0167739X17302327.html},
  journal = {Future Generation Computer Systems},
  keywords = {Autonomic computing,Cloud computing,Cloud services,Reinforcement learning,Resource provisioning}
}

@article{gil2004,
  title = {Artificial Intelligence and Grids: Workflow Planning and Beyond},
  shorttitle = {Artificial Intelligence and Grids},
  author = {Gil, Y. and Deelman, E. and Blythe, J. and Kesselman, C. and Tangmunarunkit, H.},
  year = {2004},
  month = jan,
  volume = {19},
  pages = {26--33},
  issn = {1541-1672},
  doi = {10.1109/MIS.2004.1265882},
  abstract = {A key challenge for grid computing is creating large-scale, end-to-end scientific applications that draw from pools of specialized scientific components to derive elaborate new results. We develop Pegasus, an AI planning system which is integrated into the grid environment that takes a user's highly specified desired results, generates valid workflows that take into account available resources, and submits the workflows for execution on the grid. We also begin to extend it as a more distributed and knowledge-rich architecture.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2004/gil_et_al_2004_artificial_intelligence_and_grids.pdf;/Users/rwb/Zotero/storage/J9T2EUJF/1265882.html},
  journal = {IEEE Intelligent Systems},
  keywords = {artificial intelligence,Artificial intelligence,Biology computing,Distributed computing,Earthquake engineering,Geophysics computing,grid computing,Grid computing,High energy physics instrumentation computing,Large-scale systems,Mesh generation,Pegasus system,Physics computing,planning (artificial intelligence),scientific application,scientific information systems,workflow management software,workflow planning system},
  number = {1}
}

@article{gogos2016,
  title = {Scheduling Independent Tasks on Heterogeneous Processors Using Heuristics and {{Column Pricing}}},
  author = {Gogos, Christos and Valouxis, Christos and Alefragis, Panayiotis and Goulas, George and Voros, Nikolaos and Housos, Efthymios},
  year = {2016},
  month = jul,
  volume = {60},
  pages = {48--66},
  issn = {0167-739X},
  doi = {10.1016/j.future.2016.01.016},
  abstract = {Efficiently scheduling a set of independent tasks on a virtual supercomputer formed by many heterogeneous components has great practical importance, since such systems are commonly used nowadays. Scheduling efficiency can be seen as the problem of minimizing the overall execution time (makespan) of the set of tasks under question. This problem is known to be NP-hard and is currently addressed using heuristics, evolutionary algorithms and other optimization methods. In this paper, firstly, two novel fast executing heuristics, called LSufferage and TPB, are introduced. L(ist)Sufferage is based on the known heuristic Sufferage and can achieve in general better results than it for most of the cases. T(enacious)PB is also based on another heuristic (Penalty Based) and incorporates new ideas that significantly improve the quality of the resulted schedule. Secondly, a mathematical model of the problem is presented alongside with an associated approach based on the Linear Programming method of Column Pricing. This approach, which is called Column Pricing with Restarts (CPR), can be categorized as a hybrid mathematical programming and heuristic approach and is capable of solving in reasonable time problem instances of practically any size. Experiments show that CPR achieves superior results improving over published results on problem instances of various sizes. Moreover, hardware requirements of CPR are minimal.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/gogos_et_al_2016_scheduling_independent_tasks_on_heterogeneous_processors_using_heuristics_and.pdf;/Users/rwb/Zotero/storage/XDAHJXJJ/S0167739X16000297.html},
  journal = {Future Generation Computer Systems},
  keywords = {Column pricing,Heterogeneous processors,Heuristics,Independent tasks,Mathematical programming,Task scheduling}
}

@article{gomes2000,
  title = {Artificial Intelligence and Operations Research: Challenges and Opportunities in Planning and Scheduling},
  shorttitle = {Artificial Intelligence and Operations Research},
  author = {Gomes, Carla P.},
  year = {2000},
  month = mar,
  volume = {15},
  pages = {1--10},
  issn = {1469-8005, 0269-8889},
  abstract = {Both the Artificial Intelligence (AI) and the Operations Research (OR) communities are interested in developing techniques for solving hard combinatorial problems, in particular in the domain of planning and scheduling. AI approaches encompass a rich collection of knowledge representation formalisms for dealing with a wide variety of real-world problems. Some examples are constraint programming representations, logical formalisms, declarative and functional programming languages such as Prolog and Lisp, Bayesian models, rule-based formalism, etc. The downside of such rich representations is that in general they lead to intractable problems, and we therefore often cannot use such formalisms for handling realistic size problems. OR, on the other hand, has focused on more tractable representations, such as linear programming formulations. OR-based techniques have demonstrated the ability to identify optimal and locally optimal solutions for well-defined problem spaces. In general, however, OR solutions are restricted to rigid models with limited expressive power. AI techniques, on the other hand, provide richer and more flexible representations of real-world problems, supporting efficient constraint-based reasoning mechanisms as well as mixed initiative frameworks, which allow the human expertise to be in the loop. The challenge lies in providing representations that are expressive enough to describe real-world problems and at the same time guaranteeing good and fast solutions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2000/gomes_2000_artificial_intelligence_and_operations_research.pdf;/Users/rwb/Zotero/storage/MDMFNRQW/2EC6CC745CF1FF25234A3C63D2474A41.html},
  journal = {The Knowledge Engineering Review},
  language = {en},
  number = {1}
}

@article{gonzalez1977,
  title = {Deterministic {{Processor Scheduling}}},
  author = {Gonzalez, Jr., Mario J.},
  year = {1977},
  month = sep,
  volume = {9},
  pages = {173--204},
  issn = {0360-0300},
  doi = {10.1145/356698.356700},
  file = {/Users/rwb/Dropbox/PhD/zotero/1977/gonzalez_1977_deterministic_processor_scheduling.pdf},
  journal = {ACM Comput. Surv.},
  number = {3}
}

@article{granja2014,
  title = {An Optimization Based on Simulation Approach to the Patient Admission Scheduling Problem Using a Linear Programing Algorithm},
  author = {Granja, C. and {Almada-Lobo}, B. and Janela, F. and Seabra, J. and Mendes, A.},
  year = {2014},
  month = dec,
  volume = {52},
  pages = {427--437},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2014.08.007},
  abstract = {Background
As patient's length of stay in waiting lists increases, governments are looking for strategies to control the problem. Agreements were created with private providers to diminish the workload in the public sector. However, the growth of the private sector is not following the demand for care. Given this context, new management strategies have to be considered in order to minimize patient length of stay in waiting lists while reducing the costs and increasing (or at least maintaining) the quality of care.
Method
Appointment scheduling systems are today known to be proficient in the optimization of health care services. Their utilization is focused on increasing the usage of human resources, medical equipment and reducing the patient waiting times. In this paper, a simulation-based optimization approach to the Patient Admission Scheduling Problem is presented. Modeling tools and simulation techniques are used in the optimization of a diagnostic imaging department.
Results
The proposed techniques have demonstrated to be effective in the evaluation of diagnostic imaging workflows. A simulated annealing algorithm was used to optimize the patient admission sequence towards minimizing the total completion and total waiting of patients. The obtained results showed average reductions of 5\% on the total completion and 38\% on the patients' total waiting time.},
  file = {/Users/rwb/Zotero/storage/CN2MFTYE/S1532046414001889.html},
  journal = {Journal of Biomedical Informatics},
  keywords = {Diagnostic imaging,Organizational case studies,Patient admission,Personnel staffing and scheduling,Process assessment,Workflow},
  series = {Special {{Section}}: {{Methods}} in {{Clinical Research Informatics}}}
}

@inproceedings{gummaraju2005,
  title = {Stream {{Programming}} on {{General}}-{{Purpose Processors}}},
  booktitle = {Proceedings of the 38th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Gummaraju, Jayanth and Rosenblum, Mendel},
  year = {2005},
  pages = {343--354},
  publisher = {{IEEE Computer Society}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/MICRO.2005.32},
  abstract = {In this paper we investigate mapping stream programs (i.e., programs written in a streaming style for streaming architectures such as Imagine and Raw) onto a general-purpose CPU. We develop and explore a novel way of mapping these programs onto the CPU. We show how the salient features of stream programming such as computation kernels, local memories, and asynchronous bulk memory loads and stores can be easily mapped by a simple compilation system to CPU features such as the processor caches, simultaneous multi-threading, and fast inter-thread communication support, resulting in an executable that efficiently uses CPU resources. We present an evaluation of our mapping on a hyperthreaded Intel Pentium 4 CPU as a canonical example of a general-purpose processor. We compare the mapped stream program against the same program coded in a more conventional style for the general-purpose processor. Using both micro-benchmarks and scientific applications we show that programs written in a streaming style can run comparably to equivalent programs written in a traditional style. Our results show that coding programs in a streaming style can improve performance on today\textquestiondown s machines and smooth the way for significant performance improvements with the deployment of streaming architectures.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/gummaraju_rosenblum_2005_stream_programming_on_general-purpose_processors.pdf},
  isbn = {978-0-7695-2440-5},
  keywords = {_tablet,hyper-threading.,prefetching,stream architectures/programming},
  series = {{{MICRO}} 38}
}

@article{guns2017,
  title = {{{MiningZinc}}: {{A}} Declarative Framework for Constraint-Based Mining},
  shorttitle = {{{MiningZinc}}},
  author = {Guns, Tias and Dries, Anton and Nijssen, Siegfried and Tack, Guido and De Raedt, Luc},
  year = {2017},
  month = mar,
  volume = {244},
  pages = {6--29},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2015.09.007},
  abstract = {We introduce MiningZinc, a declarative framework for constraint-based data mining. MiningZinc consists of two key components: a language component and an execution mechanism. First, the MiningZinc language allows for high-level and natural modeling of mining problems, so that MiningZinc models are similar to the mathematical definitions used in the literature. It is inspired by the Zinc family of languages and systems and supports user-defined constraints and functions. Secondly, the MiningZinc execution mechanism specifies how to compute solutions for the models. It is solver independent and supports both standard constraint solvers and specialized data mining systems. The high-level problem specification is first translated into a normalized constraint language (FlatZinc). Rewrite rules are then used to add redundant constraints or solve subproblems using specialized data mining algorithms or generic constraint programming solvers. Given a model, different execution strategies are automatically extracted that correspond to different sequences of algorithms to run. Optimized data mining algorithms, specialized processing routines and generic solvers can all be automatically combined. Thus, the MiningZinc language allows one to model constraint-based itemset mining problems in a solver independent way, and its execution mechanism can automatically chain different algorithms and solvers. This leads to a unique combination of declarative modeling with high-performance solving.},
  file = {/Users/rwb/Zotero/storage/V46IXDFY/S0004370215001435.html},
  journal = {Artificial Intelligence},
  keywords = {Constraint programming,Constraint-based mining,Declarative modeling,Itemset mining,Pattern mining},
  series = {Combining {{Constraint Solving}} with {{Mining}} and {{Learning}}}
}

@article{gupta2016,
  title = {Evaluating and {{Improving}} the {{Performance}} and {{Scheduling}} of {{HPC Applications}} in {{Cloud}}},
  author = {Gupta, A. and Faraboschi, P. and Gioachin, F. and Kale, L. V. and Kaufmann, R. and Lee, B. and March, V. and Milojicic, D. and Suen, C. H.},
  year = {2016},
  month = jul,
  volume = {4},
  pages = {307--321},
  issn = {2168-7161},
  doi = {10.1109/TCC.2014.2339858},
  abstract = {Cloud computing is emerging as a promising alternative to supercomputers for some high-performance computing (HPC) applications. With cloud as an additional deployment option, HPC users and providers are faced with the challenges of dealing with highly heterogeneous resources, where the variability spans across a wide range of processor configurations, interconnects, virtualization environments, and pricing models. In this paper, we take a holistic viewpoint to answer the question-why and whoshould choose cloud for HPC, for what applications, and how should cloud be used for HPC? To this end, we perform comprehensive performance and cost evaluation and analysis of running a set of HPC applications on a range of platforms, varying from supercomputers to clouds. Further, we improve performance of HPC applications in cloud by optimizing HPC applications' characteristics for cloud and cloud virtualization mechanisms for HPC. Finally, we present novel heuristics for online application-aware job scheduling in multi-platform environments. Experimental results and simulations using CloudSim show that current clouds cannot substitute supercomputers but can effectively complement them. Significant improvement in average turnaround time (up to 2X)and throughput (up to 6X) can be attained using our intelligent application-aware dynamic scheduling heuristics compared tosingle-platform or application-agnostic scheduling.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/gupta_et_al_2016_evaluating_and_improving_the_performance_and_scheduling_of_hpc_applications_in.pdf;/Users/rwb/Zotero/storage/V2YTHFEU/6858018.html},
  journal = {IEEE Transactions on Cloud Computing},
  keywords = {application-awareness,Bandwidth,Benchmark testing,characterization,cloud,cloud computing,Cloud computing,cloud virtualization mechanisms,Clouds,CloudSim,economics,heterogeneous resources,high-performance computing applications,HPC,HPC application scheduling,intelligent application-aware dynamic scheduling,Jacobian matrices,job scheduling,multiplatform environments,online application-aware job scheduling,parallel machines,performance analysis,performance evaluation,performance improvement,processor configurations,software performance evaluation,supercomputers,Supercomputers,virtualisation,Virtualization,virtualization environments},
  number = {3}
}

@article{gupta2016a,
  title = {Multifactorial {{Evolution}}: {{Toward Evolutionary Multitasking}}},
  shorttitle = {Multifactorial {{Evolution}}},
  author = {Gupta, A. and Ong, Y. and Feng, L.},
  year = {2016},
  month = jun,
  volume = {20},
  pages = {343--357},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2015.2458037},
  abstract = {The design of evolutionary algorithms has typically been focused on efficiently solving a single optimization problem at a time. Despite the implicit parallelism of population-based search, no attempt has yet been made to multitask, i.e., to solve multiple optimization problems simultaneously using a single population of evolving individuals. Accordingly, this paper introduces evolutionary multitasking as a new paradigm in the field of optimization and evolutionary computation. We first formalize the concept of evolutionary multitasking and then propose an algorithm to handle such problems. The methodology is inspired by biocultural models of multifactorial inheritance, which explain the transmission of complex developmental traits to offspring through the interactions of genetic and cultural factors. Furthermore, we develop a cross-domain optimization platform that allows one to solve diverse problems concurrently. The numerical experiments reveal several potential advantages of implicit genetic transfer in a multitasking environment. Most notably, we discover that the creation and transfer of refined genetic material can often lead to accelerated convergence for a variety of complex optimization functions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/gupta_et_al_2016_multifactorial_evolution.pdf;/Users/rwb/Zotero/storage/FBMNPWPD/7161358.html},
  journal = {IEEE Transactions on Evolutionary Computation},
  keywords = {_tablet,biocultural models,complex developmental trait transmission,complex optimization functions,Continuous optimization,Continuous Optimization,cross-domain optimization platform,Cultural differences,cultural factors,discrete optimization,Discrete Optimization,evolutionary algorithms,Evolutionary computation,evolutionary multitasking,Evolutionary Multitasking,genetic algorithms,genetic factors,Genetics,implicit genetic transfer,memetic computation,Memetic Computation,multifactorial evolution,multifactorial inheritance,Multitasking,Optimization,refined genetic material creation,refined genetic material transfer,single optimization problem,Sociology,Statistics},
  number = {3}
}

@article{hadka2013,
  title = {Borg: {{An Auto}}-{{Adaptive Many}}-{{Objective Evolutionary Computing Framework}}},
  shorttitle = {Borg},
  author = {Hadka, D. and Reed, P.},
  year = {2013},
  month = may,
  volume = {21},
  pages = {231--259},
  issn = {1063-6560},
  doi = {10.1162/EVCO_a_00075},
  abstract = {This study introduces the Borg multi-objective evolutionary algorithm (MOEA) for many-objective, multimodal optimization. The Borg MOEA combines {$<$}inline-formula{$><$}inline-graphic mimetype="image" xlink:href="evco\_a\_00075inline1.gif" xlink:type="simple"/{$><$}/inline-formula{$>$}-dominance, a measure of convergence speed named {$<$}inline-formula{$><$}inline-graphic mimetype="image" xlink:href="evco\_a\_00075inline2.gif" xlink:type="simple"/{$><$}/inline-formula{$>$}-progress, randomized restarts, and auto-adaptive multioperator recombination into a unified optimization framework. A comparative study on 33 instances of 18 test problems from the DTLZ, WFG, and CEC 2009 test suites demonstrates Borg meets or exceeds six state of the art MOEAs on the majority of the tested problems. The performance for each test problem is evaluated using a 1,000 point Latin hypercube sampling of each algorithm's feasible parameteri- zation space. The statistical performance of every sampled MOEA parameterization is evaluated using 50 replicate random seed trials. The Borg MOEA is not a single algorithm; instead it represents a class of algorithms whose operators are adaptively selected based on the problem. The adaptive discovery of key operators is of particular importance for benchmarking how variation operators enhance search for complex many-objective problems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/hadka_reed_2013_borg.pdf;/Users/rwb/Zotero/storage/ZGXPKCJB/6793867.html},
  journal = {Evolutionary Computation},
  keywords = {-dominance,Evolutionary algorithm,many-objective optimization,multi-objective optimization,multimodal problems},
  number = {2}
}

@inproceedings{hafsi2019,
  title = {Towards a Novel {{NSGA}}-{{II}}-Based Approach for Multi-Objective Scientific Workflow Scheduling on Hybrid Clouds},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Hafsi, Haithem and Gharsellaoui, Hamza and Bouamama, Sadok},
  year = {2019},
  month = jul,
  pages = {203--204},
  publisher = {{Association for Computing Machinery}},
  address = {{Prague, Czech Republic}},
  doi = {10.1145/3319619.3321975},
  abstract = {In the era of the big data and e-science revolution, the execution of such applications known as High Performance Computing (HPC) is becoming a challenging issue. In order to face these challenges, a new promising Large Scale Distributed Systems (LSDS) has emerged suchlike Grid and Cloud Computing. As a matter of fact, these HPC applications are commonly arranged as a form of interdependent tasks named workflows. Nevertheless, the new challenging topic is that the scheduling of these scientific workflows in the LSDS is a well-known NP-hard problem. The goal of this work is to design an Non-dominated Sorting Genetic Algorithm Version II (NSGA-II)-based approach for optimizing a multi objective scheduling of scientific workflows in hybrid distributed systems. This paper work deals with the proposition of two execution models: i) A Cumulative one aiming to improve the Pareto front quality in term of Makespan-Cost trade-off; ii) An Incremental execution fashion, what kind of Cost-driven approach leading to a solution diversity of the Pareto front in the objective space. Experiments conducted with multiple common scientific workflows point out significant improvement against the classic NSGA-II algorithm.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/hafsi_et_al_2019_towards_a_novel_nsga-ii-based_approach_for_multi-objective_scientific_workflow.pdf},
  isbn = {978-1-4503-6748-6},
  keywords = {hybrid clouds,multi-objective optimization,NSGA-II,workflow scheduling},
  series = {{{GECCO}} '19}
}

@article{harrington,
  title = {Diagnosing {{Parallel I}}/{{O Bottlenecks}} in {{HPC Applications}}},
  author = {Harrington, Peter},
  pages = {4},
  abstract = {High-Performance Computing (HPC) applications are generating increasingly large volumes of data (up to tens of PBs), which need to be stored in parallel to be scalable. Parallel I/O is a significant bottleneck in HPC applications, and is especially challenging in Adaptive Mesh Refinement (AMR) applications because the structure of output files changes dynamically during runtime. Data-intensive AMR applications run on the Cori supercomputer at NERSC show variable and often poor I/O performance, but diagnosing the root cause remains challenging. Here we analyze logs from multiple levels of Cori's parallel I/O subsystems, and find bottlenecks during file metadata operations and during the writing of file contents that reduced I/O bandwidth by up to 40x. Such bottlenecks seemed to be system-dependent and not the fault of the application. Increasing the granularity of file-system performance data will help provide conclusive causal relationships between file-system servers and metadata bottlenecks.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/harrington_diagnosing_parallel_i-o_bottlenecks_in_hpc_applications.pdf},
  language = {en}
}

@article{hasham2011,
  title = {{{CMS Workflow Execution Using Intelligent Job Scheduling}} and {{Data Access Strategies}}},
  author = {Hasham, Khawar and Delgado Peris, Antonio and Anjum, Ashiq and Evans, Dave and Gowdy, Stephen and Hernandez, Jos{\'e} M. and Huedo, Eduardo and Hufnagel, Dirk and {van Lingen}, Frank and McClatchey, Richard and Metson, Simon},
  year = {2011},
  month = jun,
  volume = {58},
  pages = {1221--1232},
  issn = {0018-9499, 1558-1578},
  doi = {10.1109/TNS.2011.2146276},
  abstract = {Complex scientific workflows can process large amounts of data using thousands of tasks. The turnaround times of these workflows are often affected by various latencies such as the resource discovery, scheduling and data access latencies for the individual workflow processes or actors. Minimizing these latencies will improve the overall execution time of a workflow and thus lead to a more efficient and robust processing environment. In this paper, we propose a pilot job concept that has intelligent data reuse and job execution strategies to minimize the scheduling, queuing, execution and data access latencies. The results have shown that significant improvements in the overall turnaround time of a workflow can be achieved with this approach. The proposed approach has been evaluated, first using the CMS Tier0 data processing workflow, and then simulating the workflows to evaluate its effectiveness in a controlled environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/hasham_et_al_2011_cms_workflow_execution_using_intelligent_job_scheduling_and_data_access.pdf},
  journal = {IEEE Transactions on Nuclear Science},
  language = {en},
  number = {3}
}

@misc{hemsoth2016,
  title = {Measuring {{Top Supercomputer Performance}} in the {{Real World}}},
  author = {Hemsoth, Nicole},
  year = {2016},
  month = jun,
  abstract = {When we cover the bi-annual listing of the world's most powerful supercomputers, the metric at the heart of those results, the high performance Linpack},
  file = {/Users/rwb/Zotero/storage/893YMWEU/measuring-top-supercomputer-performance-real-world.html},
  howpublished = {https://www.nextplatform.com/2016/06/21/measuring-top-supercomputer-performance-real-world/},
  language = {en-US}
}

@article{herroelen1998,
  title = {Resource-Constrained Project Scheduling: {{A}} Survey of Recent Developments},
  shorttitle = {Resource-Constrained Project Scheduling},
  author = {Herroelen, Willy and De Reyck, Bert and Demeulemeester, Erik},
  year = {1998},
  month = apr,
  volume = {25},
  pages = {279--302},
  issn = {0305-0548},
  doi = {10.1016/S0305-0548(97)00055-5},
  abstract = {We review recent advances in dealing with the resource-constrained project scheduling problem using an efficient depth-first branch-and-bound procedure, elaborating on the branching scheme, bounding calculations and dominance rules, and discuss the potential of using truncated branch-and-bound. We derive conclusions from the research on optimal solution procedures for the basic problem and subsequently illustrate extensions to a rich and realistic variety of related problems involving activity preemption, the use of ready times and deadlines, variable resource requirements and availabilities, generalized precedence relations, time/cost, time/resource and resource/resource trade-offs and non-regular objective functions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1998/herroelen_et_al_1998_resource-constrained_project_scheduling.pdf;/Users/rwb/Zotero/storage/JL595CFC/S0305054897000555.html},
  journal = {Computers \& Operations Research},
  number = {4}
}

@article{hirales-carbajal2012,
  title = {Multiple {{Workflow Scheduling Strategies}} with {{User Run Time Estimates}} on a {{Grid}}},
  author = {{Hirales-Carbajal}, Ad{\'a}n and Tchernykh, Andrei and Yahyapour, Ramin and {Gonz{\'a}lez-Garc{\'i}a}, Jos{\'e} Luis and R{\"o}blitz, Thomas and {Ram{\'i}rez-Alcaraz}, Juan Manuel},
  year = {2012},
  month = jun,
  volume = {10},
  pages = {325--346},
  issn = {1570-7873, 1572-9184},
  doi = {10.1007/s10723-012-9215-6},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/hirales-carbajal_et_al_2012_multiple_workflow_scheduling_strategies_with_user_run_time_estimates_on_a_grid.pdf},
  journal = {Journal of Grid Computing},
  language = {en},
  number = {2}
}

@article{hooker2018,
  title = {Constraint Programming and Operations Research},
  author = {Hooker, J. N. and {van Hoeve}, W.-J.},
  year = {2018},
  month = apr,
  volume = {23},
  pages = {172--195},
  issn = {1383-7133, 1572-9354},
  doi = {10.1007/s10601-017-9280-3},
  abstract = {We present an overview of the integration of constraint programming (CP) and operations research (OR) to solve combinatorial optimization problems. We interpret CP and OR as relying on a common primal-dual solution approach that provides the basis for integration using four main strategies. The first strategy tightly interweaves propagation from CP and relaxation from OR in a single solver. The second applies OR techniques to domain filtering in CP. The third decomposes the problem into a portion solved by CP and a portion solved by OR, using CP-based column generation or logic-based Benders decomposition. The fourth uses relaxed decision diagrams developed for CP propagation to help solve dynamic programming models in OR. The paper cites a significant fraction of the literature on CP/OR integration and concludes with future perspectives.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/hooker_van_hoeve_2018_constraint_programming_and_operations_research.pdf},
  journal = {Constraints},
  language = {en},
  number = {2}
}

@inproceedings{hsieh2017,
  title = {Throughput-{{Optimal Scheduling}} for {{Multi}}-{{Hop Networked Transportation Systems With Switch}}-{{Over Delay}}},
  booktitle = {Proceedings of the 18th {{ACM International Symposium}} on {{Mobile Ad Hoc Networking}} and {{Computing}}},
  author = {Hsieh, Ping-Chun and Liu, Xi and Jiao, Jian and Hou, I-Hong and Zhang, Yunlong and Kumar, P. R.},
  year = {2017},
  pages = {16:1--16:10},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3084041.3084065},
  abstract = {The emerging connected-vehicle technology provides a new dimension for developing more intelligent traffic control algorithms for signalized intersections. An important challenge for scheduling in networked transportation systems is the switchover delay caused by the guard time before any traffic signal change. The switch-over delay can result in significant loss of system capacity and hence needs to be accommodated in the scheduling design. To tackle this challenge, we propose a distributed online scheduling policy that extends the well-known Max-Pressure policy to address switch-over delay by introducing a bias factor favoring the current schedule. We prove that the proposed policy is throughput-optimal with switch-over delay. Furthermore, the proposed policy remains optimal when there are both connected signalized intersections and conventional fixed-time ones in the system. With connected-vehicle technology, the proposed policy can be easily incorporated into the current transportation systems without additional infrastructure. Through extensive simulation in VISSIM, we show that our policy indeed outperforms the existing popular policies.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/hsieh_et_al_2017_throughput-optimal_scheduling_for_multi-hop_networked_transportation_systems.pdf},
  isbn = {978-1-4503-4912-3},
  keywords = {Networked transportation system,Scheduling,Switchover delay,Throughput-optimality},
  series = {Mobihoc '17}
}

@inproceedings{hu2009,
  title = {Resource {{Provisioning}} for {{Cloud Computing}}},
  booktitle = {Proceedings of the 2009 {{Conference}} of the {{Center}} for {{Advanced Studies}} on {{Collaborative Research}}},
  author = {Hu, Ye and Wong, Johnny and Iszlai, Gabriel and Litoiu, Marin},
  year = {2009},
  pages = {101--111},
  publisher = {{IBM Corp.}},
  address = {{Riverton, NJ, USA}},
  doi = {10.1145/1723028.1723041},
  abstract = {In resource provisioning for cloud computing, an important issue is how resources may be allocated to an application mix such that the service level agreements (SLAs) of all applications are met. A performance model with two interactive job classes is used to determine the smallest number of servers required to meet the SLAs of both classes. For each class, the SLA is specified by the relationship: Prob [response time {$\leq$} x] {$\geq$} y. Two server allocation strategies are considered: shared allocation (SA) and dedicated allocation (DA). For the case of FCFS scheduling, analytic results for response time distribution are used to develop a heuristic algorithm that determines an allocation strategy (SA or DA) that requires the smallest number of servers. The effectiveness of this algorithm is evaluated over a range of operating conditions. The performance of SA with non-FCFS scheduling is also investigated. Among the scheduling disciplines considered, a new discipline called probability dependent priority is found to have the best performance in terms of requiring the smallest number of servers.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2009/hu_et_al_2009_resource_provisioning_for_cloud_computing.pdf},
  series = {{{CASCON}} '09}
}

@article{huang2016,
  title = {Multi-Objective Flexible Job-Shop Scheduling Problem Using Modified Discrete Particle Swarm Optimization},
  author = {Huang, Song and Tian, Na and Wang, Yan and Ji, Zhicheng},
  year = {2016},
  month = aug,
  volume = {5},
  pages = {1432},
  issn = {2193-1801},
  doi = {10.1186/s40064-016-3054-z},
  abstract = {Taking resource allocation into account, flexible job shop problem (FJSP) is a class of complex scheduling problem in manufacturing system. In order to utilize the machine resources rationally, multi-objective particle swarm optimization (MOPSO) integrating with variable neighborhood search is introduced to address FJSP efficiently. Firstly, the assignment rules (AL) and dispatching rules (DR) are provided to initialize the population. And then special discrete operators are designed to produce new individuals and earliest completion machine (ECM) is adopted in the disturbance operator to escape the optima. Secondly, personal-best archives (cognitive memories) and global-best archive (social memory), which are updated by the predefined non-dominated archive update strategy, are simultaneously designed to preserve non-dominated individuals and select personal-best positions and the global-best position. Finally, three neighborhoods are provided to search the neighborhoods of global-best archive for enhancing local search ability. The proposed algorithm is evaluated by using Kacem instances and Brdata instances, and a comparison with other approaches shows the effectiveness of the proposed algorithm for FJSP.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/huang_et_al_2016_multi-objective_flexible_job-shop_scheduling_problem_using_modified_discrete.pdf;/Users/rwb/Zotero/storage/SVQNA7RP/s40064-016-3054-z.html},
  journal = {SpringerPlus},
  number = {1}
}

@article{hwang2008,
  title = {A Comparison of Multiprocessor Task Scheduling Algorithms with Communication Costs},
  author = {Hwang, Reakook and Gen, Mitsuo and Katayama, Hiroshi},
  year = {2008},
  volume = {35},
  pages = {976--993},
  issn = {0305-0548},
  doi = {10.1016/j.cor.2006.05.013},
  journal = {Computers \& Operations Research},
  number = {3}
}

@article{iacob2019,
  title = {Exploring the Gap between the Student Expectations and the Reality of Teamwork in Undergraduate Software Engineering Group Projects},
  author = {Iacob, Claudia and Faily, Shamal},
  year = {2019},
  month = nov,
  volume = {157},
  pages = {110393},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2019.110393},
  abstract = {Software engineering group projects aim to provide a nurturing environment for learning about teamwork in software engineering. Since social and teamwork issues have been consistently identified as serious problems in such projects, we aim to better understand the breakdown between the expectations teams have at the start of a group project and their experiences at the end of the project. In this paper, we investigate how 35 teams of undergraduate students approach software engineering group project courses, and how their previous experience with collaborative software development matches their expectations for group work. We then analyse the retrospective documents delivered by the same teams at the end of a 27-week software engineering group project course, mirroring the expectations at the start of the project with the realities described by the end of it.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/iacob_faily_2019_exploring_the_gap_between_the_student_expectations_and_the_reality_of_teamwork.pdf;/Users/rwb/Zotero/storage/BG4IHYHZ/S0164121219301682.html},
  journal = {Journal of Systems and Software},
  keywords = {Engineering education,Group project,Project,Software engineering},
  language = {en}
}

@article{ibarra1977,
  title = {Heuristic {{Algorithms}} for {{Scheduling Independent Tasks}} on {{Nonidentical Processors}}},
  author = {Ibarra, Oscar H. and Kim, Chul E.},
  year = {1977},
  month = apr,
  volume = {24},
  pages = {280--289},
  issn = {0004-5411},
  doi = {10.1145/322003.322011},
  abstract = {The finishing time properties of several heuristic algorithms for scheduling n independent tasks on m nonidentical processors are studied. In particular, for m = 2 an n log n time-bounded algorithm is given which generates a schedule having a finishing time of at most ({$\surd$}5 + 1)/2 of the optimal finishing time. A simplified scheduling problem involving identical processors and restricted task sets is shown to be P-complete. However, the LPT algorithm applied to this problem yields schedules which are near optimal for large n.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1977/ibarra_kim_1977_heuristic_algorithms_for_scheduling_independent_tasks_on_nonidentical_processors.pdf},
  journal = {J. ACM},
  number = {2}
}

@article{idris2017,
  title = {An Improved Ant Colony Optimization Algorithm with Fault Tolerance for Job Scheduling in Grid Computing Systems},
  author = {Idris, Hajara and Ezugwu, Absalom E. and Junaidu, Sahalu B. and Adewumi, Aderemi O.},
  year = {2017},
  month = may,
  volume = {12},
  pages = {e0177567},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0177567},
  abstract = {The Grid scheduler, schedules user jobs on the best available resource in terms of resource characteristics by optimizing job execution time. Resource failure in Grid is no longer an exception but a regular occurring event as resources are increasingly being used by the scientific community to solve computationally intensive problems which typically run for days or even months. It is therefore absolutely essential that these long-running applications are able to tolerate failures and avoid re-computations from scratch after resource failure has occurred, to satisfy the user's Quality of Service (QoS) requirement. Job Scheduling with Fault Tolerance in Grid Computing using Ant Colony Optimization is proposed to ensure that jobs are executed successfully even when resource failure has occurred. The technique employed in this paper, is the use of resource failure rate, as well as checkpoint-based roll back recovery strategy. Check-pointing aims at reducing the amount of work that is lost upon failure of the system by immediately saving the state of the system. A comparison of the proposed approach with an existing Ant Colony Optimization (ACO) algorithm is discussed. The experimental results of the implemented Fault Tolerance scheduling algorithm show that there is an improvement in the user's QoS requirement over the existing ACO algorithm, which has no fault tolerance integrated in it. The performance evaluation of the two algorithms was measured in terms of the three main scheduling performance metrics: makespan, throughput and average turnaround time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/idris_et_al_2017_an_improved_ant_colony_optimization_algorithm_with_fault_tolerance_for_job.pdf;/Users/rwb/Zotero/storage/KBZKZF8T/article.html},
  journal = {PLOS ONE},
  keywords = {Algorithms,Bandwidth (computing),Damage mechanics,Electrical faults,Fault tolerance,Jobs,Mutation databases,Optimization},
  language = {en},
  number = {5}
}

@article{iverson1999,
  title = {Hierarchical, Competitive Scheduling of Multiple {{DAGs}} in a Dynamic Heterogeneous Environment},
  author = {Iverson, M A and {\"O}zg{\"u}ner, F},
  year = {1999},
  month = sep,
  volume = {6},
  pages = {112--120},
  issn = {0967-1846, 1361-6390},
  doi = {10.1088/0967-1846/6/3/303},
  abstract = {With the advent of large-scale heterogeneous environments, there is a need for matching and scheduling algorithms which can allow multiple, directed acyclic graph structured applications to share the computational resources of the network. This paper presents a hierarchical matching and scheduling framework where multiple applications compete for the computational resources on the network. In this environment, each application makes its own scheduling decisions. Thus, no centralized scheduling resource is required. Applications do not need direct knowledge of the other applications\textemdash knowledge of other applications arrives indirectly through load estimates (like queue lengths). This paper presents an algorithm, called the dynamic hierarchical scheduling algorithm, which schedules tasks within this framework. A series of simulations are presented to examine the performance of these algorithms in this environment, compared with a more conventional, single-user environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1999/iverson_zgner_1999_hierarchical,_competitive_scheduling_of_multiple_dags_in_a_dynamic.pdf},
  journal = {Distributed Systems Engineering},
  language = {en},
  number = {3}
}

@inproceedings{j.2019,
  title = {Pyjanitor: {{A Cleaner API}} for {{Cleaning Data}}},
  shorttitle = {Pyjanitor},
  booktitle = {Python in {{Science Conference}}},
  author = {J., Eric and Barry, Zachary and Zuckerman, Sam and Sailer, Zachary},
  year = {2019},
  pages = {50--53},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-7ddc1dd1-007},
  abstract = {The pandas library has become the de facto library for data wrangling in the Python programming language. However, inconsistencies in the pandas application programming interface (API), while idiomatic due to historical use, prevent use of expressive, fluent programming idioms that enable self-documenting pandas code. Here, we introduce pyjanitor, an open source Python package that extends the pandas API with such idioms. We describe its design and implementation of the package, provide usage examples from a variety of domains, and discuss the ways that the pyjanitor project has enabled the inclusion of first-time contributors to open source projects.},
  file = {/Users/rwb/Zotero/storage/TS7TYSSP/J. et al. - 2019 - pyjanitor A Cleaner API for Cleaning Data.pdf},
  language = {en}
}

@article{jackson2015,
  title = {Survey on {{Programming Models}} and {{Environments}} for {{Cluster}}, {{Cloud}}, and {{Grid Computing}} That {{Defends Big Data}}},
  author = {Jackson, J. Christy and Vijayakumar, V. and Quadir, Md. Abdul and Bharathi, C.},
  year = {2015},
  month = jan,
  volume = {50},
  pages = {517--523},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.04.025},
  abstract = {A collection of interlinked stand-alone computers which function together in unity as a single incorporated computing resource is a kind of parallel or distributed processing systems called as the cluster. Clusters and grids are systems which intercommunicate among them and act as a single resource. This type of functionality can be referred to as the multi-computer parallel architecture that runs on certain considerations. Nevertheless cloud computing came forth as a more illustrious programming model to handle large data sets using clusters. A programming model is nothing but how data is carried out for handling the application. Performances, portability, objective architecture, sustainment of code are key measures that have to be commemorated while designing a programming model. Applications which are meant for data analytics generally deal with large data sets that undergo several stages of processing. Some of these stages are performed consecutively, and the others are carried out in parallel on clusters, grids, and cloud. This paper portrays a survey on how programming models which are developed for cluster cloud and grid act as a support for big data analytics. In addition, study on programming models which are currently being employed by leading multinational companies are pictured in the paper.},
  file = {/Users/rwb/Zotero/storage/R6U755YX/S1877050915005268.html},
  journal = {Procedia Computer Science},
  series = {Big {{Data}}, {{Cloud}} and {{Computing Challenges}}}
}

@inproceedings{jain2017,
  title = {A Systematic Analysis of Nature Inspired Workflow Scheduling Algorithm in Heterogeneous Cloud Environment},
  booktitle = {2017 {{International Conference}} on {{Intelligent Communication}} and {{Computational Techniques}} ({{ICCT}})},
  author = {Jain, R. and Sharma, N. and Jain, P.},
  year = {2017},
  month = dec,
  pages = {242--247},
  doi = {10.1109/IN\%0021CCT.2017.8324053},
  abstract = {Cloud computing is the new form of distributed system to store and compute the large amount of data and solve complex computational problems. The size of data and computational complexity is increasing day by day. It leads us to find out an efficient solution to scheduled and dispose them properly in large cloud computing environment. The major demand to analyze large scientific issues is Workflow. In workflow scheduling, the problem is divided into the smaller dependent tasks, solve individually in either sequential or parallel mode and finally combined to leads the desired output. Workflow scheduling is NPC (NP Complete) problem, which attracts researchers to explore the solution. In this paper, we propose a extensive and systematic analysis of nature inspired workflow scheduling algorithms in heterogeneous cloud environment. Moreover, it gives a classification of nature inspired algorithms and highlights their aim, properties and their limitations. Finally, we desire to furnish a comprehensive and complete perceptive of current nature inspired literature and assist researchers by equipping them an observation into future trends and unsolved issues.},
  file = {/Users/rwb/Dropbox/PhD/zotero/017/jain_et_al_2017_a_systematic_analysis_of_nature_inspired_workflow_scheduling_algorithm_in.pdf;/Users/rwb/Zotero/storage/5XNWVFHJ/8324053.html},
  keywords = {Ant Colony Optimization (ACO),Artificial Bee Colony Optimization (ABC),cloud computing,Cloud computing,Cloud Computing,cloud computing environment,computational complexity,distributed system storage,Genetic Algorithm (GA),Genetic algorithms,heterogeneous cloud environment,Meta-heuristics,nature inspired workflow scheduling algorithm,NP complete problem,NPC problem,Particle Swarm Optimization (PSO),Processor scheduling,review,Schedules,scheduling,Scheduling,Sociology,survey,Task analysis,Task Scheduling}
}

@inproceedings{jaybhaye2017,
  ids = {jaybhaye2017a},
  title = {A Review on Scientific Workflow Scheduling in Cloud Computing},
  booktitle = {2017 2nd {{International Conference}} on {{Communication}} and {{Electronics Systems}} ({{ICCES}})},
  author = {Jaybhaye, S. M. and Attar, V. Z.},
  year = {2017},
  month = oct,
  pages = {218--223},
  doi = {10.1109/CESYS.2017.8321269},
  abstract = {Cloud Computing is a style for flexible and scalable application development. It delivers everything as a service to the customer through the internet. Rapid evaluation of cloud computing is very useful for large workflow based applications which requires large computing power and storage. Workflow scheduling of resources in cloud computing is a NP hard problem. Different optimization techniques such as deterministic, stochastic and dynamic programming, simple heuristics with thresholds can be applied. In this paper different types of workflow based applications, parameters and algorithms are studied. Workflow scheduling Algorithm with its objective parameters are compared and discussed. These approaches are used by cloud service providers in their services to maximize profit as well as to use resources efficiently.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/jaybhaye_attar_2017_a_review_on_scientific_workflow_scheduling_in_cloud_computing.pdf;/Users/rwb/Zotero/storage/6AYB7UVV/8321269.html;/Users/rwb/Zotero/storage/Y9AQRRA4/8321269.html},
  keywords = {cloud computing,Cloud computing,cloud service providers,computational complexity,Dynamic scheduling,flexible application development,Optimal scheduling,optimisation,Processor scheduling,Resource Optimization,scalable application development,scheduling,scientific information systems,scientific workflow scheduling,Task analysis,Virtual machining,workflow based applications,Workflow based applications}
}

@inproceedings{jiang2007,
  title = {Workflow {{Management}} in {{Grid Era}}: {{From Process}}-{{Driven Paradigm}} to a {{Goal}}-{{Driven One}}},
  shorttitle = {Workflow {{Management}} in {{Grid Era}}},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}} 2007: {{OTM}} 2007 {{Workshops}}},
  author = {Jiang, Jinlei and Zhang, Shaohua and Schlichter, Johann and Yang, Guangwen},
  editor = {Meersman, Robert and Tari, Zahir and Herrero, Pilar},
  year = {2007},
  pages = {169--178},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {As workflow technology evolves into the grid domain, workflow process becomes more and more complex, raising a great challenge both to workflow modeling and workflow execution. In response to the challenge, this paper puts forward a goal-driven workflow framework. The framework first proposes process pattern as an effective way for procedural knowledge representation and then deploys a pattern-based planning algorithm to (semi-) automatically generate workflow on the fly according to the goal specified by users and the running context. Such a goal-driven workflow management paradigm could not only enhance the flexibility and adaptability of workflow system but also ease the heavy burden of workflow definition.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/jiang_et_al_2007_workflow_management_in_grid_era.pdf},
  isbn = {978-3-540-76888-3},
  keywords = {Knowledge Management,Knowledge Representation,Planning,Process Pattern,Workflow Generation},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{jiang2014,
  title = {A {{DAG Scheduling Scheme}} on {{Heterogeneous Computing Systems Using Tuple}}-{{Based Chemical Reaction Optimization}}},
  author = {Jiang, Yuyi and Shao, Zhiqing and Guo, Yi},
  year = {2014},
  volume = {2014},
  issn = {2356-6140},
  doi = {10.1155/2014/404375},
  abstract = {A complex computing problem can be solved efficiently on a system with multiple computing nodes by dividing its implementation code into several parallel processing modules or tasks that can be formulated as directed acyclic graph (DAG) problems. The DAG jobs may be mapped to and scheduled on the computing nodes to minimize the total execution time. Searching an optimal DAG scheduling solution is considered to be NP-complete. This paper proposed a tuple molecular structure-based chemical reaction optimization (TMSCRO) method for DAG scheduling on heterogeneous computing systems, based on a very recently proposed metaheuristic method, chemical reaction optimization (CRO). Comparing with other CRO-based algorithms for DAG scheduling, the design of tuple reaction molecular structure and four elementary reaction operators of TMSCRO is more reasonable. TMSCRO also applies the concept of constrained critical paths (CCPs), constrained-critical-path directed acyclic graph (CCPDAG) and super molecule for accelerating convergence. In this paper, we have also conducted simulation experiments to verify the effectiveness and efficiency of TMSCRO upon a large set of randomly generated graphs and the graphs for real world problems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/jiang_et_al_2014_a_dag_scheduling_scheme_on_heterogeneous_computing_systems_using_tuple-based.pdf},
  journal = {The Scientific World Journal},
  pmcid = {PMC4095736},
  pmid = {25143977}
}

@inproceedings{jiang2014a,
  title = {Optimizing {{Scientific Workflows}} in the {{Cloud}}: {{A Montage Example}}},
  shorttitle = {Optimizing {{Scientific Workflows}} in the {{Cloud}}},
  booktitle = {2014 {{IEEE}}/{{ACM}} 7th {{International Conference}} on {{Utility}} and {{Cloud Computing}}},
  author = {Jiang, Q. and Lee, Y. C. and Arenaz, M. and Leslie, L. M. and Zomaya, A. Y.},
  year = {2014},
  month = dec,
  pages = {517--522},
  doi = {10.1109/UCC.2014.77},
  abstract = {As scientific workflows are increasingly deployed in clouds, a myriad of studies have been conducted-including the development of workflow execution systems and scheduling/resource-management algorithms-for optimizing the execution of these workflows. However, the efficacy of most, if not all, of these previous works is limited by the original design and structure of workflow, i.e., Sequential code and few bottleneck tasks. In this paper, we address the optimization of scientific workflow execution in clouds by exploiting multi-core systems with the parallelization of bottleneck tasks. To this end, we develop a workflow visualization toolkit to synthetize resource consumption and data transfer patterns, as well as to identify the bottleneck, of the workflow being studied. Parallelization techniques are then applied to the module that is identified as the bottleneck in order to take full advantage of the underlying multicore computing environment. Testing results with a 6.0-degree Montage example on Amazon EC2 with various configurations show that our optimization of workflows (bottleneck tasks in particular) reduces completion time (or make span) by 21\% to 43\% depending on the instance type being used to run the workflow, without any impact on the cost.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/jiang_et_al_2014_optimizing_scientific_workflows_in_the_cloud.pdf;/Users/rwb/Zotero/storage/GV4M95FK/7027544.html},
  keywords = {Amazon EC2,cloud computing,completion time reduction,Data transfer,data transfer pattern synthesis,Data visualization,Educational institutions,Electronic mail,makespan reduction,Montage,multicore computing environment,multicore systems,optimization,Optimization,parallel processing,parallelization,Processor scheduling,resource consumption synthesis,resource-management algorithms,scheduling algorithms,scientific information systems,scientific workflow execution optimization,sequential code,task parallelization,Vectors,visualization,workflow,workflow execution systems,workflow visualization toolkit}
}

@misc{jiang2015,
  title = {{{DRSCRO}}: {{A Metaheuristic Algorithm}} for {{Task Scheduling}} on {{Heterogeneous Systems}}},
  shorttitle = {{{DRSCRO}}},
  author = {Jiang, Yuyi and Shao, Zhiqing and Guo, Yi and Zhang, Huanhuan and Niu, Kun},
  year = {2015},
  volume = {2015},
  pages = {e396582},
  publisher = {{Hindawi}},
  issn = {1024-123X},
  doi = {10.1155/2015/396582},
  abstract = {An efficient DAG task scheduling is crucial for leveraging the performance potential of a heterogeneous system and finding a schedule that minimizes the makespan (i.e., the total execution time) of a DAG is known to be NP-complete. A recently proposed metaheuristic method, Chemical Reaction Optimization (CRO), demonstrates its capability for solving NP-complete optimization problems. This paper develops an algorithm named Double-Reaction-Structured Chemical Reaction Optimization (DRSCRO) for DAG scheduling on heterogeneous systems, which modifies the conventional CRO framework and incorporates CRO with the variable neighborhood search (VNS) method. DRSCRO has two reaction phases for super molecule selection and global optimization, respectively. In the molecule selection phase, the CRO as a metaheuristic algorithm is adopted to obtain a super molecule for accelerating convergence. For promoting the intensification capability, in the global optimization phase, the VNS algorithm with a new processor selection model is used as the initialization under the consideration of scheduling order and processor assignment, and the load balance neighborhood structure of VNS is also utilized in the ineffective reaction operator. The experimental results verify the effectiveness and efficiency of DRSCRO in terms of makespan and convergence rate.},
  file = {/Users/rwb/Zotero/storage/8TJAS6JM/Jiang et al. - 2015 - DRSCRO A Metaheuristic Algorithm for Task Schedul.pdf;/Users/rwb/Zotero/storage/NKKSKFRG/396582.html},
  howpublished = {https://www.hindawi.com/journals/mpe/2015/396582/},
  journal = {Mathematical Problems in Engineering},
  language = {en},
  type = {Research {{Article}}}
}

@article{jin2015,
  ids = {jin2015a},
  title = {Fundamental {{Limits}} of {{CDF}}-{{Based Scheduling}}: {{Throughput}}, {{Fairness}}, and {{Feedback Overhead}}},
  shorttitle = {Fundamental {{Limits}} of {{CDF}}-{{Based Scheduling}}},
  author = {Jin, H. and Jung, B. C. and Leung, V. C. M.},
  year = {2015},
  month = jun,
  volume = {23},
  pages = {894--907},
  issn = {1063-6692},
  doi = {10.1109/TNET.2014.2312534},
  abstract = {In this paper, we investigate fundamental performance limits of cumulative distribution function (CDF)-based scheduling (CS) in downlink cellular networks. CS is known as an efficient scheduling method that can assign different time fractions for users or, equivalently, satisfy different channel access ratio (CAR) requirements of users while exploiting multiuser diversity. We first mathematically analyze the throughput characteristics of CS in arbitrary fading statistics and data rate functions. It is shown that the throughput gain of CS increases as the CAR of a user decreases or the number of users in a cell increases. For Nakagami-m fading channels, we obtain the average throughput in closed form and investigate the effects of the average signal-to-noise ratio, the shape parameter m, and the CAR on the throughput performance. In addition, we propose a threshold-based opportunistic feedback technique in order to reduce feedback overhead while satisfying the CAR requirements of users. We prove that the average feedback overhead of the proposed technique is upper-bounded by -lnp, where p is the probability that no user satisfies the threshold condition in a cell. Finally, we adopt a novel fairness criterion, called qualitative fairness, which considers not only the quantity of the allocated resources to users, but also the quality of the resources. It is observed that CS provides a better qualitative fairness than other scheduling algorithms designed for controlling CARs of users.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/jin_et_al_2015_fundamental_limits_of_cdf-based_scheduling.pdf;/Users/rwb/Dropbox/PhD/zotero/IEEE/ACM Transactions on Networking/jin_et_al_2015_fundamental_limits_of_cdf-based_scheduling.pdf;/Users/rwb/Zotero/storage/YI4HJHD3/6784488.html;/Users/rwb/Zotero/storage/ZV522HKS/6784488.html},
  journal = {IEEE/ACM Transactions on Networking},
  keywords = {_tablet,arbitrary fading statistics,CAR requirements,CDF-based scheduling,Cellular networks,cellular radio,channel access ratio requirements,closed form,CS,cumulative distribution function,cumulative distribution function (CDF)-based scheduling,data rate functions,diversity reception,downlink cellular networks,Fading,fairness,fairness criterion,feedback overhead,Gain,Job shop scheduling,multiuser diversity,Nakagami channels,Nakagami-m fading channels,qualitative fairness,Scheduling algorithms,Shape,Signal to noise ratio,signal-to-noise ratio,telecommunication network reliability,threshold-based opportunistic feedback technique,Throughput,throughput characteristics,time fractions,user scheduling},
  number = {3}
}

@inproceedings{jin2018,
  title = {A {{Task Scheduling Strategy}} with a {{Sleep}}-{{Delay Timer}} and a {{Waking}}-{{Up Threshold}} in {{Cloud Computing}}},
  booktitle = {Queueing {{Theory}} and {{Network Applications}}},
  author = {Jin, Shunfu and Wang, Xiushuang and Yue, Wuyi},
  editor = {Takahashi, Yutaka and {Phung-Duc}, Tuan and Wittevrongel, Sabine and Yue, Wuyi},
  year = {2018},
  pages = {115--123},
  publisher = {{Springer International Publishing}},
  abstract = {For the purpose of satisfying the response performance of cloud users while reducing the energy consumption in cloud computing, we propose a task scheduling strategy with a sleep-delay timer and a waking-up threshold. According to the stochastic behavior of tasks with the proposed strategy, we establish a synchronous vacation queueing model with vacation-delay and N-policy. Then we derive the average sojourn time of tasks and the energy conservation level of the system in a steady state. Finally, we provide numerical experiments to investigate the impacts of system parameters on performance criteria.},
  file = {/Users/rwb/Dropbox/PhD/zotero/018/jin_et_al_2018_a_task_scheduling_strategy_with_a_sleep-delay_timer_and_a_waking-up_threshold.pdf},
  isbn = {978-3-319-93736-6},
  keywords = {_tablet,Cloud computing,Energy conservation level,Sleep-delay timer,Sojourn time,Task scheduling strategy,Waking-up threshold},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@misc{johnson,
  title = {The {{NLopt}} Nonlinear-Optimization Package,},
  author = {Johnson, Steven G.}
}

@article{jongerius2018,
  title = {Analytic {{Multi}}-{{Core Processor Model}} for {{Fast Design}}-{{Space Exploration}}},
  author = {Jongerius, R. and Anghel, A. and Dittmann, G. and Mariani, G. and Vermij, E. and Corporaal, H.},
  year = {2018},
  month = jun,
  volume = {67},
  pages = {755--770},
  issn = {0018-9340},
  doi = {10.1109/TC.2017.2780239},
  abstract = {Simulators help computer architects optimize system designs. The limited performance of simulators even of moderate size and detail makes the approach infeasible for design-space exploration of future exascale systems. Analytic models, in contrast, offer very fast turn-around times. In this paper we propose an analytic multi-core processor-performance model that takes as inputs a) a parametric microarchitecture-independent characterization of the target workload, and b) a hardware configuration of the core and the memory hierarchy. The processor-performance model considers instruction-level parallelism (ILP) per type, models single instruction, multiple data (SIMD) features, and considers cache and memory-bandwidth contention between cores. We validate our model by comparing its performance estimates with measurements from hardware performance counters on Intel Xeon and ARM Cortex-A15 systems. We estimate multi-core contention with a maximum error of 11.4 percent. The average single-thread error increases from 25 percent for a state-of-the-art simulator to 59 percent for our model, but the correlation is still 0.8, a high relative accuracy, while we achieve a speedup of several orders of magnitude. With a much higher capacity than simulators and more reliable insights than back-of-the-envelope calculations it makes automated design-space exploration of exascale systems possible, which we show using a real-world case study from radio astronomy.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/jongerius_et_al_2018_analytic_multi-core_processor_model_for_fast_design-space_exploration.pdf;/Users/rwb/Zotero/storage/HY24EMA4/8168422.html},
  journal = {IEEE Transactions on Computers},
  keywords = {analytic multicore processor model,Analytical models,ARM Cortex-A15 systems,automated design-space exploration,average single-thread error increases,cache memory,cache storage,Computational modeling,computer architects,computer architecture,fast design-space exploration,future exascale systems,Hardware,hardware configuration,hardware performance counters,instruction sets,instruction-level parallelism,memory hierarchy,memory-bandwidth contention,microprocessor chips,Modeling of computer architecture,models single instruction,moderate size,multi-core processors,multicore contention,Multicore processing,multicore processor-performance model,multiple data features,multiprocessing systems,parallel processing,parametric microarchitecture-independent characterization,performance evaluation,Program processors,SIMD processors,system designs,Tools},
  number = {6}
}

@article{juve2013,
  title = {Characterizing and Profiling Scientific Workflows},
  author = {Juve, Gideon and Chervenak, Ann and Deelman, Ewa and Bharathi, Shishir and Mehta, Gaurang and Vahi, Karan},
  year = {2013},
  month = mar,
  volume = {29},
  pages = {682--692},
  issn = {0167-739X},
  doi = {10.1016/j.future.2012.08.015},
  abstract = {Researchers working on the planning, scheduling, and execution of scientific workflows need access to a wide variety of scientific workflows to evaluate the performance of their implementations. This paper provides a characterization of workflows from six diverse scientific applications, including astronomy, bioinformatics, earthquake science, and gravitational-wave physics. The characterization is based on novel workflow profiling tools that provide detailed information about the various computational tasks that are present in the workflow. This information includes I/O, memory and computational characteristics. Although the workflows are diverse, there is evidence that each workflow has a job type that consumes the most amount of runtime. The study also uncovered inefficiency in a workflow component implementation, where the component was re-reading the same data multiple times.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/juve_et_al_2013_characterizing_and_profiling_scientific_workflows.pdf;/Users/rwb/Zotero/storage/SUAISRLW/S0167739X12001732.html},
  journal = {Future Generation Computer Systems},
  keywords = {Measurement,Performance,Profiling,Scientific workflows},
  language = {en},
  number = {3},
  series = {Special {{Section}}: {{Recent Developments}} in {{High Performance Computing}} and {{Security}}}
}

@article{kalavade1997,
  title = {The {{Extended Partitioning Problem}}: {{Hardware}}/{{Software Mapping}}, {{Scheduling}}, and {{Implementation}}-Bin {{Selection}}},
  shorttitle = {The {{Extended Partitioning Problem}}},
  author = {Kalavade, Asawaree and Lee, Edward A.},
  year = {1997},
  month = mar,
  volume = {2},
  pages = {125--163},
  issn = {0929-5585, 1572-8080},
  doi = {10.1023/A:1008872518365},
  abstract = {In system-level design, applications are represented as task graphs where tasks (called nodes) have moderate to large granularity and each node has several implementation options differing in area and execution time. We define the extended partitioning problem as the joint determination of the mapping (hardware or software), the implementation option (called implementation bin), as well as the schedule, for each node, so that the overall area allocated to nodes in hardware is minimum and a deadline constraint is met. This problem is considerably harder (and richer) than the traditional binary partitioning problem that determines just the best mapping and schedule. Both binary and extended partitioning problems are constrained optimization problems and are NP-hard.We first present an efficient(O(N2)) heuristic, called GCLP, to solve the binary partitioning problem. The heuristic reduces the greediness associated with traditional list-scheduling algorithms by formulating a global measure, called global criticality (GC). The GC measure also permits an adaptive selection of the optimization objective at each step of the algorithm; since the optimization problem is constrained by a deadline, either area or time is optimized at a given step based on the value of GC. The selected objective is used to determine the mapping of nodes that are ``normal'', i.e. nodes that do not exhibit affinity for a particular mapping. To account for nodes that are not ``normal'', we define ``extremities'' and ``repellers''. Extremities consume disproportionate amounts of resources in hardware and software. Repellers are inherently unsuitable to either hardware or software based on certain structural properties. The mapping of extremities and repellers is determined jointly by GC and their local preference.We then present an efficient ( O(N3 + N2B), for N nodes and B bins per node) heuristic for extended partitioning, called MIBS, that alternately uses GCLP and an implementation-bin selection procedure. The implementation-bin selection procedure chooses, for a node with already determined mapping, an implementation bin that maximizes the area-reduction gradient of as-yet unmapped nodes. Solutions generated by both heuristics are shown to be reasonably close to optimal. Extended partitioning generates considerably smaller overall hardware as compared to binary partitioning.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1997/kalavade_lee_1997_the_extended_partitioning_problem.pdf;/Users/rwb/Zotero/storage/8RDG2ZTM/A1008872518365.html},
  journal = {Design Automation for Embedded Systems},
  keywords = {Unread},
  language = {en},
  number = {2}
}

@article{kalra2015,
  title = {A Review of Metaheuristic Scheduling Techniques in Cloud Computing},
  author = {Kalra, Mala and Singh, Sarbjeet},
  year = {2015},
  month = nov,
  volume = {16},
  pages = {275--295},
  issn = {1110-8665},
  doi = {10.1016/j.eij.2015.07.001},
  abstract = {Cloud computing has become a buzzword in the area of high performance distributed computing as it provides on-demand access to shared pool of resources over Internet in a self-service, dynamically scalable and metered manner. Cloud computing is still in its infancy, so to reap its full benefits, much research is required across a broad array of topics. One of the important research issues which need to be focused for its efficient performance is scheduling. The goal of scheduling is to map tasks to appropriate resources that optimize one or more objectives. Scheduling in cloud computing belongs to a category of problems known as NP-hard problem due to large solution space and thus it takes a long time to find an optimal solution. There are no algorithms which may produce optimal solution within polynomial time to solve these problems. In cloud environment, it is preferable to find suboptimal solution, but in short period of time. Metaheuristic based techniques have been proved to achieve near optimal solutions within reasonable time for such problems. In this paper, we provide an extensive survey and comparative analysis of various scheduling algorithms for cloud and grid environments based on three popular metaheuristic techniques: Ant Colony Optimization (ACO), Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), and two novel techniques: League Championship Algorithm (LCA) and BAT algorithm.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/kalra_singh_2015_a_review_of_metaheuristic_scheduling_techniques_in_cloud_computing2.pdf;/Users/rwb/Zotero/storage/W68WGGEN/S1110866515000353.html},
  journal = {Egyptian Informatics Journal},
  keywords = {Ant colony optimization,Cloud task scheduling,Genetic algorithm and particle swarm optimization,League Championship Algorithm (LCA) and BAT algorithm,Metaheuristic techniques},
  number = {3}
}

@article{karniavoura2019,
  title = {Decision-{{Making Approaches}} for {{Performance QoS}} in {{Distributed Storage Systems}}: {{A Survey}}},
  shorttitle = {Decision-{{Making Approaches}} for {{Performance QoS}} in {{Distributed Storage Systems}}},
  author = {Karniavoura, F. and Magoutis, K.},
  year = {2019},
  month = aug,
  volume = {30},
  pages = {1906--1919},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2019.2893940},
  abstract = {Distributed storage systems designed to offer explicit performance quality-of-service (QoS) guarantees must regulate the allocation and use of resources to achieve a user-specified level of service. QoS-driven systems employ decision-making techniques to decide on appropriate actions to take during initial deployment or under variations in workload and/or system configuration. In this survey we cover both traditional approaches to decision-making for explicit performance QoS (control theory, multi-dimensional constrained optimization, policy-based techniques) as well as more recent approaches based on machine-learning, offering a broad perspective to the state-of-the-art in the field. As performance prediction is a central concept in decision-making, we also summarize research on performance prediction techniques used in this context.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/karniavoura_magoutis_2019_decision-making_approaches_for_performance_qos_in_distributed_storage_systems.pdf;/Users/rwb/Zotero/storage/Y23PZZEE/8618414.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Analytical models,Decision making,distributed storage,performance,Performance evaluation,Predictive models,Quality of service,Resource management,Servers},
  number = {8}
}

@article{kehris2005,
  title = {Application of {{Reinforcement Learning}} for the {{Generation}} of an {{Assembly Plant Entry Control Policy}}},
  author = {Kehris, E and Dranidis, D},
  year = {2005},
  volume = {9},
  pages = {214--220},
  abstract = {The generation of an entry control policy for an assembly plant using a reinforcement learning agent is investigated. The assembly plan studied consists of ten workstations and produces three types of products. The objective of the entry control policy is to produce a given production mix within a planning horizon, while following a given production mix. Due to the large state space, a function approximator, based on a neural network, is used to model the long-term reward function. The schedules generated by the trained agent are compared to those produced by a deterministic heuristic control policy that has been developed for this assembly plant. Simulation results show that the reinforcement learning agent produces production plans that achieve better productivity than the heuristic controller under tight planning horizons, generating sub-optimal yet acceptable production mix balance.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/kehris_dranidis_2005_application_of_reinforcement_learning_for_the_generation_of_an_assembly_plant.pdf},
  journal = {Journal of Intelligent Computation},
  language = {en},
  number = {2}
}

@article{khalil,
  title = {Learning to {{Branch}} in {{Mixed Integer Programming}}},
  author = {Khalil, Elias B and Bodic, Pierre Le and Song, Le and Nemhauser, George and Dilkina, Bistra},
  pages = {8},
  abstract = {The design of strategies for branching in Mixed Integer Programming (MIP) is guided by cycles of parameter tuning and offline experimentation on an extremely heterogeneous testbed, using the average performance. Once devised, these strategies (and their parameter settings) are essentially input-agnostic. To address these issues, we propose a machine learning (ML) framework for variable branching in MIP. Our method observes the decisions made by Strong Branching (SB), a time-consuming strategy that produces small search trees, collecting features that characterize the candidate branching variables at each node of the tree. Based on the collected data, we learn an easy-to-evaluate surrogate function that mimics the SB strategy, by means of solving a learning-to-rank problem, common in ML. The learned ranking function is then used for branching. The learning is instance-specific, and is performed on-the-fly while executing a branch-and-bound search to solve the instance. Experiments on benchmark instances indicate that our method produces significantly smaller search trees than existing heuristics, and is competitive with a state-of-the-art commercial solver.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/khalil_et_al_learning_to_branch_in_mixed_integer_programming.pdf},
  language = {en}
}

@inproceedings{khalil2017,
  title = {Learning to {{Run Heuristics}} in {{Tree Search}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Khalil, Elias B. and Dilkina, Bistra and Nemhauser, George L. and Ahmed, Shabbir and Shao, Yufen},
  year = {2017},
  month = aug,
  pages = {659--666},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/92},
  abstract = {Primal heuristics'' are a key contributor to the improved performance of exact branch-and-bound solvers for combinatorial optimization and integer programming. Perhaps the most crucial question concerning primal heuristics is that of at which nodes they should run, to which the typical answer is via hard-coded rules or fixed solver parameters tuned, offline, by trial-and-error. Alternatively, a heuristic should be run when it is most likely to succeed, based on the problem instance's characteristics, the state of the search, etc. In this work, we study the problem of deciding at which node a heuristic should be run, such that the overall (primal) performance of the solver is optimized. To our knowledge, this is the first attempt at formalizing and systematically addressing this problem. Central to our approach is the use of Machine Learning (ML) for predicting whether a heuristic will succeed at a given node. We give a theoretical framework for analyzing this decision-making process in a simplified setting, propose a ML approach for modeling heuristic success likelihood, and design practical rules that leverage the ML models to dynamically decide whether to run a heuristic at each node of the search tree. Experimentally, our approach improves the primal performance of a stateof-the-art Mixed Integer Programming solver by up to 6\% on a set of benchmark instances, and by up to 60\% on a family of hard Independent Set instances.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/khalil_et_al_2017_learning_to_run_heuristics_in_tree_search.pdf},
  isbn = {978-0-9992411-0-3},
  language = {en}
}

@article{kintsakis2019,
  title = {Reinforcement {{Learning}} Based Scheduling in a Workflow Management System},
  author = {Kintsakis, Athanassios M. and Psomopoulos, Fotis E. and Mitkas, Pericles A.},
  year = {2019},
  month = may,
  volume = {81},
  pages = {94--106},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2019.02.013},
  abstract = {Any computational process from simple data analytics tasks to training a machine learning model can be described by a workflow. Many workflow management systems (WMS) exist that undertake the task of scheduling workflows across distributed computational resources. In this work, we introduce a WMS that leverages machine learning to predict workflow task runtime and the probability of failure of task assignments to execution sites. The expected runtime of workflow tasks can be used to approximate the weight of the workflow graph branches in respect to the total workflow workload and the ability to anticipate task failures can discourage task assignments that are unlikely to succeed. We demonstrate that the proposed machine learning models can lead to significantly more informed scheduling decisions that minimize task failures and utilize execution sites more efficiently, thus leading to reduced workflow runtime. Additionally, we train a modified sequence-to-sequence neural network architecture via reinforcement learning to perform scheduling decisions as part of a WMS. Our approach introduces a WMS that can drastically improve its scheduling performance by independently learning over time, without external intervention or reliance on any specific heuristic or optimization technique. Finally, we test our approach in real-world scenarios utilizing computationally demanding and data intensive workflows and evaluate its performance against existing scheduling methodologies traditionally used in WMSes. The performance evaluation outcome confirms that the proposed approach significantly outperforms the other scheduling algorithms in a consistent manner and achieves the best execution runtime with the lowest number of failed tasks and communication costs.},
  file = {/Users/rwb/Zotero/storage/H78HN6QB/S0952197619300351.html},
  journal = {Engineering Applications of Artificial Intelligence},
  keywords = {Machine learning,Neural networks,Reinforcement Learning,Scheduling optimization,Workflow management systems},
  language = {en}
}

@article{kokilavani2011,
  title = {Load {{Balanced MinMin Algorithm}} for {{Static MetaTask Scheduling}} in {{Grid Computing}}},
  author = {Kokilavani, T. and George Amalarethinam, D.I.},
  year = {2011},
  month = apr,
  volume = {20},
  pages = {42--48},
  issn = {09758887},
  doi = {10.5120/2403-3197},
  abstract = {Grid computing has become a real alternative to traditional supercomputing environments for developing parallel applications that harness massive computational resources. However, the complexity incurred in building such parallel Grid-aware applications is higher than the traditional parallel computing environments. It addresses issues such as resource discovery, heterogeneity, fault tolerance and task scheduling. Load balanced task scheduling is very important problem in complex grid environment. So task scheduling which is one of the NP-Complete problems becomes a focus of research scholars in grid computing area. The traditional Min-Min algorithm is a simple algorithm that produces a schedule that minimizes the makespan than the other traditional algorithms in the literature. But it fails to produce a load balanced schedule. In this paper a Load Balanced Min-Min (LBMM) algorithm is proposed that reduces the makespan and increases the resource utilization. The proposed method has two-phases. In the first phase the traditional Min-Min algorithm is executed and in the second phase the tasks are rescheduled to use the unutilized resources effectively.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/kokilavani_george_amalarethinam_2011_load_balanced_minmin_algorithm_for_static_metatask_scheduling_in_grid_computing.pdf},
  journal = {International Journal of Computer Applications},
  language = {en},
  number = {2}
}

@article{kolisch1996,
  title = {Serial and Parallel Resource-Constrained Project Scheduling Methods Revisited: {{Theory}} and Computation},
  shorttitle = {Serial and Parallel Resource-Constrained Project Scheduling Methods Revisited},
  author = {Kolisch, Rainer},
  year = {1996},
  month = apr,
  volume = {90},
  pages = {320--333},
  issn = {0377-2217},
  doi = {10.1016/0377-2217(95)00357-6},
  abstract = {We consider the so-called parallel and serial scheduling method for the classical resource-constrained project scheduling problem. Theoretical results on the class of schedules generated by each method are provided. Furthermore, an in-depth computational study is undertaken to investigate the relationship of single-pass scheduling and sampling for both methods. It is shown that the performance-ranking of priority rules does not differ for single-pass scheduling and sampling, that sampling improves the performance of single-pass scheduling significantly, and that the parallel method cannot be generally considered as superior.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1996/kolisch_1996_serial_and_parallel_resource-constrained_project_scheduling_methods_revisited.pdf;/Users/rwb/Zotero/storage/HNVTPTJV/0377221795003576.html},
  journal = {European Journal of Operational Research},
  keywords = {Active and non-delay schedules,Experimental investigation,Priority rules,Resource-constrained project scheduling,Serial and parallel scheduling method,Single-pass scheduling and sampling},
  number = {2}
}

@article{kosar2009,
  title = {A New Paradigm: {{Data}}-Aware Scheduling in Grid Computing},
  shorttitle = {A New Paradigm},
  author = {Kosar, Tevfik and Balman, Mehmet},
  year = {2009},
  month = apr,
  volume = {25},
  pages = {406--413},
  issn = {0167-739X},
  doi = {10.1016/j.future.2008.09.006},
  abstract = {Efficient and reliable access to large-scale data sources and archiving destinations in a widely distributed computing environment brings new challenges. The insufficiency of the traditional systems and existing CPU-oriented batch schedulers in addressing these challenges has yielded a new emerging era: data-aware schedulers. In this article, we discuss the limitations of the traditional CPU-oriented batch schedulers in handling the challenging data management problem of large-scale distributed applications; give our vision for the new paradigm in data-intensive scheduling; and elaborate on our case study: the Stork data placement scheduler.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2009/kosar_balman_2009_a_new_paradigm.pdf;/Users/rwb/Zotero/storage/DQ3NVWF7/S0167739X08001520.html},
  journal = {Future Generation Computer Systems},
  keywords = {Data placement,Data-aware scheduling,Data-intensive applications,Grid computing,Stork},
  number = {4}
}

@article{kotsiantis,
  title = {Supervised {{Machine Learning}}: {{A Review}} of {{Classification Techniques}}},
  author = {Kotsiantis, S B},
  pages = {20},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/kotsiantis_supervised_machine_learning.pdf},
  language = {en}
}

@article{kotthoff2012,
  title = {Algorithm {{Selection}} for {{Combinatorial Search Problems}}: {{A Survey}}},
  shorttitle = {Algorithm {{Selection}} for {{Combinatorial Search Problems}}},
  author = {Kotthoff, Lars},
  year = {2012},
  month = oct,
  abstract = {The Algorithm Selection Problem is concerned with selecting the best algorithm to solve a given problem on a case-by-case basis. It has become especially relevant in the last decade, as researchers are increasingly investigating how to identify the most suitable existing algorithm for solving a problem instead of developing new algorithms. This survey presents an overview of this work focusing on the contributions made in the area of combinatorial search problems, where Algorithm Selection techniques have achieved significant performance improvements. We unify and organise the vast literature according to criteria that determine Algorithm Selection systems in practice. The comprehensive classification of approaches identifies and analyses the different directions from which Algorithm Selection has been approached. This paper contrasts and compares different methods for solving the problem as well as ways of using these solutions. It closes by identifying directions of current and future research.},
  archivePrefix = {arXiv},
  eprint = {1210.7959},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/kotthoff_2012_algorithm_selection_for_combinatorial_search_problems.pdf;/Users/rwb/Zotero/storage/QQ7NLIUR/1210.html},
  journal = {arXiv:1210.7959 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@incollection{kousalya2017,
  title = {Workflow {{Scheduling Algorithms}} and {{Approaches}}},
  booktitle = {Automated {{Workflow Scheduling}} in {{Self}}-{{Adaptive Clouds}}: {{Concepts}}, {{Algorithms}} and {{Methods}}},
  author = {Kousalya, G. and Balakrishnan, P. and Pethuru Raj, C.},
  editor = {Kousalya, G. and Balakrishnan, P. and Pethuru Raj, C.},
  year = {2017},
  pages = {65--83},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-56982-6_4},
  abstract = {Cloud infrastructures typically offer access to boundless virtual resources dynamically provisioned on demand for hosting, running, and managing a variety of mission-critical applications like scientific workflows, big data processing application, business intelligence-based applications, high-performance computing (HTC), and high transaction computing (HTC). Due to the surging popularity of the irresistible cloud idea, there are cloud datacenters spreading across the globe comprising heterogeneous cloud platforms and infrastructures catering to fast-evolving demands of worldwide businesses. The pervasive connectivity has enabled for the unprecedented success of the cloud concept. However, intensive automation is the key to the originally intended success of the cloud paradigm. Researchers across the world are focusing on unearthing powerful and pioneering tools and techniques for automated infrastructure life-cycle management. Similarly there are pathbreaking work-around approaches, algorithms, and architectures for workload consolidation. In short, there are many cloud-related aspects yearning for technologically sound automation, acceleration, and augmentation capabilities.Efficient scheduling algorithms become mandatory for automated operations of distributed and disparate cloud resources and workloads. The resource scheduling is a dynamic problem, and it is associated with on-demand resource provisioning, fault tolerance support, and hybrid resource scheduling with appropriate Quality of Service, considering time, cost, and budget. This chapter provides the details about various automated solutions for workflow scheduling and also comprehensive survey of various existing workflow scheduling algorithms in the cloud computing environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/kousalya_et_al_2017_workflow_scheduling_algorithms_and_approaches.pdf},
  isbn = {978-3-319-56982-6},
  keywords = {Cloud Environment,Cloud Service Provider,Dynamic Schedule,Schedule Algorithm,Static Schedule},
  language = {en},
  series = {Computer {{Communications}} and {{Networks}}}
}

@inproceedings{krueger2018,
  title = {Shaping {{Model}}-{{Free Reinforcement}}-{{Learning}} with {{Model}}-{{Based Pseudorewards}}},
  booktitle = {2018 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  author = {Krueger, Paul and Griffiths, Thomas},
  year = {2018},
  publisher = {{Cognitive Computational Neuroscience}},
  address = {{Philadelphia, Pennsylvania, USA}},
  doi = {10.32470/CCN.2018.1191-0},
  abstract = {Model-free and model-based reinforcement learning have provided a successful framework for understanding both human behavior and neural data. These two systems are usually thought to compete for control of behavior. However, it has also been proposed that they can be integrated in a cooperative manner. For example, the Dyna algorithm uses model-based replay of past experience to train the model-free system, and has inspired research examining whether human learners do something similar. Here we introduce an approach that links model-free and model-based learning in a new way: via the reward function. Given a model of the learning environment, dynamic programming is used to iteratively estimate state values that monotonically converge to the state values under the optimal decision policy. Pseudorewards are calculated from these values and used to shape the reward function of a model-free learner in a way that is guaranteed not to change the optimal policy. In two experiments we show that this method offers computational advantages over Dyna. It also offers a new way to think about integrating model-free and model-based reinforcement learning: that our knowledge of the world doesn't just provide a source of simulated experience for training our instincts, but that it shapes the rewards that those instincts latch onto.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/Krueger_Griffiths_2018_Shaping Model-Free Reinforcement-Learning with Model-Based Pseudorewards.pdf},
  keywords = {_tablet},
  language = {en}
}

@article{kumar2016,
  title = {Benchmarking {{NLopt}} and State-of-Art Algorithms for {{Continuous Global Optimization}} via {{Hybrid IACO}}\$\_\textbackslash mathbb\{\vphantom\}{{R}}\vphantom\{\}\$},
  author = {Kumar, Udit and Soman, Sumit and Jayadeva},
  year = {2016},
  month = apr,
  volume = {27},
  pages = {116--131},
  issn = {22106502},
  doi = {10.1016/j.swevo.2015.10.005},
  abstract = {This paper presents a comparative analysis of the performance of the Incremental Ant Colony algorithm for continuous optimization (IACOR), with different algorithms provided in the NLopt library. The key objective is to understand how the various algorithms in the NLopt library perform in combination with the Multi Trajectory Local Search (Mtsls1) technique. A hybrid approach has been introduced in the local search strategy by the use of a parameter which allows for probabilistic selection between Mtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch is made based on the algorithm being used in the previous iteration. The paper presents an exhaustive comparison on the performance of these approaches on Soft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014 benchmarks. For both benchmarks, we conclude that the best performing algorithm is a hybrid variant of Mtsls1 with BFGS for local search.},
  archivePrefix = {arXiv},
  eprint = {1503.03175},
  eprinttype = {arxiv},
  file = {/Users/rwb/Zotero/storage/A6M3BCLX/Kumar et al. - 2016 - Benchmarking NLopt and state-of-art algorithms for.pdf},
  journal = {Swarm and Evolutionary Computation},
  keywords = {80M50,Computer Science - Neural and Evolutionary Computing,G.1.6},
  language = {en}
}

@article{kwok1999,
  title = {Benchmarking and {{Comparison}} of the {{Task Graph Scheduling Algorithms}}},
  author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
  year = {1999},
  month = dec,
  volume = {59},
  pages = {381--422},
  issn = {0743-7315},
  doi = {10.1006/jpdc.1999.1578},
  abstract = {The problem of scheduling a parallel program represented by a weighted directed acyclic graph (DAG) to a set of homogeneous processors for minimizing the completion time of the program has been extensively studied. The NP-completeness of the problem has stimulated researchers to propose a myriad of heuristic algorithms. While most of these algorithms are reported to be efficient, it is not clear how they compare against each other. A meaningful performance evaluation and comparison of these algorithms is a complex task and it must take into account a number of issues. First, most scheduling algorithms are based upon diverse assumptions, making the performance comparison rather meaningless. Second, there does not exist a standard set of benchmarks to examine these algorithms. Third, most algorithms are evaluated using small problem sizes, and, therefore, their scalability is unknown. In this paper, we first provide a taxonomy for classifying various algorithms into distinct categories according to their assumptions and functionalities. We then propose a set of benchmarks that are based on diverse structures and are not biased toward a particular scheduling technique. We have implemented 15 scheduling algorithms and compared them on a common platform by using the proposed benchmarks, as well as by varying important problem parameters. We interpret the results based upon the design philosophies and principles behind these algorithms, drawing inferences why some algorithms perform better than others. We also propose a performance measure called scheduling scalability (SS) that captures the collective effectiveness of a scheduling algorithm in terms of its solution quality, the number of processors used, and the running time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1999/kwok_ahmad_1999_benchmarking_and_comparison_of_the_task_graph_scheduling_algorithms.pdf;/Users/rwb/Zotero/storage/Z438SDEB/S0743731599915782.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {benchmarks,multiprocessors,parallel processing,performance evaluation,scalability,scheduling,task graphs},
  number = {3}
}

@article{kwok1999a,
  ids = {kwok1999b},
  title = {Static {{Scheduling Algorithms}} for {{Allocating Directed Task Graphs}} to {{Multiprocessors}}},
  author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
  year = {1999},
  month = dec,
  volume = {31},
  pages = {406--471},
  issn = {0360-0300},
  doi = {10.1145/344588.344618},
  abstract = {Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1999/kwok_ahmad_1999_static_scheduling_algorithms_for_allocating_directed_task_graphs_to.pdf;/Users/rwb/Dropbox/PhD/zotero/1999/kwok_ahmad_1999_static_scheduling_algorithms_for_allocating_directed_task_graphs_to.pdf},
  journal = {ACM Comput. Surv.},
  keywords = {automatic parallelization,DAG,multiprocessors,parallel processing,software tools,static scheduling,task graphs},
  number = {4}
}

@inproceedings{lam2008,
  title = {Unrolling-Based Loop Mapping and Scheduling},
  booktitle = {2008 {{International Conference}} on {{Field}}-{{Programmable Technology}}},
  author = {Lam, Y. M. and Coutinho, J. G. F. and Luk, W. and Leong, P. H. W.},
  year = {2008},
  month = dec,
  pages = {321--324},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}},
  doi = {10.1109/FPT.2008.4762408},
  abstract = {This paper presents an loop unrolling based mapping and scheduling strategy to maximum the parallelism of an application described as task graph targeting on a heterogeneous computing systems. Loops are statically unrolled using compile-time parameters and dynamic tasks are generated to handle run-time conditions, such that the closer the match of run-time conditions and compile-time parameters, the higher the performance. Experimental results obtained using a speech recognition system show the proposed method outperforms an approach without unrolling by 2.1 times, and using the processing time of a 2.6GHz microprocessor as a reference, a speed up of 10 times can be achieved when compile-time and run-time parameters are matched, while the performance drops gradually when they are different.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/lam_et_al_2008_unrolling-based_loop_mapping_and_scheduling.pdf},
  isbn = {978-1-4244-2795-6 978-1-4244-3783-2},
  language = {en}
}

@inproceedings{langer2017,
  title = {A {{Survey}} of {{Parallel Hard}}-{{Real Time Scheduling}} on {{Task Models}} and {{Scheduling Approaches}}},
  booktitle = {{{ARCS}} 2017; 30th {{International Conference}} on {{Architecture}} of {{Computing Systems}}},
  author = {Langer, T. and Osinski, L. and Mottok, J.},
  year = {2017},
  month = apr,
  pages = {1--8},
  abstract = {The trend towards multi-core systems becomes increasinglyomnipresentandhasalreadyreachedhardreal-time systems. In the recent years more and more researchers tackle the problem of scheduling parallel tasks with real-time constraints. We present a discussion of these recent approaches to hard real-time scheduling tasks consisting of multiple threads. We identify categories in scheduling mechanisms and task systems and discuss their features, as well as the research already achieved in their fields. In the domain of task models we identified the gang constrained task model, the order constrained task model and the unconstrained task model. These task models differ regardingtheamountandtypeofthreadexecutionconstraints, which can be expressed using the model. Regarding scheduling algorithms, we identified transformation based and nontransformation based scheduling schemes. The former rely on off-line task transformation, whereas the latter doesn't require any prior information about task structures. The presented classifications show, that applied scheduling algorithms mostly are not restricted to certain task models, but that they are applied quite independently.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/langer_et_al_2017_a_survey_of_parallel_hard-real_time_scheduling_on_task_models_and_scheduling.pdf;/Users/rwb/Zotero/storage/75HI326P/7948560.html}
}

@techreport{lazio2011,
  title = {The {{Square Kilometre Array Design Reference Mission}}: {{SKA Phase}} 1},
  author = {Lazio, J},
  year = {2011},
  month = sep,
  pages = {75},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/lazio_2011_the_square_kilometre_array_design_reference_mission.pdf}
}

@inproceedings{lee1990,
  title = {An Efficient K-Way Graph Partitioning Algorithm for Task Allocation in Parallel Computing Systems},
  booktitle = {, {{Proceedings}} of the {{First International Conference}} on {{Systems Integration}}, 1990. {{Systems Integration}} '90},
  author = {Lee, C. H. and Kim, M. and Park, C. I.},
  year = {1990},
  month = apr,
  pages = {748--751},
  doi = {10.1109/ICSI.1990.138741},
  abstract = {The k-way graph partitioning problem can be transformed into the maximum k-cut problem using a proposed technique of graph modification. It is possible to transform the graph partitioning problem into the max-cut problem by incorporating node size information into the edge weight. After transformation, a very simple cost function can be devised which makes the proposed algorithm more efficient than the Kernighan-Lin (K-L) algorithm (1970). The computing time per iteration of the algorithm is O(k\texttimes N2), where N is the number of nodes in the given graph. Experimental results show that the proposed algorithm outperforms the K-L algorithm both in the quality of solutions and in the elapsed time. Also, as the difference between the sizes of the nodes increases, the performance gap between the proposed algorithm and the K-L algorithm becomes larger},
  file = {/Users/rwb/Dropbox/PhD/zotero/1990/lee_et_al_1990_an_efficient_k-way_graph_partitioning_algorithm_for_task_allocation_in_parallel.pdf;/Users/rwb/Zotero/storage/H5WZ4MQE/138741.html},
  keywords = {Clustering algorithms,computational complexity,Computer networks,Computer science,cost function,Cost function,Data structures,edge weight,graph modification,graph theory,Heuristic algorithms,Joining processes,k-way graph partitioning algorithm,Kernighan-Lin algorithm,maximum k-cut problem,node size,parallel algorithms,parallel computing systems,Parallel processing,Partitioning algorithms,performance,resource allocation,task allocation,Unread,Very large scale integration}
}

@article{lee1996,
  title = {Machine Scheduling with an Availability Constraint},
  author = {Lee, Chung-Yee},
  year = {1996},
  month = dec,
  volume = {9},
  pages = {395--416},
  issn = {0925-5001, 1573-2916},
  doi = {10.1007/BF00121681},
  abstract = {Most literature in scheduling assumes that machines are available simultaneously at all times. However, this availability may not be true in real industry settings. In this paper, we assume that the machine may not always be available. This happens often in the industry due to a machine breakdown (stochastic) or preventive maintenance (deterministic) during the scheduling period. We study the scheduling problem under this general situation and for the deterministic case.We discuss various performance measures and various machine environments. In each case, we either provide a polynomial optimal algorithm to solve the problem, or prove that the problem is NP-hard. In the latter case, we develop pseudo-polynomial dynamic programming models to solve the problem optimally and/or provide heuristics with an error bound analysis.},
  file = {/Users/rwb/Zotero/storage/FBUIRGVM/BF00121681.html},
  journal = {Journal of Global Optimization},
  keywords = {Unread},
  language = {en},
  number = {3-4}
}

@article{li2017,
  title = {Deep {{Reinforcement Learning}}: {{Framework}}, {{Applications}}, and {{Embedded Implementations}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Li, Hongjia and Wei, Tianshu and Ren, Ao and Zhu, Qi and Wang, Yanzhi},
  year = {2017},
  month = oct,
  abstract = {The recent breakthroughs of deep reinforcement learning (DRL) technique in Alpha Go and playing Atari have set a good example in handling large state and actions spaces of complicated control problems. The DRL technique is comprised of (i) an offline deep neural network (DNN) construction phase, which derives the correlation between each state-action pair of the system and its value function, and (ii) an online deep Qlearning phase, which adaptively derives the optimal action and updates value estimates.},
  archivePrefix = {arXiv},
  eprint = {1710.03792},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/li_et_al_2017_deep_reinforcement_learning.pdf},
  journal = {arXiv:1710.03792 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{li2018,
  title = {Deep Reinforcement Learning: {{Algorithm}}, Applications, and Ultra-Low-Power Implementation},
  shorttitle = {Deep Reinforcement Learning},
  author = {Li, Hongjia and Cai, Ruizhe and Liu, Ning and Lin, Xue and Wang, Yanzhi},
  year = {2018},
  month = jun,
  volume = {16},
  pages = {81--90},
  issn = {1878-7789},
  doi = {10.1016/j.nancom.2018.02.003},
  abstract = {In order to overcome the limitation of traditional reinforcement learning techniques on the restricted dimensionality of state and action spaces, the recent breakthroughs of deep reinforcement learning (DRL) in Alpha Go and playing Atari set a good example in handling large state and action spaces of complicated control problems. The DRL technique is comprised of an offline deep neural network (DNN) construction phase and an online deep Q-learning phase. In the offline phase, DNNs are utilized to derive the correlation between each state\textendash action pair of the system and its value function. In the online phase, a deep Q-learning technique is adopted based on the offline-trained DNN to derive the optimal action and meanwhile update the value estimates and the DNN. This paper is the first to provide a comprehensive study of applications of the DRL framework on cloud computing and residential smart grid systems along with efficient hardware implementations. Based on the introduction of the general DRL framework, we develop two applications, one for the cloud computing resource allocation problem and one for the residential smart grid user-end task scheduling problem. The former could achieve up to 54.1\% energy saving compared with baselines through automatically and dynamically distributing resources to servers. The latter achieves up to 22.77\% total energy cost reduction compared with the baseline algorithm. The DRL framework is mainly utilized for the complicated control problems and requires light-weight and low-power implementations in edge and portable systems. In order to achieve this goal, we develop the ultra-low-power implementation of the DRL framework using the stochastic computing technique, which has the potential of significantly enhancing the computation speed and reducing hardware footprint and therefore the power/energy consumption. The overall implementation is based on the effective stochastic computing-based implementations of approximate parallel counter-based inner product blocks and tanh activation functions. The stochastic computing-based implementation achieves only 57941.61 {$\mu$}m2 area and 6.30 mW power with 412.47 ns delay.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/Li et al_2018_Deep reinforcement learning.pdf;/Users/rwb/Zotero/storage/FT8FUEV5/S1878778917300856.html},
  journal = {Nano Communication Networks},
  keywords = {_tablet,Algorithm,Deep reinforcement learning,Stochastic computing,Ultra-low-power}
}

@inproceedings{li2019,
  title = {{{DeepJS}}: {{Job Scheduling Based}} on {{Deep Reinforcement Learning}} in {{Cloud Data Center}}},
  shorttitle = {{{DeepJS}}},
  booktitle = {Proceedings of the 2019 4th {{International Conference}} on {{Big Data}} and {{Computing}}},
  author = {Li, Fengcun and Hu, Bo},
  year = {2019},
  month = may,
  pages = {48--53},
  publisher = {{Association for Computing Machinery}},
  address = {{Guangzhou, China}},
  doi = {10.1145/3335484.3335513},
  abstract = {Job scheduling is a key building block of a cloud data center. Hand-crafted heuristics cannot automatically adapt to the change of the environment and optimize for specific workloads. We present the DeepJS, a job scheduling algorithm based on deep reinforcement learning under the framework of the bin packing problem. DeepJS can automatically obtain a fitness calculation method which will minimize the makespan (maximize the throughput) of a set of jobs directly from experience. Through a trace-driven simulation, we demonstrate the convergence and generalization of DeepJS and the essence of DeepJS learning. The results prove that DeepJS outperforms the heuristic-based job scheduling algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/li_hu_2019_deepjs.pdf},
  isbn = {978-1-4503-6278-8},
  keywords = {bin packing problem,cloud data center,deep reinforcement learning,Job scheduling},
  series = {{{ICBDC}} 2019}
}

@inproceedings{lilja1993,
  title = {Experiments with a {{Task Partitioning Model}} for {{Heterogeneous Computing}}},
  author = {Lilja, D.J.},
  year = {1993},
  pages = {29--35},
  publisher = {{IEEE}},
  doi = {10.1109/WHP.1993.664362},
  abstract = {One potentially promising approach for exploiting the best features of a variety of different computer architectures is to partition an application program to simultaneously execute on two or more different machines interconnected with a high-speed network. A fundamentalproblem with this heterogeneous computing, however, is the dinculty of partitioning an application program across the machines. Thispaper presents a partitioning strategy that relates the relathe performance of two heterogeneous machines to the cmrnunication cost of transferring partial results across their interconnection network. Experiments are described that use this strategy to partition two different application programs across the sequential front-end processor of a Connection Machine CM-200, and its parallel back-erid array.},
  isbn = {978-0-8186-3532-8},
  keywords = {Unread},
  language = {en}
}

@article{lin,
  ids = {lina},
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  pages = {29},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus twofold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/lin_self-improving_reactive_agents_based_on_reinforcement_learning,_planning_and.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/lin_self-improving_reactive_agents_based_on_reinforcement_learning,_planning_and2.pdf},
  language = {en}
}

@article{lin2018,
  title = {Necessary/Sufficient Conditions for {{Pareto}} Optimum in Cooperative Difference Game},
  author = {Lin, Yaning and Zhang, Weihai},
  year = {2018},
  volume = {39},
  pages = {1043--1060},
  issn = {1099-1514},
  doi = {10.1002/oca.2395},
  abstract = {This paper is concerned with the necessary/sufficient conditions for the Pareto optimum of the cooperative difference game in finite horizon. Utilizing the necessary and sufficient characterization of the Pareto optimum, the problem is transformed into a set of constrained optimal control problems with a special structure. Employing the discrete version of Pontryagin's maximum principle, the necessary conditions for the existence of the Pareto solutions are derived. Under certain convex assumptions, it is shown that the necessary conditions are sufficient too. Next, the obtained results are extended to the linear-quadratic case. For a fixed initial state, the necessary conditions resulting from the maximum principle and the convexity condition on the cost functional provide the necessary and sufficient description of the well-posedness of the weighted sum optimal control problem. For an arbitrary initial state, the solvability of the related difference Riccati equation provides a sufficient condition under which the Pareto-efficient strategies are equivalent to the weighted sum optimal controls. In addition, all Pareto solutions are derived based on the solutions of a set of difference equations. Two examples show the effectiveness of the proposed results.},
  copyright = {Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/lin_zhang_2018_necessary-sufficient_conditions_for_pareto_optimum_in_cooperative_difference.pdf;/Users/rwb/Zotero/storage/2QQ4ZILG/oca.html},
  journal = {Optimal Control Applications and Methods},
  keywords = {_tablet,cooperative difference game,discrete-time systems,LQ optimal control theory,Pareto optimum},
  language = {en},
  number = {2}
}

@article{little2011,
  title = {{{OR FORUM}}\textemdash{{Little}}'s {{Law}} as {{Viewed}} on {{Its}} 50th {{Anniversary}}},
  author = {Little, John D. C.},
  year = {2011},
  month = jun,
  volume = {59},
  pages = {536--549},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1110.0940},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/little_2011_or_forumlittle's_law_as_viewed_on_its_50th_anniversary.pdf},
  journal = {Operations Research},
  language = {en},
  number = {3}
}

@article{liu2008,
  title = {A Throughput Maximization Strategy for Scheduling Transaction-Intensive Workflows on {{SwinDeW}}-{{G}}},
  author = {Liu, Ke and Chen, Jinjun and Yang, Yun and Jin, Hai},
  year = {2008},
  month = oct,
  volume = {20},
  pages = {1807--1820},
  issn = {1532-0634},
  doi = {10.1002/cpe.1316},
  abstract = {With the rapid development of e-business, workflow systems now have to deal with transaction-intensive workflows whose main characteristic is the huge number of concurrent workflow instances. For such workflows, it is important to maximize the overall throughput to provide good quality of service. However, most of the existing scheduling algorithms are designed for scheduling of a single complex scientific workflow instance and are not efficient enough for scheduling transaction-intensive workflows. To address this problem, we propose a throughput maximization strategy (TMS), which contains two specific algorithms for scheduling transaction-intensive workflows at the instance and task levels, respectively. The first algorithm called Opposite Average Load tries to maximize the overall throughput by pursuing the overall load balance at the instance level, whereas the second algorithm called Extended Min\textendash Min tries to further maximize the overall throughput at the task level by increasing the utilization rate of resources within each local autonomous group. The comparison and simulation performed on Swinburne Decentralized Workflow for Grid (SwinDeW-G), a peer-to-peer-based grid workflow environment, demonstrate that our strategy can improve the overall throughput significantly over existing scheduling algorithms when scheduling transaction-intensive workflows. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/liu_et_al_2008_a_throughput_maximization_strategy_for_scheduling_transaction-intensive.pdf;/Users/rwb/Zotero/storage/P5RLIAJH/cpe.html},
  journal = {Concurrency and Computation: Practice and Experience},
  keywords = {p2p-based grid workflow systems,scheduling algorithms,transaction-intensive workflows},
  language = {en},
  number = {15}
}

@inproceedings{liu2014,
  title = {A {{Survey}} on {{Workflow Management}} and {{Scheduling}} in {{Cloud Computing}}},
  booktitle = {2014 14th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}}},
  author = {Liu, L. and Zhang, M. and Lin, Y. and Qin, L.},
  year = {2014},
  month = may,
  pages = {837--846},
  doi = {10.1109/CCGrid.2014.83},
  abstract = {Workflow application can be executed in cloud computing environments in utility-based fashion. In our paper, we first give a survey of cloud workflow application and present the cloud-based workflow architecture for Smart City. Then a variety of workflow scheduling algorithms are reviewed. The purpose of this paper is to making taxonomy for workflow management and scheduling in cloud environment, and also applying this cloud-based workflow architecture to Smart City environments, further presenting several research challenges in this area.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/liu_et_al_2014_a_survey_on_workflow_management_and_scheduling_in_cloud_computing2.pdf;/Users/rwb/Zotero/storage/NVSEVRQN/6846537.html},
  keywords = {Cities and towns,cloud computing,Cloud computing,cloud-based architecture,cloud-based workflow architecture,Computer architecture,Government,Optimization,scheduling,scheduling algorithm,Scheduling algorithms,smart city environments,software architecture,utility-based fashion,workflow management,workflow management software,workflow scheduling algorithms}
}

@article{liu2015,
  title = {A {{Survey}} of {{Data}}-{{Intensive Scientific Workflow Management}}},
  author = {Liu, Ji and Pacitti, Esther and Valduriez, Patrick and Mattoso, Marta},
  year = {2015},
  month = dec,
  volume = {13},
  pages = {457--493},
  issn = {1572-9184},
  doi = {10.1007/s10723-015-9329-8},
  abstract = {Nowadays, more and more computer-based scientific experiments need to handle massive amounts of data. Their data processing consists of multiple computational steps and dependencies within them. A data-intensive scientific workflow is useful for modeling such process. Since the sequential execution of data-intensive scientific workflows may take much time, Scientific Workflow Management Systems (SWfMSs) should enable the parallel execution of data-intensive scientific workflows and exploit the resources distributed in different infrastructures such as grid and cloud. This paper provides a survey of data-intensive scientific workflow management in SWfMSs and their parallelization techniques. Based on a SWfMS functional architecture, we give a comparative analysis of the existing solutions. Finally, we identify research issues for improving the execution of data-intensive scientific workflows in a multisite cloud.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/liu_et_al_2015_a_survey_of_data-intensive_scientific_workflow_management.pdf},
  journal = {Journal of Grid Computing},
  keywords = {Cloud,Distributed and parallel data management,Grid,Multisite cloud,Parallelization,Scheduling,Scientific workflow,Scientific workflow management system},
  language = {en},
  number = {4}
}

@article{liu2017,
  title = {A {{Hierarchical Framework}} of {{Cloud Resource Allocation}} and {{Power Management Using Deep Reinforcement Learning}}},
  author = {Liu, Ning and Li, Zhe and Xu, Zhiyuan and Xu, Jielong and Lin, Sheng and Qiu, Qinru and Tang, Jian and Wang, Yanzhi},
  year = {2017},
  month = mar,
  abstract = {Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces.},
  archivePrefix = {arXiv},
  eprint = {1703.04221},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/liu_et_al_2017_a_hierarchical_framework_of_cloud_resource_allocation_and_power_management.pdf},
  journal = {arXiv:1703.04221 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{liu2017a,
  title = {A {{Mathematical Programming}}- and {{Simulation}}-{{Based Framework}} to {{Evaluate Cyberinfrastructure Design Choices}}},
  booktitle = {2017 {{IEEE}} 13th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Liu, Zhengchun and Kettimuthu, Rajkumar and Leyffer, Sven and Palkar, Prashant and Foster, Ian},
  year = {2017},
  month = oct,
  pages = {148--157},
  publisher = {{IEEE}},
  address = {{Auckland}},
  doi = {10.1109/eScience.2017.27},
  abstract = {Modern scientific experimental facilities such as xray light sources increasingly require on-demand access to largescale computing for data analysis, for example to detect experimental errors or to select the next experiment. As the number of such facilities, the number of instruments at each facility, and the scale of computational demands all grow, the question arises as to how to meet these demands most efficiently and cost-effectively. A single computer per instrument is unlikely to be cost-effective because of low utilization and high operating costs. A single national compute facility, on the other hand, introduces a single point of failure and perhaps excessive communication costs. We introduce here methods for evaluating these and other potential design points, such as per-facility computer systems and a distributed multisite ``superfacility.'' We use the U.S. Department of Energy light sources as a use case and build a mixed-integer programming model and a customizable superfacility simulator to enable joint optimization of design choices and associated operational decisions. The methodology and tools provide new insights into design choices for on-demand computing facilities for real-time analysis of scientific experiment data. The simulator can also be used to support facility operations, for example by simulating the impact of events such as outages.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/liu_et_al_2017_a_mathematical_programming-_and_simulation-based_framework_to_evaluate.pdf},
  isbn = {978-1-5386-2686-3},
  language = {en}
}

@incollection{lodi2013,
  title = {The {{Heuristic}} ({{Dark}}) {{Side}} of {{MIP Solvers}}},
  booktitle = {Hybrid {{Metaheuristics}}},
  author = {Lodi, Andrea},
  editor = {Talbi, El-Ghazali},
  year = {2013},
  pages = {273--284},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-30671-6_10},
  abstract = {The evolution of Mixed-Integer Linear Programming (MIP) solvers has reached a very stable and effective level in which solving real-world problems is possible. However, the computed solution is not always the optimal one also because optimality is often not of primary interest for day-by-day users. We show some structural characteristics of MIP solvers and of computation for MIP problems that reveal the heuristic nature of the solvers. Moreover, we discuss the key components of MIP solvers with special emphasis on the role of heuristic decisions within the solution process. Finally, we present MIP solvers as ``open'' frameworks whose flexibility can be exploited to devise sophisticated hybrid algorithms.},
  isbn = {978-3-642-30671-6},
  keywords = {Crew Schedule,Global Constraint,Heuristic Decision,Linear Programming Relaxation,Mixed Integer Programming},
  language = {en},
  series = {Studies in {{Computational Intelligence}}}
}

@article{lodi2017,
  title = {On Learning and Branching: A Survey},
  shorttitle = {On Learning and Branching},
  author = {Lodi, Andrea and Zarpellon, Giulia},
  year = {2017},
  month = jul,
  volume = {25},
  pages = {207--236},
  issn = {1134-5764, 1863-8279},
  doi = {10.1007/s11750-017-0451-6},
  abstract = {This paper surveys learning techniques to deal with the two most crucial decisions in the branch-and-bound algorithm for Mixed-Integer Linear Programming, namely variable and node selections. Because of the lack of deep mathematical understanding on those decisions, the classical and vast literature in the field is inherently based on computational studies and heuristic, often problem-specific, strategies. We will both interpret some of those early contributions in the light of modern (Machine) Learning techniques, and give the details of the recent algorithms that instead explicitly incorporate Machine Learning paradigms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/lodi_zarpellon_2017_on_learning_and_branching.pdf},
  journal = {TOP},
  language = {en},
  number = {2}
}

@inproceedings{lofstead2010,
  title = {Managing {{Variability}} in the {{IO Performance}} of {{Petascale Storage Systems}}},
  booktitle = {{{SC}} '10: {{Proceedings}} of the 2010 {{ACM}}/{{IEEE International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Lofstead, J. and Zheng, F. and Liu, Q. and Klasky, S. and Oldfield, R. and Kordenbrock, T. and Schwan, K. and Wolf, M.},
  year = {2010},
  month = nov,
  pages = {1--12},
  doi = {10.1109/SC.2010.32},
  abstract = {Significant challenges exist for achieving peak or even consistent levels of performance when using IO systems at scale. They stem from sharing IO system resources across the processes of single largescale applications and/or multiple simultaneous programs causing internal and external interference, which in turn, causes substantial reductions in IO performance. This paper presents interference effects measurements for two different file systems at multiple supercomputing sites. These measurements motivate developing a 'managed' IO approach using adaptive algorithms varying the IO system workload based on current levels and use areas. An implementation of these methods deployed for the shared, general scratch storage system on Oak Ridge National Laboratory machines achieves higher overall performance and less variability in both a typical usage environment and with artificially introduced levels of 'noise'. The latter serving to clearly delineate and illustrate potential problems arising from shared system usage and the advantages derived from actively managing it.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/lofstead_et_al_2010_managing_variability_in_the_io_performance_of_petascale_storage_systems.pdf;/Users/rwb/Zotero/storage/IBH2RKA7/5644883.html},
  keywords = {adaptive algorithms,Aggregates,Bandwidth,file systems,Interference,interference effects measurements,IO performance,Laboratories,Limiting,oak ridge national laboratory machines,Organizations,petascale storage systems,storage management,variability management,Writing}
}

@inproceedings{lofstead2011,
  title = {Six {{Degrees}} of {{Scientific Data}}: {{Reading Patterns}} for {{Extreme Scale Science IO}}},
  shorttitle = {Six {{Degrees}} of {{Scientific Data}}},
  booktitle = {Proceedings of the 20th {{International Symposium}} on {{High Performance Distributed Computing}}},
  author = {Lofstead, Jay and Polte, Milo and Gibson, Garth and Klasky, Scott and Schwan, Karsten and Oldfield, Ron and Wolf, Matthew and Liu, Qing},
  year = {2011},
  pages = {49--60},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1996130.1996139},
  abstract = {Petascale science simulations generate 10s of TBs of application data per day, much of it devoted to their checkpoint/restart fault tolerance mechanisms. Previous work demonstrated the importance of carefully managing such output to prevent application slowdown due to IO blocking, resource contention negatively impacting simulation performance and to fully exploit the IO bandwidth available to the petascale machine. This paper takes a further step in understanding and managing extreme-scale IO. Specifically, its evaluations seek to understand how to efficiently read data for subsequent data analysis, visualization, checkpoint restart after a failure, and other read-intensive operations. In their entirety, these actions support the 'end-to-end' needs of scientists enabling the scientific processes being undertaken. Contributions include the following. First, working with application scientists, we define 'read' benchmarks that capture the common read patterns used by analysis codes. Second, these read patterns are used to evaluate different IO techniques at scale to understand the effects of alternative data sizes and organizations in relation to the performance seen by end users. Third, defining the novel notion of a 'data district' to characterize how data is organized for reads, we experimentally compare the read performance seen with the ADIOS middleware's log-based BP format to that seen by the logically contiguous NetCDF or HDF5 formats commonly used by analysis tools. Measurements assess the performance seen across patterns and with different data sizes, organizations, and read process counts. Outcomes demonstrate that high end-to-end IO performance requires data organizations that offer flexibility in data layout and placement on parallel storage targets, including in ways that can make tradeoffs in the performance of data writes vs. reads.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/lofstead_et_al_2011_six_degrees_of_scientific_data.pdf},
  isbn = {978-1-4503-0552-5},
  keywords = {adios,analysis,hdf5,log-based,logically contiguous,netcdf,pnetcdf,visualization},
  series = {{{HPDC}} '11}
}

@inproceedings{lofstead2013,
  title = {Insights for {{Exascale IO APIs}} from {{Building}} a {{Petascale IO API}}},
  booktitle = {Proceedings of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Lofstead, Jay and Ross, Robert},
  year = {2013},
  pages = {87:1--87:12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2503210.2503238},
  abstract = {Near the dawn of the petascale era, IO libraries had reached a stability in their function and data layout with only incremental changes being incorporated. The shift in technology, particularly the scale of parallel file systems and the number of compute processes, prompted revisiting best practices for optimal IO performance. Among other efforts like PLFS, the project that led to ADIOS, the ADaptable IO System, was motivated by both the shift in technology and the historical requirement, for optimal IO performance, to change how simulations performed IO depending on the platform. To solve both issues, the ADIOS team, along with consultation with other leading IO experts, sought to build a new IO platform based on the assumptions inherent in the petascale hardware platforms. This paper helps inform the design of future IO platforms with a discussion of lessons learned as part of the process of designing and building ADIOS.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/lofstead_ross_2013_insights_for_exascale_io_apis_from_building_a_petascale_io_api.pdf},
  isbn = {978-1-4503-2378-9},
  series = {{{SC}} '13}
}

@article{ludascher2006,
  title = {Scientific Workflow Management and the {{Kepler}} System},
  author = {Lud{\"a}scher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
  year = {2006},
  month = aug,
  volume = {18},
  pages = {1039--1065},
  issn = {1532-0634},
  doi = {10.1002/cpe.994},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/ludscher_et_al_2006_scientific_workflow_management_and_the_kepler_system.pdf;/Users/rwb/Zotero/storage/7CWFA8LE/cpe.html},
  journal = {Concurrency and Computation: Practice and Experience},
  language = {en},
  number = {10}
}

@article{ludascher2006a,
  title = {Scientific Workflow Management and the {{Kepler}} System},
  author = {Lud{\"a}scher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
  year = {2006},
  month = aug,
  volume = {18},
  pages = {1039--1065},
  issn = {1532-0626, 1532-0634},
  doi = {10.1002/cpe.994},
  file = {/Users/rwb/Zotero/storage/6NVV8SBL/Ludscher et al. - 2006 - Scientific workflow management and the Kepler syst.pdf},
  journal = {Concurrency and Computation: Practice and Experience},
  language = {en},
  number = {10}
}

@article{ludascher2006b,
  title = {Scientific Workflow Management and the {{Kepler}} System},
  author = {Lud{\"a}scher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
  year = {2006},
  month = aug,
  volume = {18},
  pages = {1039--1065},
  issn = {1532-0626, 1532-0634},
  doi = {10.1002/cpe.994},
  file = {/Users/rwb/Zotero/storage/4IRRJJQY/Ludscher et al. - 2006 - Scientific workflow management and the Kepler syst.pdf},
  journal = {Concurrency and Computation: Practice and Experience},
  language = {en},
  number = {10}
}

@article{lutzeyer2017,
  title = {Comparing {{Graph Spectra}} of {{Adjacency}} and {{Laplacian Matrices}}},
  author = {Lutzeyer, J. F. and Walden, A. T.},
  year = {2017},
  month = dec,
  abstract = {Typically, graph structures are represented by one of three different matrices: the adjacency matrix, the unnormalised and the normalised graph Laplacian matrices. The spectral (eigenvalue) properties of these different matrices are compared. For each pair, the comparison is made by applying an affine transformation to one of them, which enables comparison whilst preserving certain key properties such as normalised eigengaps. Bounds are given on the eigenvalue differences thus found, which depend on the minimum and maximum degree of the graph. The monotonicity of the bounds and the structure of the graphs are related. The bounds on a real social network graph, and on three model graphs, are illustrated and analysed. The methodology is extended to provide bounds on normalised eigengap differences which again turn out to be in terms of the graph's degree extremes. It is found that if the degree extreme difference is large, different choices of representation matrix may give rise to disparate inference drawn from graph signal processing algorithms; smaller degree extreme differences result in consistent inference, whatever the choice of representation matrix. The different inference drawn from signal processing algorithms is visualised using the spectral clustering algorithm on the three representation matrices corresponding to a model graph and a real social network graph.},
  archivePrefix = {arXiv},
  eprint = {1712.03769},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/lutzeyer_walden_2017_comparing_graph_spectra_of_adjacency_and_laplacian_matrices.pdf},
  journal = {arXiv:1712.03769 [stat]},
  keywords = {Statistics - Methodology},
  language = {en},
  primaryClass = {stat}
}

@article{mahmood2017,
  ids = {mahmood2017},
  title = {Hard {{Real}}-{{Time Task Scheduling}} in {{Cloud Computing Using}} an {{Adaptive Genetic Algorithm}}},
  author = {Mahmood, Amjad and Khan, Salman and Bahlool, Rashed A. and Mahmood, Amjad and Khan, Salman A. and Bahlool, Rashed A.},
  year = {2017},
  month = apr,
  volume = {6},
  pages = {15},
  doi = {10.3390/computers6020015},
  abstract = {In the Infrastructure-as-a-Service cloud computing model, virtualized computing resources in the form of virtual machines are provided over the Internet. A user can rent an arbitrary number of computing resources to meet their requirements, making cloud computing an attractive choice for executing real-time tasks. Economical task allocation and scheduling on a set of leased virtual machines is an important problem in the cloud computing environment. This paper proposes a greedy and a genetic algorithm with an adaptive selection of suitable crossover and mutation operations (named as AGA) to allocate and schedule real-time tasks with precedence constraint on heterogamous virtual machines. A comprehensive simulation study has been done to evaluate the performance of the proposed algorithms in terms of their solution quality and efficiency. The simulation results show that AGA outperforms the greedy algorithm and non-adaptive genetic algorithm in terms of solution quality.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/mahmood_et_al_2017_hard_real-time_task_scheduling_in_cloud_computing_using_an_adaptive_genetic.pdf;/Users/rwb/Zotero/storage/JE7KJFKH/15.html;/Users/rwb/Zotero/storage/SI4595FA/15.html},
  journal = {Computers},
  keywords = {cloud computing,genetic algorithms,real-time systems,task scheduling},
  language = {en},
  number = {2}
}

@article{malawski2015,
  title = {Algorithms for Cost- and Deadline-Constrained Provisioning for Scientific Workflow Ensembles in {{IaaS}} Clouds},
  author = {Malawski, Maciej and Juve, Gideon and Deelman, Ewa and Nabrzyski, Jarek},
  year = {2015},
  month = jul,
  volume = {48},
  pages = {1--18},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.01.004},
  abstract = {Large-scale applications expressed as scientific workflows are often grouped into ensembles of inter-related workflows. In this paper, we address a new and important problem concerning the efficient management of such ensembles under budget and deadline constraints on Infrastructure as a Service (IaaS) clouds. IaaS clouds are characterized by on-demand resource provisioning capabilities and a pay-per-use model. We discuss, develop, and assess novel algorithms based on static and dynamic strategies for both task scheduling and resource provisioning. We perform the evaluation via simulation using a set of scientific workflow ensembles with a broad range of budget and deadline parameters, taking into account task granularity, uncertainties in task runtime estimations, provisioning delays, and failures. We find that the key factor determining the performance of an algorithm is its ability to decide which workflows in an ensemble to admit or reject for execution. Our results show that an admission procedure based on workflow structure and estimates of task runtimes can significantly improve the quality of solutions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/malawski_et_al_2015_algorithms_for_cost-_and_deadline-constrained_provisioning_for_scientific.pdf;/Users/rwb/Zotero/storage/HBEYVECE/S0167739X15000059.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cloud computing,Resource provisioning,Scientific workflows,Workflow ensembles},
  series = {Special {{Section}}: {{Business}} and {{Industry Specific Cloud}}}
}

@article{malik,
  title = {Constraint {{Programming Techniques}} for {{Optimal Instruction Scheduling}}},
  author = {Malik, Abid Muslim},
  pages = {151},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/malik_constraint_programming_techniques_for_optimal_instruction_scheduling.pdf},
  language = {en}
}

@inproceedings{mao2016,
  title = {Resource {{Management}} with {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 15th {{ACM Workshop}} on {{Hot Topics}} in {{Networks}}  - {{HotNets}} '16},
  author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
  year = {2016},
  pages = {50--56},
  publisher = {{ACM Press}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1145/3005745.3005750},
  abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-ofthe-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
  file = {/Users/rwb/Zotero/storage/LA6LBKPV/Mao et al. - 2016 - Resource Management with Deep Reinforcement Learni.pdf},
  isbn = {978-1-4503-4661-0},
  language = {en}
}

@inproceedings{mao2016a,
  title = {{{DAG Constrained Scheduling Prototype}} for an {{Astronomy Exa}}-{{Scale HPC Application}}},
  booktitle = {2016 {{IEEE}} 18th {{International Conference}} on {{High Performance Computing}} and {{Communications}}; {{IEEE}} 14th {{International Conference}} on {{Smart City}}; {{IEEE}} 2nd {{International Conference}} on {{Data Science}} and {{Systems}} ({{HPCC}}/{{SmartCity}}/{{DSS}})},
  author = {Mao, Yishu and Zhu, Yongxin and Huang, Tian and Song, Han and Xue, Qixuan},
  year = {2016},
  month = dec,
  pages = {631--638},
  doi = {10.1109/HPCC-SmartCity-DSS.2016.0094},
  abstract = {The Square Kilometer Array (SKA) under construction aims to be the world's largest telescope. Its Science Data Processing (SDP) is responsible for processing the observed data into science-ready data products. The exa-scale data throughtput calls for scheduling to assign computation tasks to networked computing island to relieve computation pressure. Popular scheduling methods in the classic high performance computing centers cannot fit into perplexing constraints over the computation tasks due to evolving telescope management policies and a large number of users across the world. Aiming at using the SDP workflow to construct radio signal images, we propose a set of comprehensive strategies, Genetic Algorithm (GA) optimization and Critical-path Aware Earliest Finish Time (EFT), to reduce the overall execution time on different laxity time scenarios. If the observation tasks are planned in well advance, GA, which optimizes the scheduling results through a heuristic manner, is verified to outperform Round Robin. But if observation tasks are issued in short notice, to reduce time consumption, Critical-path Aware EFT is implemented. It is partitioned into upper and lower level to be a hierarchical scheduler and this mechanism provides both coarse and fine grained scheduling which guarantees the communication within the scheduler. Implemented on the popular cloud computing simulator CloudSim with datasets of different scales, Critical-path Aware EFT proves to be an effective and extendable method.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/mao_et_al_2016_dag_constrained_scheduling_prototype_for_an_astronomy_exa-scale_hpc_application.pdf;/Users/rwb/Zotero/storage/DRXAMXMW/7828435.html},
  keywords = {Arrays,astronomy,astronomy computing,Astronomy exascale HPC application,cloud computing,Cloud computing,cloud computing simulator,CloudSim,computation task assignment,critical-path aware earliest finish time,critical-path aware EFT,DAG,DAG constrained scheduling prototype,directed graphs,Electronic mail,exa-scale,genetic algorithm optimization,genetic algorithms,Genetic algorithms,hierarchical scheduler,high performance computing center,networked computing island,observed data processing,overall execution time reduction,parallel processing,Processor scheduling,radio signal image,radioastronomical techniques,scheduling,Scheduling,scheduling method,Science Data Processing,science-ready data product,SDP workflow,SKA,Square Kilometer Array,telescope management policies,Telescopes}
}

@article{mao2018,
  title = {Learning {{Scheduling Algorithms}} for {{Data Processing Clusters}}},
  author = {Mao, Hongzi and Schwarzkopf, Malte and Venkatakrishnan, Shaileshh Bojja and Meng, Zili and Alizadeh, Mohammad},
  year = {2018},
  month = oct,
  abstract = {Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems, however, use simple generalized heuristics and ignore workload structure, since developing and tuning a bespoke heuristic for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond specifying a high-level objective such as minimizing average job completion time. Off-the-shelf RL techniques, however, cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent new RL training methods for continuous job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima outperforms several heuristics, including hand-tuned ones, by at least 21\%. Further experiments with an industrial production workload trace demonstrate that Decima delivers up to a 17\% reduction in average job completion time and scales to large clusters.},
  archivePrefix = {arXiv},
  eprint = {1810.01963},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/Mao et al_2018_Learning Scheduling Algorithms for Data Processing Clusters.pdf;/Users/rwb/Zotero/storage/GW5KSPTV/1810.html},
  journal = {arXiv:1810.01963 [cs, stat]},
  keywords = {_tablet,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{maravelias2009,
  title = {Integration of Production Planning and Scheduling: {{Overview}}, Challenges and Opportunities},
  shorttitle = {Integration of Production Planning and Scheduling},
  author = {Maravelias, Christos T. and Sung, Charles},
  year = {2009},
  month = dec,
  volume = {33},
  pages = {1919--1930},
  issn = {0098-1354},
  doi = {10.1016/j.compchemeng.2009.06.007},
  abstract = {We review the integration of medium-term production planning and short-term scheduling. We begin with an overview of supply chain management and the associated planning problems. Next, we formally define the production planning problem and explain why integration with scheduling leads to better solutions. We present the major modeling approaches for the integration of scheduling and planning decisions, and discuss the major solution strategies. We close with an account of the challenges and opportunities in this area.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2009/maravelias_sung_2009_integration_of_production_planning_and_scheduling.pdf;/Users/rwb/Zotero/storage/GI9373M8/S0098135409001501.html},
  journal = {Computers \& Chemical Engineering},
  keywords = {Mixed-integer programming,Production planning,Scheduling,Supply chain management},
  number = {12},
  series = {{{FOCAPO}} 2008 \textendash{} {{Selected Papers}} from the {{Fifth International Conference}} on {{Foundations}} of {{Computer}}-{{Aided Process Operations}}}
}

@article{martin2016,
  title = {A Multi-Agent Based Cooperative Approach to Scheduling and Routing},
  author = {Martin, Simon and Ouelhadj, Djamila and Beullens, Patrick and Ozcan, Ender and Juan, Angel A. and Burke, Edmund K.},
  year = {2016},
  month = oct,
  volume = {254},
  pages = {169--178},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2016.02.045},
  abstract = {In this paper, we propose a general agent-based distributed framework where each agent is implementing a different metaheuristic/local search combination. Moreover, an agent continuously adapts itself during the search process using a direct cooperation protocol based on reinforcement learning and pattern matching. Good patterns that make up improving solutions are identified and shared by the agents. This agent-based system aims to provide a modular flexible framework to deal with a variety of different problem domains. We have evaluated the performance of this approach using the proposed framework which embodies a set of well known metaheuristics with different configurations as agents on two problem domains, Permutation Flow-shop Scheduling and Capacitated Vehicle Routing. The results show the success of the approach yielding three new best known results of the Capacitated Vehicle Routing benchmarks tested, whilst the results for Permutation Flow-shop Scheduling are commensurate with the best known values for all the benchmarks tested.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/martin_et_al_2016_a_multi-agent_based_cooperative_approach_to_scheduling_and_routing.pdf;/Users/rwb/Zotero/storage/4H47RE59/S0377221716300984.html},
  journal = {European Journal of Operational Research},
  keywords = {Combinatorial optimization,Cooperative search,Metaheuristics,Scheduling,Vehicle routing},
  number = {1}
}

@article{masdari2016,
  ids = {masdari2016a},
  title = {Towards Workflow Scheduling in Cloud Computing: {{A}} Comprehensive Analysis},
  shorttitle = {Towards Workflow Scheduling in Cloud Computing},
  author = {Masdari, Mohammad and ValiKardan, Sima and Shahi, Zahra and Azar, Sonay Imani},
  year = {2016},
  month = may,
  volume = {66},
  pages = {64--82},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2016.01.018},
  abstract = {Workflow scheduling is one of the prominent issues in cloud computing which is aimed at complete execution of workflows by considering their QoS requirements such as deadline and budget constraints. Numerous state of the art workflow scheduling schemes have been proposed in the literature for scheduling simple and scientific workflows in the cloud computing and this paper presents a comprehensive survey and analysis of these schemes. It illuminates the objectives of scheduling schemes in the cloud computing and provides a classification of the proposed schemes based on the type of scheduling algorithm applied in each scheme. Beside, each scheme is illustrated and a complete comparison of them is presented to highlight their objectives, properties and limitations. Finally, the concluding remarks and future research directions are provided.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/masdari_et_al_2016_towards_workflow_scheduling_in_cloud_computing.pdf;/Users/rwb/Zotero/storage/L4VD4QFY/S108480451600045X.html;/Users/rwb/Zotero/storage/SWL7FU9M/S108480451600045X.html},
  journal = {Journal of Network and Computer Applications},
  keywords = {Cloud computing,Metaheuristic,QoS,Workflow scheduling}
}

@article{masdari2017,
  title = {A {{Survey}} of {{PSO}}-{{Based Scheduling Algorithms}} in {{Cloud Computing}}},
  author = {Masdari, Mohammad and Salehi, Farbod and Jalali, Marzie and Bidaki, Moazam},
  year = {2017},
  month = jan,
  volume = {25},
  pages = {122--158},
  issn = {1573-7705},
  doi = {10.1007/s10922-016-9385-9},
  abstract = {Cloud computing provides effective mechanisms for distributing the computing tasks to the virtual resources. To provide cost-effective executions and achieve objectives such as load balancing, availability and reliability in the cloud environment, appropriate task and workflow scheduling solutions are needed. Various metaheuristic algorithms are applied to deal with the problem of scheduling, which is an NP-hard problem. This paper presents an in-depth analysis of the Particle Swarm Optimization (PSO)-based task and workflow scheduling schemes proposed for the cloud environment in the literature. Moreover, it provides a classification of the proposed scheduling schemes based on the type of the PSO algorithms which have been applied in these schemes and illuminates their objectives, properties and limitations. Finally, the critical future research directions are outlined.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/masdari_et_al_2017_a_survey_of_pso-based_scheduling_algorithms_in_cloud_computing.pdf},
  journal = {Journal of Network and Systems Management},
  keywords = {_tablet,Cost,Load balancing,Makespan,Meta-heuristic,PSO,SLA,Task,Workflow},
  language = {en},
  number = {1}
}

@article{mattoso2015,
  title = {Dynamic Steering of {{HPC}} Scientific Workflows: {{A}} Survey},
  shorttitle = {Dynamic Steering of {{HPC}} Scientific Workflows},
  author = {Mattoso, Marta and Dias, Jonas and Oca{\~n}a, Kary A. C. S. and Ogasawara, Eduardo and Costa, Flavio and Horta, Felipe and Silva, V{\'i}tor and {de Oliveira}, Daniel},
  year = {2015},
  month = may,
  volume = {46},
  pages = {100--113},
  issn = {0167-739X},
  doi = {10.1016/j.future.2014.11.017},
  abstract = {The field of scientific workflow management systems has grown significantly as applications start using them successfully. In 2007, several active researchers in scientific workflow developments presented the challenges for the state of the art in workflow technologies at that time. Many issues have been addressed, but one of them named `dynamic workflows and user steering' remains with many open problems despite the contributions presented in the recent years. This article surveys the early and current efforts in this topic and proposes a taxonomy to identify the main concepts related to addressing issues in dynamic steering of high performance computing (HPC) in scientific workflows. The main concepts are related to putting the human-in-the-loop of the workflow lifecycle, involving user support in real-time monitoring, notification, analysis and interference by adapting the workflow execution at runtime.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/mattoso_et_al_2015_dynamic_steering_of_hpc_scientific_workflows.pdf;/Users/rwb/Zotero/storage/Z8YTQ3UE/S0167739X14002519.html},
  journal = {Future Generation Computer Systems},
  keywords = {_tablet,Cloud computing,Dynamic workflows,HPC,Scientific workflows,Steering}
}

@article{maurya2018,
  title = {On Benchmarking Task Scheduling Algorithms for Heterogeneous Computing Systems},
  author = {Maurya, Ashish Kumar and Tripathi, Anil Kumar},
  year = {2018},
  month = jul,
  volume = {74},
  pages = {3039--3070},
  issn = {1573-0484},
  doi = {10.1007/s11227-018-2355-0},
  abstract = {The task scheduling problem on heterogeneous computing systems has been broadly studied, and many heuristic algorithms are proposed to solve this problem. It is interesting to go for significant performance assessment and comparison among these heuristic algorithms. In this work, we first carry out performance evaluation and comparison of task scheduling algorithms for heterogeneous computing systems for randomly generated graphs and the graphs generated from real-world applications such as Fast Fourier Transform, Gaussian Elimination, Montage and Epigenomics workflow. Further, we explores possibility of a framework for benchmarking of task scheduling algorithms for heterogeneous computing systems. This proposed approach provides for generation of graphs through a Directed Acyclic Graph generator, then produces schedules through a scheduler which makes use of scheduling algorithms and finally analyses the results obtained by using various performance metrics. The proposed framework is general in nature.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/maurya_tripathi_2018_on_benchmarking_task_scheduling_algorithms_for_heterogeneous_computing_systems.pdf},
  journal = {The Journal of Supercomputing},
  language = {en},
  number = {7}
}

@book{may2001,
  title = {Parallel {{I}}/{{O}} for High Performance Computing},
  author = {May, John M.},
  year = {2001},
  publisher = {{Morgan Kaufmann Publishers}},
  address = {{San Francisco, CA}},
  isbn = {978-1-55860-664-7},
  keywords = {Computer input-output equipment,High performance computing,Parallel processing (Electronic computers)},
  lccn = {QA76.88. M39 2001}
}

@inproceedings{meisner2012,
  title = {{{BigHouse}}: {{A}} Simulation Infrastructure for Data Center Systems},
  shorttitle = {{{BigHouse}}},
  booktitle = {2012 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems Software}}},
  author = {Meisner, D. and Wu, J. and Wenisch, T. F.},
  year = {2012},
  month = apr,
  pages = {35--45},
  doi = {10.1109/ISPASS.2012.6189204},
  abstract = {Recently, there has been an explosive growth in Internet services, greatly increasing the importance of data center systems. Applications served from ``the cloud'' are driving data center growth and quickly overtaking traditional workstations. Although there are a many tools for evaluating components of desktop and server architectures in detail, scalable modeling tools are noticeably missing. We describe BigHouse a simulation infrastructure for data center systems. Instead of simulating servers using detailed microarchitectural models, BigHouse raises the level of abstraction. Using a combination of queuing theory and stochastic modeling, BigHouse can simulate server systems in minutes rather than hours. BigHouse leverages statistical simulation techniques to limit simulation turnaround time to the minimum runtime needed for a desired accuracy. In this paper, we introduce BigHouse, describe its design, and present case studies for how it has already been applied to build and validate models of data center workloads and systems. Furthermore, we describe statistical techniques incorporated into BigHouse to accelerate and parallelize its simulations, and demonstrate its scalability to model large cluster systems while maintaining reasonable simulation time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/meisner_et_al_2012_bighouse.pdf;/Users/rwb/Zotero/storage/EXHVLAG3/6189204.html},
  keywords = {Analytical models,BigHouse,Calibration,cloud,cloud computing,cluster system,computer centres,data center system,Data models,desktop architecture,Internet service,Load modeling,modeling tool,Object oriented modeling,queueing theory,queuing theory,server architecture,Servers,simulation infrastructure,statistical analysis,statistical simulation technique,stochastic modeling,stochastic processes}
}

@article{mika2008,
  title = {Tabu Search for Multi-Mode Resource-Constrained Project Scheduling with Schedule-Dependent Setup Times},
  author = {Mika, Marek and Walig{\'o}ra, Grzegorz and W{\k{e}}glarz, Jan},
  year = {2008},
  month = jun,
  volume = {187},
  pages = {1238--1250},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2006.06.069},
  abstract = {In this paper, a multi-mode resource-constrained project scheduling problem with schedule-dependent setup times is considered. A schedule-dependent setup time is defined as a setup time dependent on the assignment of resources to activities over time, when resources are, e.g., placed in different locations. In such a case, the time necessary to prepare the required resource for processing an activity depends not only on the sequence of activities but, more generally, on the locations in which successive activities are executed. Activities are non-preemptable, resources are renewable, and the objective is to minimize the project duration. A local search metaheuristic\textemdash tabu search is proposed to solve this strongly NP-hard problem, and it is compared with the multi-start iterative improvement method as well as with random sampling. A computational experiment is described, performed on a set of instances based on standard test problems constructed by the ProGen project generator. The algorithms are computationally compared, the results are analyzed and discussed, and some conclusions are given.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/mika_et_al_2008_tabu_search_for_multi-mode_resource-constrained_project_scheduling_with.pdf;/Users/rwb/Zotero/storage/TZFKS6RY/S0377221706008344.html},
  journal = {European Journal of Operational Research},
  keywords = {Multi-mode resource-constrained project scheduling problem,Project scheduling,Setup time,Tabu search},
  number = {3}
}

@article{mishra2017,
  title = {A {{Review}} and {{Classification}} of {{Grid Computing Systems}}},
  author = {Mishra, Manoj Kumar and Patel, Yashwant Singh and Ghosh, Moumita and Mund, G B},
  year = {2017},
  volume = {13},
  pages = {34},
  issn = {0973-1873},
  abstract = {Grid Computing is used for high throughput computing at a lower-cost. It has the ability to accumulate the power of geographically scattered and heterogeneous resources to form a cohesive resource for performing higher level computations. Today's multidimensional and multidomain applications need multiple resources with diverse characteristics, i.e. huge computational power, larger storage capacity, higher processing speed and quicker accessibility of data. In recent past, many Grid Systems emerged in the form of various projects to solve many real-world complex problems. The resources required for efficient and reliable execution of the applications should be selected in accordance with the application requirements. This may be possible by classifying the Grid resources based on their major characteristics. The distinction of various Grid Systems into different categories aims towards supporting resource selection process in accordance with the application requirements. This paper presents a review and the state-of-the-art classification of Grid Systems in the perspective of scheduling and selection. A sequence of Grid System classification is presented based on Application type, Functionality, Scale, and Scope, including a brief study on each type. Finally, concluding remarks provides an assessment of various Grid Systems with the help of information collected from different sources.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/mishra_et_al_2017_a_review_and_classification_of_grid_computing_systems.pdf},
  journal = {International Journal of Computational Intelligence Research},
  language = {en},
  number = {3}
}

@article{mnih2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {529--533},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  abstract = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  copyright = {2015 Nature Publishing Group},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/mnih_et_al_2015_human-level_control_through_deep_reinforcement_learning.pdf;/Users/rwb/Zotero/storage/UDCWWXPK/nature14236.html},
  journal = {Nature},
  language = {en},
  number = {7540}
}

@article{mohammadi2018,
  ids = {mohammadi2018a},
  title = {Integer Linear Programming-Based Cost Optimization for Scheduling Scientific Workflows in Multi-Cloud Environments},
  author = {Mohammadi, Somayeh and Pedram, Hossein and PourKarimi, Latif},
  year = {2018},
  month = jun,
  pages = {1--29},
  issn = {0920-8542, 1573-0484},
  doi = {10.1007/s11227-018-2465-8},
  abstract = {Given that multi-cloud environments contain considerably diverse resources, scheduling workflows in these environments significantly reduces financial costs and overcomes the resource limitations imposed by commercial cloud providers. Accordingly, this study addressed the problem of scientific workflow scheduling in multi-cloud settings under deadline constraint to minimize associated financial costs. To this end, we proposed integer linear programming models that can be solved in a reasonable time by available solvers. In a mathematical model, the objective of a problem and real and physical constraints or restrictions are formulated using exact mathematical functions. Such formulation enabled us to comprehensively understand the system under evaluation, consider secondary preferences and post-optimality analysis and apply useful revisions to inappropriately selected input data. We analyzed the treatment of optimal cost under variations in deadline and workflow size. As part of the post-optimality analysis, sensitivity analysis and deadline revision were implemented. Results indicated that our proposed approach outperforms previously developed methods in terms of financial cost reduction.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/mohammadi_et_al_2018_integer_linear_programming-based_cost_optimization_for_scheduling_scientific.pdf;/Users/rwb/Dropbox/PhD/zotero/2018/mohammadi_et_al_2018_integer_linear_programming-based_cost_optimization_for_scheduling_scientific2.pdf;/Users/rwb/Zotero/storage/KVIUMW5L/s11227-018-2465-8.html},
  journal = {The Journal of Supercomputing},
  language = {en}
}

@article{monette,
  title = {Just-{{In}}-{{Time Scheduling}} with {{Constraint Programming}}},
  author = {Monette, Jean-Noel and Deville, Yves and Hentenryck, Pascal Van},
  pages = {8},
  abstract = {This paper considers Just-In-Time Job-Shop Scheduling, in which each activity has an earliness and a tardiness cost with respect to a due date. It proposes a constraint programming approach, which includes a novel filtering algorithm and dedicated heuristics. The filtering algorithm uses a machine relaxation to produce a lower bound that can be obtained by solving a Just-In-Time Pert problem. It also includes pruning rules which update the variable bounds and detect precedence constraints. The paper presents experimental results which demonstrate the effectiveness of the approach over a wide range of benchmarks.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/monette_et_al_just-in-time_scheduling_with_constraint_programming.pdf},
  language = {en}
}

@inproceedings{moradi2016,
  title = {A Centralized Reinforcement Learning Method for Multi-Agent Job Scheduling in {{Grid}}},
  booktitle = {2016 6th {{International Conference}} on {{Computer}} and {{Knowledge Engineering}} ({{ICCKE}})},
  author = {Moradi, Milad},
  year = {2016},
  month = oct,
  pages = {171--176},
  publisher = {{IEEE}},
  address = {{Mashhad, Iran}},
  doi = {10.1109/ICCKE.2016.7802135},
  abstract = {One of the main challenges in Grid systems is designing an adaptive, scalable, and model-independent method for job scheduling to achieve a desirable degree of load balancing and system efficiency. Centralized job scheduling methods have some drawbacks, such as single point of failure and lack of scalability. Moreover, decentralized methods require a coordination mechanism with limited communications. In this paper, we propose a multi-agent approach to job scheduling in Grid, named Centralized Learning Distributed Scheduling (CLDS), by utilizing the reinforcement learning framework. The CLDS is a model free approach that uses the information of jobs and their completion time to estimate the efficiency of resources. In this method, there are a learner agent and several scheduler agents that perform the task of learning and job scheduling with the use of a coordination strategy that maintains the communication cost at a limited level. We evaluated the efficiency of the CLDS method by designing and performing a set of experiments on a simulated Grid system under different system scales and loads. The results show that the CLDS can effectively balance the load of system even in large scale and heavy loaded Grids, while maintains its adaptive performance and scalability.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/moradi_2016_a_centralized_reinforcement_learning_method_for_multi-agent_job_scheduling_in.pdf},
  isbn = {978-1-5090-3586-1},
  language = {en}
}

@article{moreno2002,
  title = {Integrating {{AI}} Planning Techniques with Workflow Management System},
  author = {Moreno, M. D. R. - and Kearney, P.},
  year = {2002},
  month = jul,
  volume = {15},
  pages = {285--291},
  issn = {0950-7051},
  doi = {10.1016/S0950-7051(01)00167-8},
  abstract = {There is a variety of applications that can benefit from the ability to find optimal or good solutions to a proposed problem, automatically. The artificial intelligent (AI) community has been actively involved in efficient problem-solving in complex domains such as military or spacecraft problems with successful results. In this paper, we describe the integration of AI planning techniques with an existing workflow management system. We show how these techniques can improve the overall system functionality and help automate the definition of business processes. The work is based on a short study carried out at BT research laboratories as part of a larger programme that aims to provide technologies for a new generation of business support systems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2002/moreno_kearney_2002_integrating_ai_planning_techniques_with_workflow_management_system.pdf;/Users/rwb/Zotero/storage/5YGE6TNV/S0950705101001678.html},
  journal = {Knowledge-Based Systems},
  keywords = {AI Planning techniques,Contingent planners,Workflow Management Systems},
  number = {5}
}

@article{mu2013,
  title = {Disrupted Capacitated Vehicle Routing Problem with Order Release Delay},
  author = {Mu, Qianxin and Eglese, Richard W.},
  year = {2013},
  month = aug,
  volume = {207},
  pages = {201--216},
  issn = {1572-9338},
  doi = {10.1007/s10479-011-0947-7},
  abstract = {With the popularity of the just-in-time system, more and more companies are operating with little or no inventories, which make them highly vulnerable to delays on supply. This paper discusses a situation when the supply of the commodity does not arrive at the depot on time, so that not enough of the commodity is available to be loaded on all vehicles at the start of the delivery period. New routing plans need to be developed in such a case to reduce the impact the delay of supply may have on the distribution company. The resulting vehicle routing problem is different from other types of vehicle routing problems as it involves waiting and multiple trips. Two approaches have been developed to solve the order release delay problem, both of which involve a Tabu Search algorithm. Computational results show the proposed approaches can largely reduce the disruption costs that are caused by the delayed supply and they are especially effective when the length of delay is long.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/mu_eglese_2013_disrupted_capacitated_vehicle_routing_problem_with_order_release_delay.pdf},
  journal = {Annals of Operations Research},
  language = {en},
  number = {1}
}

@article{mu2013a,
  title = {Disrupted Capacitated Vehicle Routing Problem with Order Release Delay},
  author = {Mu, Qianxin and Eglese, Richard W.},
  year = {2013},
  month = aug,
  volume = {207},
  pages = {201--216},
  issn = {1572-9338},
  doi = {10.1007/s10479-011-0947-7},
  abstract = {With the popularity of the just-in-time system, more and more companies are operating with little or no inventories, which make them highly vulnerable to delays on supply. This paper discusses a situation when the supply of the commodity does not arrive at the depot on time, so that not enough of the commodity is available to be loaded on all vehicles at the start of the delivery period. New routing plans need to be developed in such a case to reduce the impact the delay of supply may have on the distribution company. The resulting vehicle routing problem is different from other types of vehicle routing problems as it involves waiting and multiple trips. Two approaches have been developed to solve the order release delay problem, both of which involve a Tabu Search algorithm. Computational results show the proposed approaches can largely reduce the disruption costs that are caused by the delayed supply and they are especially effective when the length of delay is long.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/mu_eglese_2013_disrupted_capacitated_vehicle_routing_problem_with_order_release_delay2.pdf},
  journal = {Annals of Operations Research},
  language = {en},
  number = {1}
}

@article{muhuri2018,
  title = {On Arrival Scheduling of Real-Time Precedence Constrained Tasks on Multi-Processor Systems Using Genetic Algorithm},
  author = {Muhuri, Pranab K and Rauniyar, Amit and Nath, Rahul},
  year = {2018},
  month = oct,
  issn = {0167-739X},
  doi = {10.1016/j.future.2018.10.013},
  abstract = {This paper models the problem of precedence based real-time task scheduling as a dynamic constraint problem. It presents a new scheduling approach, termed as 'Dynamic Genetic Algorithm for Real-Time Scheduling' (dGA-RTS). The significant feature of dGA-RTS is that it can handle dynamic as well as static scheduling of inter-dependent tasks for real-time systems. In parallel or distributed systems, the main aim of dynamic task scheduling is to allocate processors to a given set of tasks to execute them within optimized completion times without violating the task dependencies, if any. The dGA-RTS schedules the tasks in the waiting list when any new task arrives in the scheduler and minimizes the overall schedule length of the task set ensuring deadline compliance. We also illustrate an implementation technique to deal with synchronization problems in multi-processor systems. In order to exhibit the applicability of our approach, we perform experiments with suitable benchmark as well as synthetic test cases. Further, we conduct extensive simulations and compare the results with different performance metrics. The comparative study of the results with existing approaches indicates that our proposed approach is more efficient in generating feasible solutions},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/muhuri_et_al_2018_on_arrival_scheduling_of_real-time_precedence_constrained_tasks_on.pdf;/Users/rwb/Zotero/storage/F33EV2ZL/S0167739X18312810.html},
  journal = {Future Generation Computer Systems},
  keywords = {Arrival time,Dynamic task scheduling,Genetic algorithms,Precedence constraints,Real-time multiprocessor systems,Task allocation}
}

@article{myers1999,
  title = {At the {{Boundary}} of {{Workflow}} and {{AI}}},
  author = {Myers, Karen L and Berry, Pauline M},
  year = {1999},
  pages = {9},
  abstract = {Many domains of interest to the workflow community are characterized by ever-changing requirements and unpredictable environments. Workflow systems must increase in sophistication to provide the reactivity and flexibility necessary for process management under such dynamic conditions. This paper describes how techniques from the AI community, specifically reactive control, planning, and scheduling, could be leveraged to develop powerful, next-generation adaptive workflow engines that provide many of the these advanced process management capabilities. Although motivated by somewhat different concerns and grounded in different perspectives, there is much overlap between the objectives and requirements of these two communities of workflow management and Artificial Intelligence. Two systems under development by the authors which embrace this synergy are described.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1999/myers_berry_1999_at_the_boundary_of_workflow_and_ai.pdf},
  journal = {AAAI Technical Report WS-99-02},
  language = {en}
}

@misc{nagai2018,
  title = {{{ALMA Propers Guide}}},
  author = {Nagai, Hiroshi and Remijan, Anthony and Zwaan, Martin},
  year = {2018},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/nagai_et_al_2018_alma_propers_guide.pdf},
  howpublished = {https://almascience.nrao.edu/proposing/proposers-guide},
  language = {en}
}

@article{nanda2014,
  title = {A Survey on Nature Inspired Metaheuristic Algorithms for Partitional Clustering},
  author = {Nanda, Satyasai Jagannath and Panda, Ganapati},
  year = {2014},
  month = jun,
  volume = {16},
  pages = {1--18},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2013.11.003},
  abstract = {The partitional clustering concept started with K-means algorithm which was published in 1957. Since then many classical partitional clustering algori\ldots},
  file = {/Users/rwb/Zotero/storage/GSP7Q9PV/S221065021300076X.html},
  journal = {Swarm and Evolutionary Computation},
  language = {en}
}

@inproceedings{nethercote2007,
  title = {{{MiniZinc}}: {{Towards}} a {{Standard CP Modelling Language}}},
  shorttitle = {{{MiniZinc}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} \textendash{} {{CP}} 2007},
  author = {Nethercote, Nicholas and Stuckey, Peter J. and Becket, Ralph and Brand, Sebastian and Duck, Gregory J. and Tack, Guido},
  editor = {Bessi{\`e}re, Christian},
  year = {2007},
  pages = {529--543},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {There is no standard modelling language for constraint programming (CP) problems. Most solvers have their own modelling language. This makes it difficult for modellers to experiment with different solvers for a problem.In this paper we present MiniZinc, a simple but expressive CP modelling language which is suitable for modelling problems for a range of solvers and provides a reasonable compromise between many design possibilities. Equally importantly, we also propose a low-level solver-input language called FlatZinc, and a straightforward translation from MiniZinc to FlatZinc that preserves all solver-supported global constraints. This lets a solver writer support MiniZinc with a minimum of effort\textemdash they only need to provide a simple FlatZinc front-end to their solver, and then combine it with an existing MiniZinc-to-FlatZinc translator. Such a front-end may then serve as a stepping stone towards a full MiniZinc implementation that is more tailored to the particular solver.A standard language for modelling CP problems will encourage experimentation with and comparisons between different solvers. Although MiniZinc is not perfect\textemdash no standard modelling language will be\textemdash we believe its simplicity, expressiveness, and ease of implementation make it a practical choice for a standard language.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/nethercote_et_al_2007_minizinc.pdf},
  isbn = {978-3-540-74970-7},
  keywords = {Array Access,Element Constraint,Global Constraint,Modelling Language,Standard Language},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{netto2018,
  title = {{{HPC Cloud}} for {{Scientific}} and {{Business Applications}}: {{Taxonomy}}, {{Vision}}, and {{Research Challenges}}},
  shorttitle = {{{HPC Cloud}} for {{Scientific}} and {{Business Applications}}},
  author = {Netto, Marco A. S. and Calheiros, Rodrigo N. and Rodrigues, Eduardo R. and Cunha, Renato L. F. and Buyya, Rajkumar},
  year = {2018},
  month = jan,
  volume = {51},
  pages = {1--29},
  issn = {03600300},
  doi = {10.1145/3150224},
  abstract = {High Performance Computing (HPC) clouds are becoming an alternative to on-premise clusters for executing scientific applications and business analytics services. Most research efforts in HPC cloud aim to understand the cost-benefit of moving resource-intensive applications from on-premise environments to public cloud platforms. Industry trends show hybrid environments are the natural path to get the best of the on-premise and cloud resources---steady (and sensitive) workloads can run on on-premise resources and peak demand can leverage remote resources in a pay-as-you-go manner. Nevertheless, there are plenty of questions to be answered in HPC cloud, which range from how to extract the best performance of an unknown underlying platform to what services are essential to make its usage easier. Moreover, the discussion on the right pricing and contractual models to fit small and large users is relevant for the sustainability of HPC clouds. This paper brings a survey and taxonomy of efforts in HPC cloud and a vision on what we believe is ahead of us, including a set of research challenges that, once tackled, can help advance businesses and scientific discoveries. This becomes particularly relevant due to the fast increasing wave of new HPC applications coming from big data and artificial intelligence.},
  archivePrefix = {arXiv},
  eprint = {1710.08731},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/netto_et_al_2018_hpc_cloud_for_scientific_and_business_applications.pdf},
  journal = {ACM Computing Surveys},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  language = {en},
  number = {1}
}

@article{nguyen2016,
  title = {Throughput-{{Driven Partitioning}} of {{Stream Programs}} on {{Heterogeneous Distributed Systems}}},
  author = {Nguyen, V. T. N. and Kirner, R.},
  year = {2016},
  month = mar,
  volume = {27},
  pages = {913--926},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2015.2416726},
  abstract = {Graph partitioning is an important problem in computer science and is of NP-hard complexity. In practice it is usually solved using heuristics. In this article we introduce the use of graph partitioning to partition the workload of stream programs to optimise the throughput on heterogeneous distributed platforms. Existing graph partitioning heuristics are not adequate for this problem domain. In this article we present two new heuristics to capture the problem space of graph partitioning for stream programs to optimise throughput. The first algorithm is an adaptation of the well-known Kernighan-Lin algorithm, called KL-Adapted (KLA), which is relatively slow. As a second algorithm we have developed the Congestion Avoidance (CA) partitioning algorithm, which performs reconfiguration moves optimised to our problem type. We compare both KLA and CA with the generic meta-heuristic Simulated Annealing (SA). All three methods achieve similar throughput results for most cases, but with significant differences in calculation time. For small graphs KLA is faster than SA, but KLA is slower for larger graphs. CA on the other hand is always orders of magnitudes faster than both KLA and SA, even for large graphs. This makes CA potentially useful for re-partitioning of systems during runtime.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/nguyen_kirner_2016_throughput-driven_partitioning_of_stream_programs_on_heterogeneous_distributed.pdf;/Users/rwb/Zotero/storage/R7T38BVP/7069261.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {CA partitioning algorithm,computational complexity,congestion avoidance partitioning algorithm,Cost function,distributed processing,graph partitioning,graph theory,heterogeneous distributed system,Kernighan-Lin algorithm,KLA,NP-hard complexity,Partitioning algorithms,Programming,SA,Schedules,simulated annealing,stream program,Stream programs; throughput optimisation; graph partitioning; simulated annealing,Streaming media,Throughput,throughput-driven partitioning},
  number = {3}
}

@article{nijboer,
  title = {Parametric {{Model}} and {{Analysis}}},
  author = {Nijboer, Ronald},
  pages = {47},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/nijboer_parametric_model_and_analysis.pdf},
  language = {en}
}

@article{nikhil1990,
  title = {Executing a Program on the {{MIT}} Tagged-Token Dataflow Architecture},
  author = {Nikhil, RS and others},
  year = {1990},
  volume = {39},
  pages = {300--318},
  journal = {IEEE Transactions on computers},
  number = {3}
}

@inproceedings{nikravesh2014,
  title = {Cloud {{Resource Auto}}-Scaling {{System Based}} on {{Hidden Markov Model}} ({{HMM}})},
  booktitle = {2014 {{IEEE International Conference}} on {{Semantic Computing}}},
  author = {Nikravesh, A. Y. and Ajila, S. A. and Lung, C.},
  year = {2014},
  month = jun,
  pages = {124--127},
  doi = {10.1109/ICSC.2014.43},
  abstract = {The elasticity characteristic of cloud computing enables clients to acquire and release resources on demand. This characteristic reduces clients' cost by making them pay for the resources they actually have used. On the other hand, clients are obligated to maintain Service Level Agreement (SLA) with their users. One approach to deal with this cost-performance trade-off is employing an auto-scaling system which automatically adjusts application's resources based on its load. In this paper we have proposed an auto-scaling system based on Hidden Markov Model (HMM). We have conducted an experiment on Amazon EC2 infrastructure to evaluate our model. Our results show HMM can generate correct scaling actions in 97\% of time. CPU utilization, throughput, and response time are being considered as performance metrics in our experiment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/nikravesh_et_al_2014_cloud_resource_auto-scaling_system_based_on_hidden_markov_model_(hmm).pdf;/Users/rwb/Zotero/storage/6K57HG9Q/6882012.html},
  keywords = {Accuracy,Amazon EC2 infrastructure,auto-scaling system,cloud computing,Cloud computing,cloud computing elasticity characteristic,cloud resource auto-scaling system,Computational modeling,CPU utilization metric,Databases,hidden Markov model,Hidden Markov model,hidden Markov models,Hidden Markov models,HMM,Learning (artificial intelligence),Measurement,proactive auto-scaling,resource allocation,resource provisioning,response time metric,scaling actions,service level agreement,SLA,throughput metric}
}

@misc{nvidiacorporation2018,
  title = {Achieved {{FLOPs}}},
  author = {NVIDIA Corporation},
  year = {2018},
  file = {/Users/rwb/Zotero/storage/U2P44DHB/achievedflops.html},
  howpublished = {https://docs.nvidia.com/gameworks/content/developertools/desktop/nsight/analysis/report/cudaexperiments/kernellevel/achievedflops.htm},
  journal = {Achieved FLOPs}
}

@inproceedings{olston2011,
  title = {Nova: Continuous {{Pig}}/{{Hadoop}} Workflows},
  shorttitle = {Nova},
  booktitle = {Proceedings of the 2011 International Conference on {{Management}} of Data - {{SIGMOD}} '11},
  author = {Olston, Christopher and Seth, Siddharth and Tian, Chao and ZiCornell, Topher and Wang, Xiaodan and Chiou, Greg and Chitnis, Laukik and Liu, Francis and Han, Yiping and Larsson, Mattias and Neumann, Andreas and Rao, Vellanki B.N. and Sankarasubramanian, Vijayanand},
  year = {2011},
  pages = {1081},
  publisher = {{ACM Press}},
  address = {{Athens, Greece}},
  doi = {10.1145/1989323.1989439},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/olston_et_al_2011_nova.pdf},
  isbn = {978-1-4503-0661-4},
  language = {en}
}

@article{orhean2018,
  title = {New Scheduling Approach Using Reinforcement Learning for Heterogeneous Distributed Systems},
  author = {Orhean, Alexandru Iulian and Pop, Florin and Raicu, Ioan},
  year = {2018},
  month = jul,
  volume = {117},
  pages = {292--302},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2017.05.001},
  abstract = {Computer clusters, cloud computing and the exploitation of parallel architectures and algorithms have become the norm when dealing with scientific applications that work with large quantities of data and perform complex and time-consuming calculations. With the rise of social media applications and smart devices, the amount of digital data and the velocity at which it is produced have increased exponentially, determining the development of distributed system frameworks and platforms that increase productivity, consistency, fault-tolerance and security of parallel applications. The performance of such systems is mainly influenced by the architectural disposition and composition of the physical machines, the resource allocation and the scheduling of jobs and tasks. This paper proposes a reinforcement learning algorithm to solve the scheduling problem in distributed systems. The machine learning technique takes into consideration the heterogeneity of the nodes and their disposition within the grid, and the arrangement of tasks in a directed acyclic graph of dependencies, ultimately determining a scheduling policy for a better execution time. This paper also proposes a platform, in which the algorithm is implemented, that offers scheduling as a service to distributed systems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/orhean_et_al_2018_new_scheduling_approach_using_reinforcement_learning_for_heterogeneous.pdf;/Users/rwb/Zotero/storage/DWIKXXJA/S0743731517301521.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {Distributed systems,Machine learning,SARSA,Scheduling}
}

@inproceedings{page2005,
  title = {Dynamic {{Task Scheduling}} Using {{Genetic Algorithms}} for {{Heterogeneous Distributed Computing}}},
  booktitle = {19th {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Page, A. J. and Naughton, T. J.},
  year = {2005},
  month = apr,
  pages = {189a-189a},
  doi = {10.1109/IPDPS.2005.184},
  abstract = {An algorithm has been developed to dynamically schedule heterogeneous tasks on heterogeneous processors in a distributed system. The scheduler operates in an environment with dynamically changing resources and adapts to variable system resources. It operates in a batch fashion and utilises a genetic algorithm to minimise the total execution time. We have compared our scheduler to six other schedulers, three batch-mode and three immediate-mode schedulers. We have performed simulations with randomly generated task sets, using uniform, normal, and Poisson distributions, whilst varying the communication overheads between the clients and scheduler. We have achieved more efficient results than all other schedulers across a range of different scenarios while scheduling 10,000 tasks on up to 50 heterogeneous processors.},
  file = {/Users/rwb/Zotero/storage/RPBI8GR4/1420076.html},
  keywords = {batch processing,batch processing (computers),Computer science,Distributed computing,Dynamic scheduling,dynamic task scheduling,genetic algorithm,genetic algorithms,Genetic algorithms,heterogeneous distributed computing,Heuristic algorithms,normal distribution,NP-hard problem,Optimal scheduling,Poisson distribution,Processor scheduling,resource allocation,Resource management,scheduling,Scheduling algorithm,uniform distribution,Unread}
}

@article{page2005a,
  ids = {page2005b},
  title = {Framework for {{Task Scheduling}} in {{Heterogeneous Distributed Computing Using Genetic Algorithms}}},
  author = {Page, Andrew J. and Naughton, Thomas J.},
  year = {2005},
  month = nov,
  volume = {24},
  pages = {415--429},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-005-9002-x},
  abstract = {An algorithm has been developed to dynamically schedule heterogeneous tasks on heterogeneous processors in a distributed system. The scheduler operates in an environment with dynamically changing resources and adapts to variable system resources. It operates in a batch fashion and utilises a genetic algorithm to minimise the total execution time. We have compared our scheduler to six other schedulers, three batch-mode and three immediate-mode schedulers. Experiments show that the algorithm outperforms each of the others and can achieve near optimal efficiency, with up to 100,000 tasks being scheduled.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/page_naughton_2005_framework_for_task_scheduling_in_heterogeneous_distributed_computing_using.pdf},
  journal = {Artificial Intelligence Review},
  keywords = {distributed computing,genetic algorithms,task scheduling,Unread},
  language = {en},
  number = {3-4}
}

@incollection{pandey2014,
  title = {Scalable {{Deployment}} of a {{LIGO Physics Application}} on {{Public Clouds}}: {{Workflow Engine}} and {{Resource Provisioning Techniques}}},
  shorttitle = {Scalable {{Deployment}} of a {{LIGO Physics Application}} on {{Public Clouds}}},
  booktitle = {Cloud {{Computing}} for {{Data}}-{{Intensive Applications}}},
  author = {Pandey, Suraj and Sammut, Letizia and Calheiros, Rodrigo N. and Melatos, Andrew and Buyya, Rajkumar},
  editor = {Li, Xiaolin and Qiu, Judy},
  year = {2014},
  pages = {3--25},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-1905-5_1},
  abstract = {Cloud computing has empowered users to provision virtually unlimited computational resources and are accessible over the Internet on demand. This makes Cloud computing a compelling technology that tackles the issues rising with the growing size and complexity of scientific applications, which are characterized by high variance in usage, large volume of data and high compute load, flash crowds, unpredictable load, and varying compute and storage requirements. In order to provide users an automated and scalable platform for hosting scientific workflow applications, while hiding the complexity of the underlying Cloud infrastructure, we present the design and implementation of a PaaS middleware solution along with resource provisioning techniques. We apply our PaaS solution to the data analysis pipeline of a physics application, a gravitational wave search, utilizing public Clouds. The system architecture, a load-balancing approach, and the system's behavior over varying loads are detailed. The performance evaluation on scalability and load-balancing characteristics of the automated PaaS middleware demonstrates the feasibility and advantages of the approach over existing monolithic approaches.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/pandey_et_al_2014_scalable_deployment_of_a_ligo_physics_application_on_public_clouds.pdf},
  isbn = {978-1-4939-1904-8 978-1-4939-1905-5},
  language = {en}
}

@inproceedings{pathania2016,
  title = {Distributed {{Scheduling}} for {{Many}}-Cores {{Using Cooperative Game Theory}}},
  booktitle = {Proceedings of the 53rd {{Annual Design Automation Conference}}},
  author = {Pathania, Anuj and Venkataramani, Vanchinathan and Shafique, Muhammad and Mitra, Tulika and Henkel, J{\"o}rg},
  year = {2016},
  pages = {133:1--133:6},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2897937.2898009},
  abstract = {Many-cores are envisaged to include hundreds of processing cores etched on to a single die and will execute tens of multi-threaded tasks in parallel to exploit their massive parallel processing potential. A task can be sped up by assigning it to more than one core. Moreover, processing requirements of tasks are in a constant state of flux and some of the cores assigned to a task entering a low processing requirement phase can be transferred to a task entering high requirement phase, maximizing overall performance of the system. This scheduling problem of partial core reallocations can be solved optimally in polynomial time using a dynamic programming based scheduler. Dynamic programming is an inherently centralized algorithm that uses only one of the available cores for scheduling-related computations and hence is not scalable. In this work, we introduce a distributed scheduler that disburses all scheduling-related computations throughout the many-core allowing it to scale up. We prove that our proposed scheduler is optimal and hence converges to the same solution as the centralized optimal scheduler. Our simulations show that the proposed distributed scheduler can result in 1000x reduction in per-core processing overhead in comparison to the centralized scheduler and hence is more suited for scheduling on many-cores.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/pathania_et_al_2016_distributed_scheduling_for_many-cores_using_cooperative_game_theory.pdf},
  isbn = {978-1-4503-4236-0},
  keywords = {_tablet,distributed scheduling,many-core,multi-agent systems},
  series = {{{DAC}} '16}
}

@misc{pegasus2014,
  title = {{{WorkflowGenerator}}},
  author = {Pegasus},
  year = {2014},
  month = oct
}

@article{pemberton,
  title = {{{ON THE NEED FOR DYNAMIC SCHEDULING OF IMAGING SATELLITES}}},
  author = {Pemberton, J C and Greenwald, L G},
  pages = {6},
  abstract = {Imaging satellites are traditionally scheduled in a static fashion; namely the schedule is created off-line and then uploaded to one or more imaging satellites to be executed as an immutable sequence of commands. In this paper, we make the case for dynamic scheduling of imaging satellites. Dynamic schedules will allow satellite systems to take advantage of information gathered during the execution of the schedule and react to changes in the environment, desired tasking, and the availability of resources. We develop the remote sensing scheduling problem and discuss contingency conditions under which the satellite scheduling problem becomes dynamic. We then review existing work on contingency scheduling and conditional scheduling and propose extensions to address the dynamic satellite scheduling problem. Dynamic schedules will yield improved mission schedules and reduced mission costs.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/pemberton_greenwald_on_the_need_for_dynamic_scheduling_of_imaging_satellites.pdf},
  language = {en}
}

@article{peng2015,
  title = {Random Task Scheduling Scheme Based on Reinforcement Learning in Cloud Computing},
  author = {Peng, Zhiping and Cui, Delong and Zuo, Jinglong and Li, Qirui and Xu, Bo and Lin, Weiwei},
  year = {2015},
  month = dec,
  volume = {18},
  pages = {1595--1607},
  issn = {1573-7543},
  doi = {10.1007/s10586-015-0484-2},
  abstract = {Task scheduling is a necessary prerequisite for performance optimization and resource management in the cloud computing system. Focusing on accurate scaled cloud computing environment and efficient task scheduling under resource constraints problems, we introduce fine-grained cloud computing system model and optimization task scheduling scheme in this paper. The system model is comprised of clearly defined separate submodels including task schedule submodel, task execute submodel and task transmission submodel, so that they can be accurately analyzed in the order of processing of user requests. Moreover the submodels are scalable enough to capture the flexibility of the cloud computing paradigm. By analyzing the submodels, where results are repeated to obtain sufficient accuracy, we design a novel task scheduling scheme based on reinforcement learning and queuing theory to optimize task scheduling under the resource constraints, and the state aggregation technologies is employed to accelerate the learning progress. Our results, on the one hand, demonstrate the efficiency of the task scheduling scheme and, on the other hand, reveal the relationship between the arrival rate, server rate, number of VMs and the number of buffer size.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/peng_et_al_2015_random_task_scheduling_scheme_based_on_reinforcement_learning_in_cloud_computing.pdf},
  journal = {Cluster Computing},
  keywords = {Cloud computing,Queuing theory,Reinforcement learning,State aggregation,Task scheduling},
  language = {en},
  number = {4}
}

@article{peng2018,
  title = {Sim-to-{{Real Transfer}} of {{Robotic Control}} with {{Dynamics Randomization}}},
  author = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2018},
  month = may,
  pages = {1--8},
  doi = {10.1109/ICRA.2018.8460528},
  abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
  archivePrefix = {arXiv},
  eprint = {1710.06537},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/peng_et_al_2018_sim-to-real_transfer_of_robotic_control_with_dynamics_randomization.pdf;/Users/rwb/Zotero/storage/6WECRRSZ/1710.html},
  journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords = {Computer Science - Robotics,Computer Science - Systems and Control}
}

@article{perez2015,
  title = {Multiobjective {{Monte Carlo Tree Search}} for {{Real}}-{{Time Games}}},
  author = {Perez, D. and Mostaghim, S. and Samothrakis, S. and Lucas, S. M.},
  year = {2015},
  month = dec,
  volume = {7},
  pages = {347--360},
  issn = {1943-068X},
  doi = {10.1109/TCIAIG.2014.2345842},
  abstract = {Multiobjective optimization has been traditionally a matter of study in domains like engineering or finance, with little impact on games research. However, action-decision based on multiobjective evaluation may be beneficial in order to obtain a high quality level of play. This paper presents a multiobjective Monte Carlo tree search algorithm for planning and control in real-time game domains, those where the time budget to decide the next move to make is close to 40 ms. A comparison is made between the proposed algorithm, a single-objective version of Monte Carlo tree search and a rolling horizon implementation of nondominated sorting evolutionary algorithm II (NSGA-II). Two different benchmarks are employed, deep sea treasure (DST) and the multiobjective physical traveling salesman problem (MO-PTSP). Using the same heuristics on each game, the analysis is focused on how well the algorithms explore the search space. Results show that the algorithm proposed outperforms NSGA-II. Additionally, it is also shown that the algorithm is able to converge to different optimal solutions or the optimal Pareto front (if achieved during search).},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/perez_et_al_2015_multiobjective_monte_carlo_tree_search_for_real-time_games.pdf;/Users/rwb/Zotero/storage/H8YHSAHT/6872573.html},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {action-decision,Approximation algorithms,computer games,deep sea treasure,DST,Equations,evolutionary computation,game heuristics,Games,MO-PTSP,Monte Carlo methods,Monte Carlo tree search,multiobjective evaluation,multiobjective Monte Carlo tree search algorithm,multiobjective optimization,multiobjective physical traveling salesman problem,nondominated sorting evolutionary algorithm II,NSGA-II,optimal Pareto front,optimal solutions,Optimization,real-time game domains control,real-time game domains planning,real-time games,Real-time systems,rolling horizon implementation,sorting,travelling salesman problems,tree searching,Vectors},
  number = {4}
}

@inproceedings{perret2016,
  title = {Mapping {{Hard Real}}-Time {{Applications}} on {{Many}}-Core {{Processors}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Real}}-{{Time Networks}} and {{Systems}}},
  author = {Perret, Quentin and Maur{\`e}re, Pascal and Noulard, {\'E}ric and Pagetti, Claire and Sainrat, Pascal and Triquet, Beno{\^i}t},
  year = {2016},
  pages = {235--244},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2997465.2997496},
  abstract = {Many-core processors are interesting candidates for the design of modern avionics computers. Indeed, the computational power offered by such platforms opens new horizons to design more demanding systems and to integrate more applications on a single target. However, they also bring challenging research topics because of their lack of predictability and their programming complexity. In this paper, we focus on the problem of mapping large applications on a complex platform such as the KALRAY MPPA\textregistered -256 while maintaining a strong temporal isolation from co-running applications. We propose a constraint programming formulation of the mapping problem that enables an efficient parallelization and we demonstrate the ability of our approach to deal with large problems using a real world case study.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/perret_et_al_2016_mapping_hard_real-time_applications_on_many-core_processors.pdf},
  isbn = {978-1-4503-4787-7},
  series = {{{RTNS}} '16}
}

@book{pinedo2012,
  title = {Scheduling: Theory, Algorithms, and Systems},
  shorttitle = {Scheduling},
  author = {Pinedo, Michael},
  year = {2012},
  edition = {4th ed},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/pinedo_2012_scheduling.pdf},
  isbn = {978-1-4614-1986-0},
  keywords = {Production scheduling},
  language = {en},
  lccn = {TS157.5 .P56 2012}
}

@article{poola2014,
  title = {Fault-Tolerant {{Workflow Scheduling}} Using {{Spot Instances}} on {{Clouds}}},
  author = {Poola, Deepak and Ramamohanarao, Kotagiri and Buyya, Rajkumar},
  year = {2014},
  month = jan,
  volume = {29},
  pages = {523--533},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2014.05.047},
  abstract = {Scientific workflows are used to model applications of high throughput computation and complex large scale data analysis. In recent years, Cloud computing is fast evolving as the target platform for such applications among researchers. Furthermore, new pricing models have been pioneered by Cloud providers that allow users to provision resources and to use them in an efficient manner with significant cost reductions. In this paper, we propose a scheduling algorithm that schedules tasks on Cloud resources using two different pricing models (spot and on-demand instances) to reduce the cost of execution whilst meeting the workflow deadline. The proposed algorithm is fault tolerant against the premature termination of spot instances and also robust against performance variations of Cloud resources. Experimental results demonstrate that our heuristic reduces up to 70\% execution cost as against using only on-demand instances.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/poola_et_al_2014_fault-tolerant_workflow_scheduling_using_spot_instances_on_clouds.pdf;/Users/rwb/Zotero/storage/WRFIVYAF/S1877050914002245.html},
  journal = {Procedia Computer Science},
  keywords = {Cloud,Fault-Tolerance,Scheduling,Spot Instances,Workflows},
  series = {2014 {{International Conference}} on {{Computational Science}}}
}

@incollection{poola2017,
  title = {Chapter 15 - {{A Taxonomy}} and {{Survey}} of {{Fault}}-{{Tolerant Workflow Management Systems}} in {{Cloud}} and {{Distributed Computing Environments}}},
  booktitle = {Software {{Architecture}} for {{Big Data}} and the {{Cloud}}},
  author = {Poola, Deepak and Salehi, Mohsen Amini and Ramamohanarao, Kotagiri and Buyya, Rajkumar},
  editor = {Mistrik, Ivan and Bahsoon, Rami and Ali, Nour and Heisel, Maritta and Maxim, Bruce},
  year = {2017},
  month = jan,
  pages = {285--320},
  publisher = {{Morgan Kaufmann}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-805467-3.00015-6},
  abstract = {During the recent years, workflows have emerged as an important abstraction for collaborative research and managing complex large-scale distributed data analytics. Workflows are increasingly becoming prevalent in various distributed environments, such as clusters, grids, and clouds. These environments provide complex infrastructures that aid workflows in scaling and parallel execution of their components. However, they are prone to performance variations and different types of failures. Thus, workflow management systems need to be robust against performance variations and tolerant against failures. Numerous research studies have investigated fault-tolerant aspect of the workflow management system in different distributed systems. In this study, we analyze these efforts and provide an in-depth taxonomy of them. We present the ontology of faults and fault-tolerant techniques then position the existing workflow management systems with respect to the taxonomies and the techniques. In addition, we classify various failure models, metrics, tools, and support systems. Finally, we identify and discuss the strengths and weaknesses of the current techniques and provide recommendations on future directions and open areas for the research community.},
  file = {/Users/rwb/Zotero/storage/GE9HA364/B9780128054673000156.html},
  isbn = {978-0-12-805467-3},
  keywords = {Algorithms,Checkpointing,Cloud computing,Distributed systems,Fault-tolerance,Task duplication,Task retry,Workflows}
}

@inproceedings{pothina2018,
  title = {{{EarthSim}}: {{Flexible Environmental Simulation Workflows Entirely Within Jupyter Notebooks}}},
  shorttitle = {{{EarthSim}}},
  booktitle = {Python in {{Science Conference}}},
  author = {Pothina, Dharhas and Rudiger, Philipp and Bednar, James and Christensen, Scott and Winters, Kevin and Pevey, Kimberly and Ball, Christopher and Brener, Gregory},
  year = {2018},
  pages = {48--55},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-4af1f417-007},
  abstract = {Building environmental simulation workflows is typically a slow process involving multiple proprietary desktop tools that do not interoperate well. In this work, we demonstrate building flexible, lightweight workflows entirely in Jupyter notebooks. We demonstrate these capabilities through examples in hydrology and hydrodynamics using the AdH (Adaptive Hydraulics) and GSSHA (Gridded Surface Subsurface Hydrologic Analysis) simulators. The goal of this work is to provide a set of tools that work well together and with the existing scientific python ecosystem, that can be used in browser based environments and that can easily be reconfigured and repurposed as needed to rapidly solve specific emerging issues such as hurricanes or dam failures.},
  file = {/Users/rwb/Zotero/storage/2HAU87LG/Pothina et al. - 2018 - EarthSim Flexible Environmental Simulation Workfl.pdf},
  language = {en}
}

@techreport{pressel2000,
  title = {Performance {{Metrics}} for {{Parallel Systems}}:},
  shorttitle = {Performance {{Metrics}} for {{Parallel Systems}}},
  author = {Pressel, D. M.},
  year = {2000},
  month = jan,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA373437},
  file = {/Users/rwb/Dropbox/PhD/zotero/2000/pressel_2000_performance_metrics_for_parallel_systems.pdf},
  language = {en}
}

@inproceedings{prodan2005,
  title = {Dynamic {{Scheduling}} of {{Scientific Workflow Applications}} on the {{Grid}}: {{A Case Study}}},
  shorttitle = {Dynamic {{Scheduling}} of {{Scientific Workflow Applications}} on the {{Grid}}},
  booktitle = {Proceedings of the 2005 {{ACM Symposium}} on {{Applied Computing}}},
  author = {Prodan, Radu and Fahringer, Thomas},
  year = {2005},
  pages = {687--694},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1066677.1066835},
  abstract = {The existing Grid workflow scheduling projects do not handle recursive loops which are characteristic to many scientific problems. We propose a hybrid approach for scheduling Directed Graph (DG)-based workflows in a Grid environment with dynamically changing computational and network resources. Our dynamic scheduling algorithm is based on the iterative invocation of classical static Directed Acyclic Graphs (DAGs) scheduling heuristics generated using well-defined cycle elimination and task migration techniques. We approach the static scheduling problem as an application of a modular optimisation tool using genetic algorithms. We report successful implementation and experimental results on a pilot real-world material science workflow application.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/prodan_fahringer_2005_dynamic_scheduling_of_scientific_workflow_applications_on_the_grid.pdf},
  isbn = {978-1-58113-964-8},
  keywords = {genetic algorithms,grid computing,optimisation,performance steering,scheduling,scientific workflows},
  series = {{{SAC}} '05}
}

@article{przybylski2017,
  title = {Multi-Objective Branch and Bound},
  author = {Przybylski, Anthony and Gandibleux, Xavier},
  year = {2017},
  month = aug,
  volume = {260},
  pages = {856--872},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2017.01.032},
  abstract = {Branch and bound is a well-known generic method for computing an optimal solution of a single-objective optimization problem. Based on the idea ``divide to conquer'', it consists in an implicit enumeration principle viewed as a tree search. Although the branch and bound was first suggested by Land and Doig (1960), the first complete algorithm introduced as a multi-objective branch and bound that we identified was proposed by Kiziltan and Yucaoglu (1983). Rather few multi-objective branch and bound algorithms have been proposed. This situation is not surprising as the contributions on the extensions of the components of branch and bound for multi-objective optimization are recent. For example, the concept of bound sets, which extends the classic notion of bounds, has been mentioned by Villarreal and Karwan (1981). But it was only developed for the first time in 2001 by Ehrgott and Gandibleux, and fully defined in 2007. This paper describes a state-of-the-art of multi-objective branch and bound, which reviews concepts, components and published algorithms. It mainly focuses on the contributions belonging to the class of optimization problems who has received the most of attention in this context from 1983 until 2015: the linear optimization problems with zero-one variables and mixed 0\textendash 1/continuous variables. Only papers aiming to compute a complete set of efficient solutions are discussed.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/przybylski_gandibleux_2017_multi-objective_branch_and_bound.pdf;/Users/rwb/Zotero/storage/PGBMAE5P/S037722171730067X.html},
  journal = {European Journal of Operational Research},
  keywords = {Bound sets,Branch and bound,Multiple objective programming},
  number = {3}
}

@inproceedings{qamhieh2014,
  title = {An Experimental Analysis of {{DAG}} Scheduling Methods in Hard Real-Time Multiprocessor Systems},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Research}} in {{Adaptive}} and {{Convergent Systems}} - {{RACS}} '14},
  author = {Qamhieh, Manar and Midonnet, Serge},
  year = {2014},
  pages = {284--290},
  publisher = {{ACM Press}},
  address = {{Towson, Maryland}},
  doi = {10.1145/2663761.2664236},
  abstract = {The scheduling of real-time parallel tasks on multiprocessor systems is more complicated than the one of independent sequential tasks, specially for the Directed Acyclic Graph (DAG) parallel model. The complexity is due to the structure of the DAG tasks and the precedence constraints between their subtasks. The trivial DAG scheduling method is to apply directly common real-time scheduling algorithms despite their lack of compatibility with the parallel model. Another scheduling method called the stretching method is summarized as converting each parallel DAG task in the set into a collection of independent sequential threads that are easier to be scheduled.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/qamhieh_midonnet_2014_an_experimental_analysis_of_dag_scheduling_methods_in_hard_real-time.pdf},
  isbn = {978-1-4503-3060-2},
  language = {en}
}

@inproceedings{qie2018,
  title = {A {{Clustered Virtual Machine Allocation Strategy Based}} on an {{N}}-{{Threshold Sleep}}-{{Mode}} in a {{Cloud Environment}}},
  booktitle = {Queueing {{Theory}} and {{Network Applications}}},
  author = {Qie, Xiuchen and Jin, Shunfu and Yue, Wuyi},
  editor = {Takahashi, Yutaka and {Phung-Duc}, Tuan and Wittevrongel, Sabine and Yue, Wuyi},
  year = {2018},
  pages = {124--132},
  publisher = {{Springer International Publishing}},
  abstract = {In an effort to improve the energy efficiency of cloud data centers, in this paper, we propose a clustered Virtual Machine (VM) allocation strategy based on an N-threshold sleep-mode in which all the VMs in a cloud data center are clustered into two modules. The VMs in Module I are always awake, whereas the VMs in Module II will go to sleep under a light traffic load. When the number of waiting requests reaches or exceeds the threshold N, sleeping VMs will resume processing requests independently after their corresponding sleep timers expire. Accordingly, we establish an N-policy partially asynchronous multiple vacations queueing model, and derive the energy saving rate of the system. Numerical results are provided to show the efficiency of the proposed strategy in reducing energy consumption.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/qie_et_al_2018_a_clustered_virtual_machine_allocation_strategy_based_on_an_n-threshold.pdf},
  isbn = {978-3-319-93736-6},
  keywords = {_tablet,Cloud data center,Clustered VM allocation,Energy saving rate,N-threshold,Sleep-mode},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{qiu2019,
  title = {A Novel {{QoS}}-Enabled Load Scheduling Algorithm Based on Reinforcement Learning in Software-Defined Energy Internet},
  author = {Qiu, Chao and Cui, Shaohua and Yao, Haipeng and Xu, Fangmin and Yu, F. Richard and Zhao, Chenglin},
  year = {2019},
  month = mar,
  volume = {92},
  pages = {43--51},
  issn = {0167-739X},
  doi = {10.1016/j.future.2018.09.023},
  abstract = {Recently, smart grid and Energy Internet (EI) are proposed to solve energy crisis and global warming, where improved communication mechanisms are important. Software-defined networking (SDN) has been used in smart grid for real-time monitoring and communicating, which requires steady web-environment with no packet loss and less time delay. With the explosion of network scales, the idea of multiple controllers has been proposed, where the problem of load scheduling needs to be solved. However, some traditional load scheduling algorithms have inferior robustness under the complicated environments in smart grid, and inferior time efficiency without pre-strategy, which are hard to meet the requirement of smart grid. Therefore, we present a novel controller mind (CM) framework to implement automatic management among multiple controllers. Specially, in order to solve the problem of complexity and pre-strategy in the system, we propose a novel Quality of Service (QoS) enabled load scheduling algorithm based on reinforcement learning in this paper. Simulation results show the effectiveness of our proposed scheme in the aspects of load variation and time efficiency.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/qiu_et_al_2019_a_novel_qos-enabled_load_scheduling_algorithm_based_on_reinforcement_learning.pdf;/Users/rwb/Zotero/storage/2CMIR7SJ/S0167739X1830308X.html},
  journal = {Future Generation Computer Systems},
  keywords = {Energy internet,Load scheduling,Quality of Service (QoS),Reinforcement learning,Smart grid,Software-defined networking}
}

@article{quinn,
  title = {The {{SKA Data System}}},
  author = {Quinn, Peter},
  pages = {9},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/quinn_the_ska_data_system.pdf},
  language = {en}
}

@article{quinn2015,
  title = {Delivering {{SKA Science}}},
  author = {Quinn, P. and Axelrod, T. and Bird, I. and Dodson, R. and Szalay, A. and Wicenec, A.},
  year = {2015},
  month = apr,
  pages = {147},
  journal = {Advancing Astrophysics with the Square Kilometre Array (AASKA14)}
}

@article{rabanal2017,
  title = {Applications of River Formation Dynamics},
  author = {Rabanal, Pablo and Rodr{\'i}guez, Ismael and Rubio, Fernando},
  year = {2017},
  month = sep,
  volume = {22},
  pages = {26--35},
  issn = {1877-7503},
  doi = {10.1016/j.jocs.2017.08.002},
  abstract = {River formation dynamics is a metaheuristic where solutions are constructed by iteratively modifying the values associated to the nodes of a graph. Its gradient orientation provides interesting features such as the fast reinforcement of new shortcuts, the natural avoidance of cycles, and the focused elimination of blind alleys. Since the method was firstly proposed in 2007, several research groups have applied it to a wide variety of application domains, such as telecommunications, software testing, industrial manufacturing processes, or navigation. In this paper we review the main works of the last decade where the river formation dynamics metaheuristic has been applied to solve optimization problems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/rabanal_et_al_2017_applications_of_river_formation_dynamics.pdf;/Users/rwb/Zotero/storage/B66LSMR8/S1877750317307184.html},
  journal = {Journal of Computational Science},
  keywords = {Applications,Heuristic methods,River formation dynamics,Swarm intelligence}
}

@inproceedings{rahman2007,
  title = {A {{Dynamic Critical Path Algorithm}} for {{Scheduling Scientific Workflow Applications}} on {{Global Grids}}},
  booktitle = {Third {{IEEE International Conference}} on E-{{Science}} and {{Grid Computing}} (e-{{Science}} 2007)},
  author = {Rahman, M. and Venugopal, S. and Buyya, R.},
  year = {2007},
  month = dec,
  pages = {35--42},
  doi = {10.1109/E-SCIENCE.2007.3},
  abstract = {Effective scheduling is a key concern for the execution of performance driven grid applications. In this paper, we propose a dynamic critical path (DCP) based workflow scheduling algorithm that determines efficient mapping of tasks by calculating the critical path in the workflow task graph at every step. It assigns priority to a task in the critical path which is estimated to complete earlier. Using simulation, we have compared the performance of our proposed approach with other existing heuristic and meta-heuristic based scheduling strategies for different type and size of workflows. Our results demonstrate that DCP based approach can generate better schedule for most of the type of workflows irrespective of their size particularly when resource availability changes frequently.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/rahman_et_al_2007_a_dynamic_critical_path_algorithm_for_scheduling_scientific_workflow.pdf;/Users/rwb/Zotero/storage/UCKMMDZB/4426869.html},
  keywords = {Application software,Availability,dynamic critical path,Dynamic scheduling,Electrical capacitance tomography,global grids,grid computing,Grid computing,Heuristic algorithms,Laboratories,Processor scheduling,scheduling,Scheduling algorithm,scientific information systems,scientific workflow,Software algorithms,workflow management software,workflow scheduling algorithm,workflow task graph}
}

@article{rahman2013,
  title = {Adaptive Workflow Scheduling for Dynamic Grid and Cloud Computing Environment},
  author = {Rahman, Mustafizur and Hassan, Rafiul and Ranjan, Rajiv and Buyya, Rajkumar},
  year = {2013},
  month = sep,
  volume = {25},
  pages = {1816--1842},
  issn = {1532-0626},
  doi = {10.1002/cpe.3003},
  abstract = {SUMMARYEffective scheduling is a key concern for the execution of performance-driven grid applications such as workflows. In this paper, we first define the workflow scheduling problem and describe the existing heuristic-based and metaheuristic-based workflow scheduling strategies in grids. Then, we propose a dynamic critical-path-based adaptive workflow scheduling algorithm for grids, which determines efficient mapping of workflow tasks to grid resources dynamically by calculating the critical path in the workflow task graph at every step. Using simulation, we compared the performance of the proposed approach with the existing approaches, discussed in this paper for different types and sizes of workflows. The results demonstrate that the heuristic-based scheduling techniques can adapt to the dynamic nature of resource and avoid performance degradation in dynamically changing grid environments. Finally, we outline a hybrid heuristic combining the features of the proposed adaptive scheduling technique with metaheuristics for optimizing execution cost and time as well as meeting the users requirements to efficiently manage the dynamism and heterogeneity of the hybrid cloud environment. Copyright ? 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/rahman_et_al_2013_adaptive_workflow_scheduling_for_dynamic_grid_and_cloud_computing_environment.pdf;/Users/rwb/Zotero/storage/HQS3G6VW/cpe.html},
  journal = {Concurrency and Computation: Practice and Experience},
  keywords = {adaptive scheduling,grid computing,workflow management},
  number = {13}
}

@article{ramamritham1989,
  title = {Distributed Scheduling of Tasks with Deadlines and Resource Requirements},
  author = {Ramamritham, K. and Stankovic, J. A. and Zhao, W.},
  year = {1989},
  month = aug,
  volume = {38},
  pages = {1110--1123},
  issn = {0018-9340},
  doi = {10.1109/12.30866},
  abstract = {A set of four heuristic algorithms is presented to schedule tasks that have headlines and resource requirements in a distributed system. When a task arrives at a node, the local scheduler at that node attempts to guarantee that the task will complete execution on that node before its deadline. If the attempt fails, the scheduling components on individual nodes cooperate to determine which other node in the system has sufficient resource surplus to guarantee the task. Simulation studies are performed to compare the performance of these algorithms with respect to each other as well to two baselines. The first baseline is the noncooperative algorithm where a task that cannot be guaranteed locally is not sent to any other node. The second is an (ideal) algorithm that behaves exactly like the bidding algorithm but incurs no communication overheads. The simulation studies examine how communication delay, task laxity, load differences on the nodes, and task computation times affect the performance of the algorithms. The results show that distributed scheduling is effective even in a hard real-time environment and that the relative performance of these algorithms is a function of the system state.{$<$}{$>$}},
  file = {/Users/rwb/Dropbox/PhD/zotero/1989/ramamritham_et_al_1989_distributed_scheduling_of_tasks_with_deadlines_and_resource_requirements.pdf;/Users/rwb/Zotero/storage/2G7ARP2Z/30866.html},
  journal = {IEEE Transactions on Computers},
  keywords = {_tablet,Aerospace electronics,bidding algorithm,communication delay,Computational modeling,deadlines,Delay,Distributed computing,distributed processing,distributed scheduling,distributed system,hard real-time environment,heuristic algorithms,Heuristic algorithms,heuristic programming,Process control,Processor scheduling,Real time systems,real-time systems,resource requirements,scheduling,Scheduling algorithm,simulation studies,task computation times,task laxity,tasks,Timing},
  number = {8}
}

@inproceedings{rao2009,
  title = {{{VCONF}}: A Reinforcement Learning Approach to Virtual Machines Auto-Configuration},
  shorttitle = {{{VCONF}}},
  booktitle = {Proceedings of the 6th International Conference on {{Autonomic}} Computing - {{ICAC}} '09},
  author = {Rao, Jia and Bu, Xiangping and Xu, Cheng-Zhong and Wang, Leyi and Yin, George},
  year = {2009},
  pages = {137},
  publisher = {{ACM Press}},
  address = {{Barcelona, Spain}},
  doi = {10.1145/1555228.1555263},
  abstract = {Virtual machine (VM) technology enables multiple VMs to share resources on the same host. Resources allocated to the VMs should be re-configured dynamically in response to the change of application demands or resource supply. Because VM execution involves privileged domain and VM monitor, this causes uncertainties in VMs' resource to performance mapping and poses challenges in online determination of appropriate VM configurations. In this paper, we propose a reinforcement learning (RL) based approach, namely VCONF, to automate the VM configuration process. VCONF employs model-based RL algorithms to address the scalability and adaptability issues in applying RL in systems management. Experimental results on both controlled environments and a testbed of clouds with Xen VMs and representative server workloads demonstrate the effectiveness of VCONF. The approach is able to find optimal (near optimal) configurations in small scale systems and shows good adaptability and scalability.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2009/rao_et_al_2009_vconf.pdf},
  isbn = {978-1-60558-564-2},
  language = {en}
}

@article{ratcliffe2016,
  title = {{{SKA1 LOW SDP}} - {{CSP Interface Control Document}}},
  author = {Ratcliffe, S and Graser, F and Carlson, B and B, Opperman},
  year = {2016},
  month = mar,
  volume = {1},
  journal = {SKA Project Document number 100-000000-002},
  number = {1}
}

@article{reklaitis2000,
  title = {Overview of {{Planning}} and {{Scheduling Technologies}}},
  author = {Reklaitis, G V},
  year = {2000},
  pages = {10},
  file = {/Users/rwb/Dropbox/PhD/zotero/2000/reklaitis_2000_overview_of_planning_and_scheduling_technologies.pdf},
  language = {en}
}

@article{remis2017,
  title = {Exploiting Social Network Graph Characteristics for Efficient {{BFS}} on Heterogeneous Chips},
  author = {Remis, Luis and Garzaran, Maria Jesus and Asenjo, Rafael and Navarro, Angeles},
  year = {2017},
  month = nov,
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2017.11.003},
  abstract = {Several approaches implement efficient BFS algorithms for multicores and for GPUs. However, when targeting heterogeneous architectures, it is still an open problem how to distribute the work among the CPU cores and the accelerators. In this paper, we assess several approaches to perform BFS on different heterogeneous chips (a multicore CPU and an integrated GPU). In particular, we propose three heterogeneous approaches that exploit the collaboration between both devices: Selective, Concurrent and Asynchronous. We identify how to take advantage of the features of social network graphs, that are a particular example of highly connected graphs-with fewer iterations and more unbalanced-, as well as the drawbacks of each algorithmic implementation. One key feature of our approaches is that they switch between different versions of the algorithm, depending on the device that collaborates in the computation. Through exhaustive evaluation we find that our heterogeneous implementations can be up to 1.56\texttimes ~faster and 1.32\texttimes ~more energy efficient with respect to the best baseline where only one device is used, being the overhead w.r.t. an oracle scheduler below 10\%. We also compare with other related heterogeneous approach finding that ours can be up to 3.6\texttimes ~faster.},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\Journal of Parallel and Distributed Computing\\2017\\remis_et_al_2017_exploiting_social_network_graph_characteristics_for_efficient_bfs_on.pdf;/Users/rwb/Zotero/storage/YTUJKVTR/S0743731517303015.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {BFS,Heterogeneous chips,Performanceenergy efficiency,Social network graphs,Unread}
}

@inproceedings{rey2016,
  title = {{{HeSP}}: {{A Simulation Framework}} for {{Solving}} the {{Task Scheduling}}-{{Partitioning Problem}} on {{Heterogeneous Architectures}}},
  shorttitle = {{{HeSP}}},
  booktitle = {Euro-{{Par}} 2016: {{Parallel Processing}}},
  author = {Rey, Ant{\'o}n and Igual, Francisco D. and {Prieto-Mat{\'i}as}, Manuel},
  year = {2016},
  month = aug,
  pages = {183--195},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-43659-3_14},
  abstract = {In this paper we describe HeSP, a complete simulation framework to study a general task scheduling-partitioning problem on heterogeneous architectures, which treats recursive task partitioning and scheduling decisions on equal footing. Considering recursive partitioning as an additional degree of freedom, tasks can be dynamically partitioned or merged at runtime for each available processor type, exposing additional or reduced degrees of parallelism as needed. Our simulations reveal that, for a specific class of dense linear algebra algorithms taken as a driving example, simultaneous decisions on task scheduling and partitioning yield significant performance gains on two different heterogeneous platforms: a highly heterogeneous CPU-GPU system and a low-power asymmetric big.LITTLE ARM platform. The insights extracted from the framework can be further applied to actual runtime task schedulers in order to improve performance on current or future architectures and for different task-parallel codes.},
  file = {/Users/rwb/Zotero/storage/JZWV3ZPX/978-3-319-43659-3_14.html},
  isbn = {978-3-319-43658-6 978-3-319-43659-3},
  keywords = {Unread},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{rey2016a,
  title = {{{HeSP}}: {{A Simulation Framework}} for {{Solving}} the~{{Task Scheduling}}-{{Partitioning Problem}} on~{{Heterogeneous Architectures}}},
  shorttitle = {{{HeSP}}},
  booktitle = {Euro-{{Par}} 2016: {{Parallel Processing}}},
  author = {Rey, Ant{\'o}n and Igual, Francisco D. and {Prieto-Mat{\'i}as}, Manuel},
  editor = {Dutot, Pierre-Fran{\c c}ois and Trystram, Denis},
  year = {2016},
  pages = {183--195},
  publisher = {{Springer International Publishing}},
  abstract = {In this paper we describe HeSP, a complete simulation framework to study a general task scheduling-partitioning problem on heterogeneous architectures, which treats recursive task partitioning and scheduling decisions on equal footing. Considering recursive partitioning as an additional degree of freedom, tasks can be dynamically partitioned or merged at runtime for each available processor type, exposing additional or reduced degrees of parallelism as needed. Our simulations reveal that, for a specific class of dense linear algebra algorithms taken as a driving example, simultaneous decisions on task scheduling and partitioning yield significant performance gains on two different heterogeneous platforms: a highly heterogeneous CPU-GPU system and a low-power asymmetric big.LITTLE ARM platform. The insights extracted from the framework can be further applied to actual runtime task schedulers in order to improve performance on current or future architectures and for different task-parallel codes.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/rey_et_al_2016_hesp.pdf},
  isbn = {978-3-319-43659-3},
  keywords = {Cholesky Factorization,Data Block,Memory Space,Schedule Heuristic,Tile Size},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{ribeiro,
  title = {Embedding a {{Priori Knowledge}} in {{Reinforcement Learning}}},
  author = {Ribeiro, Carlos H C},
  pages = {21},
  abstract = {In the last years, temporal differences methods have been put forward as convenient tools for reinforcement learning. Techniques based on temporal differences, however, suffer from a serious drawback: as stochastic adaptive algorithms, they may need extensive exploration of the state-action space before convergence is achieved. Although the basic methods are now reasonably well understood, it is precisely the structural simplicity of the reinforcement learning principle \textendash learning through experimentation \textendash{} that causes these excessive demands on the learning agent. Additionally, one must consider that the agent is very rarely a tabula rasa: some rough knowledge about characteristics of the surrounding environment is often available. In this paper, I present methods for embedding a priori knowledge in a reinforcement learning technique in such a way that both the mathematical structure of the basic learning algorithm and the capacity to generalise experience across the state-action space are kept. Extensive experimental results show that the resulting variants may lead to good performance, provided a sensible balance between risky use of prior imprecise knowledge and cautious use of learning experience is adopted.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/ribeiro_embedding_a_priori_knowledge_in_reinforcement_learning.pdf},
  language = {en}
}

@inproceedings{richie-halford2018,
  title = {Cloudknot: {{A Python Library}} to {{Run}} Your {{Existing Code}} on {{AWS Batch}}},
  shorttitle = {Cloudknot},
  booktitle = {Python in {{Science Conference}}},
  author = {{Richie-Halford}, Adam and Rokem, Ariel},
  year = {2018},
  pages = {8--14},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-4af1f417-001},
  abstract = {We introduce Cloudknot, a software library that simplifies cloudbased distributed computing by programmatically executing user-defined functions (UDFs) in AWS Batch. It takes as input a Python function, packages it as a container, creates all the necessary AWS constituent resources to submit jobs, monitors their execution and gathers the results, all from within the Python environment. Cloudknot minimizes the cognitive load of learning a new API by introducing only one new object and using the familiar map method. It overcomes limitations of previous similar libraries, such as Pywren, that runs UDFs on AWS Lambda, because most data science workloads exceed the current limits of AWS Lambda on execution time, RAM, and local storage.},
  file = {/Users/rwb/Zotero/storage/QWHITCUN/Richie-Halford and Rokem - 2018 - Cloudknot A Python Library to Run your Existing C.pdf},
  language = {en}
}

@inproceedings{rocklin2015,
  title = {Dask: {{Parallel Computation}} with {{Blocked}} Algorithms and {{Task Scheduling}}},
  shorttitle = {Dask},
  booktitle = {Python in {{Science Conference}}},
  author = {Rocklin, Matthew},
  year = {2015},
  pages = {126--132},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-7b98e3ed-013},
  abstract = {Dask enables parallel and out-of-core computation. We couple blocked algorithms with dynamic and memory aware task scheduling to achieve a parallel and out-of-core NumPy clone. We show how this extends the effective scale of modern hardware to larger datasets and discuss how these ideas can be more broadly applied to other parallel collections.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/rocklin_2015_dask.pdf},
  language = {en}
}

@inproceedings{rocklin2015a,
  title = {Dask: {{Parallel Computation}} with {{Blocked}} Algorithms and {{Task Scheduling}}},
  shorttitle = {Dask},
  booktitle = {Python in {{Science Conference}}},
  author = {Rocklin, Matthew},
  year = {2015},
  pages = {126--132},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-7b98e3ed-013},
  abstract = {Dask enables parallel and out-of-core computation. We couple blocked algorithms with dynamic and memory aware task scheduling to achieve a parallel and out-of-core NumPy clone. We show how this extends the effective scale of modern hardware to larger datasets and discuss how these ideas can be more broadly applied to other parallel collections.},
  file = {/Users/rwb/Zotero/storage/LRNC3CKM/Rocklin - 2015 - Dask Parallel Computation with Blocked algorithms.pdf},
  language = {en}
}

@article{rodriguez-moreno2004,
  title = {An {{AI Planning}}-Based {{Tool}} for {{Scheduling Satellite Nominal Operations}}},
  author = {{Rodriguez-Moreno}, Maria Dolores and Borrajo, Daniel and Meziat, Daniel},
  year = {2004},
  month = dec,
  volume = {25},
  pages = {9--9},
  issn = {2371-9621},
  doi = {10.1609/aimag.v25i4.1782},
  copyright = {Copyright (c)},
  file = {/Users/rwb/Dropbox/PhD/zotero/2004/rodriguez-moreno_et_al_2004_an_ai_planning-based_tool_for_scheduling_satellite_nominal_operations.pdf;/Users/rwb/Zotero/storage/J6PMBB9T/1782.html},
  journal = {AI Magazine},
  language = {en},
  number = {4}
}

@article{rodriguez2014,
  title = {Deadline {{Based Resource Provisioningand Scheduling Algorithm}} for {{Scientific Workflows}} on {{Clouds}}},
  author = {Rodriguez, Maria Alejandra and Buyya, Rajkumar},
  year = {2014},
  month = apr,
  volume = {2},
  pages = {222--235},
  issn = {2372-0018},
  doi = {10.1109/TCC.2014.2314655},
  abstract = {Cloud computing is the latest distributed computing paradigm and it offers tremendous opportunities to solve large-scale scientific problems. However, it presents various challenges that need to be addressed in order to be efficiently utilized for workflow applications. Although the workflow scheduling problem has been widely studied, there are very few initiatives tailored for cloud environments. Furthermore, the existing works fail to either meet the user's quality of service (QoS) requirements or to incorporate some basic principles of cloud computing such as the elasticity and heterogeneity of the computing resources. This paper proposes a resource provisioning and scheduling strategy for scientific workflows on Infrastructure as a Service (IaaS) clouds. We present an algorithm based on the meta-heuristic optimization technique, particle swarm optimization (PSO), which aims to minimize the overall workflow execution cost while meeting deadline constraints. Our heuristic is evaluated using CloudSim and various well-known scientific workflows of different sizes. The results show that our approach performs better than the current state-of-the-art algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/rodriguez_buyya_2014_deadline_based_resource_provisioningand_scheduling_algorithm_for_scientific.pdf;/Users/rwb/Zotero/storage/9E8LWXZU/6782394.html},
  journal = {IEEE Transactions on Cloud Computing},
  keywords = {cloud computing,Cloud computing,CloudSim,Computational modeling,Computer applications,computing resources elasticity,computing resources heterogeneity,cost reduction,deadline based resource provisioning,deadline constraints,distributed computing paradigm,Distributed processing,IaaS clouds,infrastructure as a service clouds,Mathematical model,meta-heuristic optimization technique,natural sciences computing,particle swarm optimisation,particle swarm optimization,Processor scheduling,PSO,QoS,quality of service,Quality of service,resource allocation,resource provisioning,scheduling,scheduling algorithm,scientific workflow,scientific workflows,user quality of service,workflow execution cost minimization,workflow management software},
  number = {2}
}

@article{rodriguez2016,
  title = {A Taxonomy and Survey on Scheduling Algorithms for Scientific Workflows in {{IaaS}} Cloud Computing Environments},
  author = {Rodriguez, Maria Alejandra and Buyya, Rajkumar},
  year = {2016},
  volume = {29},
  pages = {e4041},
  issn = {1532-0634},
  doi = {10.1002/cpe.4041},
  abstract = {Large-scale scientific problems are often modeled as workflows. The ever-growing data and compute requirements of these applications has led to extensive research on how to efficiently schedule and deploy them in distributed environments. The emergence of the latest distributed systems paradigm, cloud computing, brings with it tremendous opportunities to run scientific workflows at low costs without the need of owning any infrastructure. It provides a virtually infinite pool of resources that can be acquired, configured, and used as needed and are charged on a pay-per-use basis. However, along with these benefits come numerous challenges that need to be addressed to generate efficient schedules. This work identifies these challenges and studies existing algorithms from the perspective of the scheduling models they adopt as well as the resource and application model they consider. A detailed taxonomy that focuses on features particular to clouds is presented, and the surveyed algorithms are classified according to it. In this way, we aim to provide a comprehensive understanding of existing literature and aid researchers by providing an insight into future directions and open issues.},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  file = {/Users/rwb/Zotero/storage/7P6YEESC/cpe.html},
  journal = {Concurrency and Computation: Practice and Experience},
  keywords = {IaaS cloud,resource provisioning,scheduling,scientific workflow,survey,taxonomy},
  language = {en},
  number = {8}
}

@article{rodriguez2018,
  title = {Scheduling Dynamic Workloads in Multi-Tenant Scientific Workflow as a Service Platforms},
  author = {Rodriguez, Maria A. and Buyya, Rajkumar},
  year = {2018},
  month = feb,
  volume = {79},
  pages = {739--750},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.05.009},
  abstract = {With the advent of cloud computing and the availability of data collected from increasingly powerful scientific instruments, workflows have become a prevailing mean to achieve significant scientific advances at an increased pace. Emerging Workflow as a Service (WaaS) platforms offer scientists a simple, easily accessible, and cost-effective way of deploying their applications in the cloud at anytime and from anywhere. They are multi-tenant frameworks and are designed to manage the execution of a continuous workload of heterogeneous workflows. To achieve this, they leverage the compute, storage, and network resources offered by Infrastructure as a Service (IaaS) providers. Hence, at any given point in time, a WaaS platform should be capable of efficiently scheduling an arbitrarily large number of workflows with different characteristics and quality of service requirements. As a result, we propose a resource provisioning and scheduling strategy designed specifically for WaaS environments. The algorithm is scalable and dynamic to adapt to changes in the environment and workload. It leverages containers to address resource utilization inefficiencies and aims to minimize the overall cost of leasing the infrastructure resources while meeting the deadline constraint of each individual workflow. To the best of our knowledge, this is the first approach that explicitly addresses VM sharing in the context of WaaS by modeling the use of containers in the resource provisioning and scheduling heuristics. Our simulation results demonstrate its responsiveness to environmental uncertainties, its ability to meet deadlines, and its cost-efficiency when compared to a state-of-the-art algorithm.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/rodriguez_buyya_2018_scheduling_dynamic_workloads_in_multi-tenant_scientific_workflow_as_a_service.pdf;/Users/rwb/Zotero/storage/8JT5I2V4/S0167739X17301681.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cloud computing,Cost minimization,Deadline,Resource provisioning,Scheduling,Workflow as a service}
}

@inproceedings{rooyen2018,
  title = {Autonomous Observation Scheduling in Astronomy},
  booktitle = {Observatory {{Operations}}: {{Strategies}}, {{Processes}}, and {{Systems VII}}},
  author = {van Rooyen, Ruby and Maartens, Deneys S. and Martinez, Peter},
  year = {2018},
  month = jul,
  volume = {10704},
  pages = {1070410},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2311839},
  abstract = {Rapid technology development has opened up exciting new possibilities in astronomy with larger, more sensitive telescopes that cover large frequency ranges. Telescopes have become bigger and more complex, housing multiple instruments and having wide ranging science aims. These technological advances and cheaper internet makes remote collaboration across astronomies, with multi-wavelength astronomy, quick follow-up and network observation strategies, easy to achieve. {$<$}p{$>$} {$<$}/p{$>$}In turn, each telescope still has to deal with weather and dynamic conditions on oversubscribed systems. It is thus natural to turn to advanced multi-purpose scheduling software development in order to enhance scientific return in an effort to identify better solutions. {$<$}p{$>$} {$<$}/p{$>$}In this paper we present a strawman Python scheduler based on a modular design for scheduling astronomy observations that is easy to adapt to the specific requirements of an observation, but still general enough to optimize in a simple evaluation schema very similar to dynamic scheduling strategies already being used in so many industrial areas.},
  file = {/Users/rwb/Zotero/storage/FHV2ABD7/12.2311839.html}
}

@article{rossi,
  title = {Handbook of {{Constraint Programming}}},
  author = {Rossi, Edited F and {van Beek}, P and Walsh, T},
  pages = {969},
  language = {en}
}

@incollection{rossi2008,
  ids = {rossi2008a},
  title = {Chapter 4 {{Constraint Programming}}},
  author = {Rossi, Francesca and {van Beek}, Peter and Walsh, Toby},
  year = {2008},
  pages = {181--211},
  publisher = {{Elsevier}},
  doi = {10.1016/s1574-6526(07)03004-0},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/rossi_et_al_2008_chapter_4_constraint_programming.pdf;/Users/rwb/Dropbox/PhD/zotero/2008/rossi_et_al_2008_chapter_4_constraint_programming2.pdf},
  isbn = {978-0-444-52211-5},
  language = {en}
}

@techreport{sadykov2003,
  title = {Integer {{Programming}} and {{Constraint Programming}} in {{Solving}} a {{Multi}}-{{Machine Assignment Scheduling Problem With Deadlines}} and {{Release Dates}}},
  author = {Sadykov, Ruslan and Wolsey, Laurence A.},
  year = {2003},
  month = nov,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {We consider both branch-and-cut and column generation approaches for the problem of finding a minimum cost assignment of jobs with release dates and deadlines to unrelated parallel machines. Results are presented for several variants both with and without Constraint Programming. Among the variants, the most effective strategy is to combine a tight and compact, but approximate, Mixed Integer Programming formulation with a global constraint testing single machine feasibility. All the algorithms have been implemented in the Mosel modelling and optimization language.},
  file = {/Users/rwb/Zotero/storage/YYARDMVI/papers.html},
  keywords = {Integer Programming and Constraint Programming in Solving a Multi-Machine Assignment Scheduling Problem With Deadlines and Release Dates,Laurence A. Wolsey,Ruslan Sadykov,SSRN},
  language = {en},
  number = {ID 988640},
  type = {{{SSRN Scholarly Paper}}}
}

@article{sadykov2006,
  ids = {sadykov2006a},
  title = {Integer {{Programming}} and {{Constraint Programming}} in {{Solving}} a {{Multimachine Assignment Scheduling Problem}} with {{Deadlines}} and {{Release Dates}}},
  author = {Sadykov, Ruslan and Wolsey, Laurence A.},
  year = {2006},
  month = may,
  volume = {18},
  pages = {209--217},
  issn = {1091-9856},
  doi = {10.1287/ijoc.1040.0110},
  abstract = {We consider both branch-and-cut and column-generation approaches for the problem of finding a minimum-cost assignment of jobs with release dates and deadlines to unrelated parallel machines. Results are presented for several variants both with and without constraint programming. Among the variants, the most effective strategy is to combine a tight and compact, but approximate, mixed integer programming (MIP) formulation with a global constraint testing single machine feasibility. Instances with up to nine machines and 54 jobs have been solved. All the algorithms have been implemented in the Mosel modeling and optimization language.},
  file = {/Users/rwb/Zotero/storage/3VB5QTQV/ijoc.1040.html;/Users/rwb/Zotero/storage/YIQJ4ZMH/ijoc.1040.html},
  journal = {INFORMS Journal on Computing},
  number = {2}
}

@article{saifullah2014,
  title = {Parallel {{Real}}-{{Time Scheduling}} of {{DAGs}}},
  author = {Saifullah, A. and Ferry, D. and Li, J. and Agrawal, K. and Lu, C. and Gill, C. D.},
  year = {2014},
  month = dec,
  volume = {25},
  pages = {3242--3252},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2013.2297919},
  abstract = {Recently, multi-core processors have become mainstream in processor design. To take full advantage of multi-core processing, computation-intensive real-time systems must exploit intra-task parallelism. In this paper, we address the problem of realtime scheduling for a general model of deterministic parallel tasks, where each task is represented as a directed acyclic graph (DAG) with nodes having arbitrary execution requirements. We prove processor-speed augmentation bounds for both preemptive and nonpreemptive real-time scheduling for general DAG tasks on multi-core processors. We first decompose each DAG into sequential tasks with their own release times and deadlines. Then we prove that these decomposed tasks can be scheduled using preemptive global EDF with a resource augmentation bound of 4. This bound is as good as the best known bound for more restrictive models, and is the first for a general DAG model. We also prove that the decomposition has a resource augmentation bound of 4 plus a constant non-preemption overhead for non-preemptive global EDF scheduling. To our knowledge, this is the first resource augmentation bound for non-preemptive scheduling of parallel tasks. Finally, we evaluate our analytical results through simulations that demonstrate that the derived resource augmentation bounds are safe in practice.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/saifullah_et_al_2014_parallel_real-time_scheduling_of_dags.pdf;/Users/rwb/Zotero/storage/4V3WIAH8/6714435.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {deterministic parallel tasks,directed acyclic graph,directed graphs,general DAG model,intra-task parallelism,Job shop scheduling,multi-core processor,Multicore processing,multicore processors,multiprocessing systems,nonpreemptive real-time scheduling,parallel real-time scheduling,Parallel task,preemptive real-time scheduling,processor scheduling,Processor scheduling,processor-speed augmentation bounds,real-time scheduling,Real-time systems,resource augmentation bound,Schedules,Timing},
  number = {12}
}

@article{salimans2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = mar,
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archivePrefix = {arXiv},
  eprint = {1703.03864},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/salimans_et_al_2017_evolution_strategies_as_a_scalable_alternative_to_reinforcement_learning.pdf;/Users/rwb/Zotero/storage/R2UNJF7N/1703.html},
  journal = {arXiv:1703.03864 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{sama2015,
  title = {Metaheuristics for {{Real}}-{{Time Near}}-{{Optimal Train Scheduling}} and {{Routing}}},
  booktitle = {2015 {{IEEE}} 18th {{International Conference}} on {{Intelligent Transportation Systems}}},
  author = {Sam{\`a}, Marcella and DAriano, Andrea and Toli, Alessandro and Pacciarelli, Dario and Corman, Francesco},
  year = {2015},
  month = sep,
  pages = {1678--1683},
  issn = {2153-0017},
  doi = {10.1109/ITSC.2015.272},
  abstract = {This paper focuses on metaheuristic algorithms for the real-time traffic management problem of scheduling and routing trains in a complex and busy railway network. Since the problem is strongly NP-hard, heuristic algorithms are developed to compute good quality solutions in a short computation time. In this work, a number of algorithmic improvements are implemented in the AGLIBRARY optimization solver, that manages trains at the microscopic level of block sections and block signals and at a precision of seconds. The solver outcome is a detailed conflict-free train schedule, being able to avoid deadlocks and to minimize train delays. The proposed algorithmic framework starts from a good initial solution for the train scheduling problem with fixed routes, obtained via a truncated branch-and-bound algorithm. Variable neighbourhood search and tabu search metaheuristics are then applied to improve the solution by re-routing some trains. Computational experiments are performed on a UK railway network with dense traffic in order to compare the two types of studied metaheuristics.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/sam_et_al_2015_metaheuristics_for_real-time_near-optimal_train_scheduling_and_routing.pdf;/Users/rwb/Zotero/storage/KZDZNTDV/7313364.html},
  keywords = {AGLIBRARY optimization solver,computational complexity,conflict-free train schedule,Delays,Heuristic algorithms,NP-hard problem,optimisation,Processor scheduling,rail traffic,Rail transportation,railways,real-time near-optimal train routing,real-time near-optimal train scheduling,real-time traffic management problem,Routing,Schedules,scheduling,Search problems,tabu search metaheuristics,tree searching,truncated branch-and-bound algorithm,UK railway network,variable neighbourhood search metaheuristics,vehicle routing}
}

@inproceedings{samuelsson2018,
  title = {Applying {{Reinforcement Learning}} to {{Basic Routing Problem}}},
  booktitle = {Queueing {{Theory}} and {{Network Applications}}},
  author = {Sam{\'u}elsson, Sigur\dh ur Gauti and Hyyti{\"a}, Esa},
  editor = {Takahashi, Yutaka and {Phung-Duc}, Tuan and Wittevrongel, Sabine and Yue, Wuyi},
  year = {2018},
  pages = {238--249},
  publisher = {{Springer International Publishing}},
  abstract = {Routing jobs to parallel servers is a common and important task in today's computer and communication systems. As each routing decision affects the jobs arriving later, determining the (near) optimal decisions is non-trivial. In this paper, we apply reinforcement learning techniques to the job routing problem with heterogeneous servers and a general cost structure. We study the convergence of the reinforcement learning to a near-optimal policy (that we can determine by other means), and compare its performance against heuristic policies such as Join-the-Shortest-Queue (JSQ) and Shortest-Expected-Delay (SED).},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/samelsson_hyyti_2018_applying_reinforcement_learning_to_basic_routing_problem.pdf},
  isbn = {978-3-319-93736-6},
  keywords = {_tablet,Job dispatching,Machine learning,Parallel servers,Reinforcement learning,Task assignment,Value function},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{saraswathi2015,
  ids = {saraswathi2015},
  title = {Dynamic {{Resource Allocation Scheme}} in {{Cloud Computing}}},
  author = {Saraswathi, A.T. and Kalaashri, Y.R.A. and Padmavathi, S.},
  year = {2015},
  volume = {47},
  pages = {30--36},
  issn = {18770509},
  doi = {10.1016/j.procs.2015.03.180},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/saraswathi_et_al_2015_dynamic_resource_allocation_scheme_in_cloud_computing.pdf;/Users/rwb/Zotero/storage/K8RE843M/saraswathi_et_al_2015_dynamic_resource_allocation_scheme_in_cloud_computing.pdf;/Users/rwb/Zotero/storage/B3TTKQVL/S1877050915004482.html},
  journal = {Procedia Computer Science},
  keywords = {_tablet,Cloud computing,Lease type,Pre-emption.,Priority,Resource allocation,Virtualization},
  language = {en}
}

@book{schedmd,
  title = {{{SLURM Quick Start User Guide}}},
  author = {{SchedMD}}
}

@incollection{senkul2002,
  title = {A {{Logical Framework}} for {{Scheduling Workflows Under Resource Allocation Constraints}}},
  booktitle = {{{VLDB}} '02: {{Proceedings}} of the 28th {{International Conference}} on {{Very Large Databases}}},
  author = {Senkul, Pinar and Kifer, Michael and Toroslu, Ismail H.},
  year = {2002},
  pages = {694--705},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-155860869-6/50067-6},
  abstract = {A workflow consists of a collection of coordinated tasks designed to carry out a welldefined complex process, such as catalog ordering, trip planning, or a business process in an enterprise. Scheduling of workflows is a problem of finding a correct execution sequence for the workflow tasks, i.e., execution that obeys the constraints that embody the business logic of the workflow. Research on workflow scheduling has largely concentrated on temporal constraints, which specify correct ordering of tasks. Another important class of constraints \textemdash{} those that arise from resource allocation \textemdash{} has received relatively little attention in workflow modeling. Since typically resources are not limitless and cannot be shared, scheduling of a workflow execution involves decisions as to which resources to use and when. In this work, we present a framework for workflows whose correctness is given by a set of resource allocation constraints and develop techniques for scheduling such systems. Our framework integrates Concurrent Transaction Logic (CTR) with constraint logic programming (CLP), yielding a new logical formalism, which we call Concurrent Constraint Transaction Logic, or CCTR.},
  isbn = {978-1-55860-869-6},
  language = {en}
}

@article{senkul2005,
  ids = {senkul2005a},
  title = {An Architecture for Workflow Scheduling under Resource Allocation Constraints},
  author = {Senkul, Pinar and Toroslu, Ismail H.},
  year = {2005},
  month = jul,
  volume = {30},
  pages = {399--422},
  issn = {0306-4379},
  doi = {10.1016/j.is.2004.03.003},
  abstract = {Research on specification and scheduling of workflows has concentrated on temporal and causality constraints, which specify existence and order dependencies among tasks. However, another set of constraints that specify resource allocation is also equally important. The resources in a workflow environment are agents such as person, machine, software, etc. that execute the task. Execution of a task has a cost and this may vary depending on the resources allocated in order to execute that task. Resource allocation constraints define restrictions on how to allocate resources, and scheduling under resource allocation constraints provide proper resource allocation to tasks. In this work, we provide an architecture to specify and to schedule workflows under resource allocation constraints as well as under the temporal and causality constraints. A specification language with the ability to express resources and resource allocation constraints and a scheduler module that contains a constraint solver in order to find correct resource assignments are core and novel parts of this architecture.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/senkul_toroslu_2005_an_architecture_for_workflow_scheduling_under_resource_allocation_constraints.pdf;/Users/rwb/Dropbox/PhD/zotero/Information Systems/senkul_toroslu_2005_an_architecture_for_workflow_scheduling_under_resource_allocation_constraints.pdf;/Users/rwb/Zotero/storage/7VF3KP58/S030643790400047X.html;/Users/rwb/Zotero/storage/XKIL7HPN/S030643790400047X.html},
  journal = {Information Systems},
  keywords = {Constraint programming,Resource,Resource allocation constraints,Scheduling,Workflow},
  number = {5}
}

@article{shahrabi2017,
  title = {A Reinforcement Learning Approach to Parameter Estimation in Dynamic Job Shop Scheduling},
  author = {Shahrabi, Jamal and Adibi, Mohammad Amin and Mahootchi, Masoud},
  year = {2017},
  month = aug,
  volume = {110},
  pages = {75--82},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2017.05.026},
  abstract = {In this paper, reinforcement learning (RL) with a Q-factor algorithm is used to enhance performance of the scheduling method proposed for dynamic job shop scheduling (DJSS) problem which considers random job arrivals and machine breakdowns. In fact, parameters of an optimization process at any rescheduling point are selected by continually improving policy which comes from RL. The scheduling method is based on variable neighborhood search (VNS) which is introduced to address the DJSS problem. A new approach is also introduced to calculate reward values in learning processes based on quality of selected parameters. The proposed method is compared with general variable neighborhood search and some common dispatching rules that have been widely used in the literature for the DJSS problem. Results illustrate the high performance of the proposed method in a simulated environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/shahrabi_et_al_2017_a_reinforcement_learning_approach_to_parameter_estimation_in_dynamic_job_shop.pdf;/Users/rwb/Zotero/storage/QHJBALIC/S0360835217302309.html},
  journal = {Computers \& Industrial Engineering},
  keywords = {Dynamic job shop scheduling,Qfactor,Reinforcement learning,Variable neighborhood search}
}

@article{shen2017,
  title = {Distributed {{Autonomous Virtual Resource Management}} in {{Datacenters Using Finite}}-{{Markov Decision Process}}},
  author = {Shen, H. and Chen, L.},
  year = {2017},
  month = dec,
  volume = {25},
  pages = {3836--3849},
  issn = {1063-6692},
  doi = {10.1109/TNET.2017.2759276},
  abstract = {To provide robust infrastructure as a service, clouds currently perform load balancing by migrating virtual machines (VMs) from heavily loaded physical machines (PMs) to lightly loaded PMs. Previous reactive load balancing algorithms migrate VMs upon the occurrence of load imbalance, while previous proactive load balancing algorithms predict PM overload to conduct VM migration. However, both methods cannot maintain long-term load balance and produce high overhead and delay due to migration VM selection and destination PM selection. To overcome these problems, in this paper, we propose a proactive Markov Decision Process (MDP)-based load balancing algorithm. We handle the challenges of allying MDP in virtual resource management in cloud datacenters, which allows a PM to proactively find an optimal action to transit to a lightly loaded state that will maintain for a longer period of time. We also apply the MDP to determine destination PMs to achieve long-term PM load balance state. Our algorithm reduces the numbers of service level agreement (SLA) violations by long-term load balance maintenance, and also reduces the load balancing overhead (e.g., CPU time and energy) and delay by quickly identifying VMs and destination PMs to migrate. We further propose enhancement methods for higher performance. First, we propose a cloud profit oriented reward system in the MDP model so that when the MDP tries to maximize the rewards for load balance, it concurrently improves the actual profit of the datacenter. Second, we propose a new MDP model, which considers the actions for both migrating a VM out of a PM and migrating a VM into a PM, in order to reduce the overhead and improve the effectiveness of load balancing. Our trace-driven experiments show that our algorithm outperforms both previous reactive and proactive load balancing algorithms in terms of SLA violation, load balancing efficiency, and long-term load balance maintenance. Our experimental results also show the effectiveness of our proposed enhancement methods.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/shen_chen_2017_distributed_autonomous_virtual_resource_management_in_datacenters_using.pdf;/Users/rwb/Zotero/storage/IEQHLEBV/8085392.html},
  journal = {IEEE/ACM Transactions on Networking},
  keywords = {Algorithm design and analysis,cloud computing,Cloud computing,computer centres,Delays,distributed autonomous virtual resource management,load balancing algorithm,load balancing overhead,Load management,Load modeling,long-term load balance maintenance,Markov decision process,Markov processes,MDP model,Prediction algorithms,proactive load balancing algorithms,proactive Markov decision process,reactive load balancing algorithms,resource allocation,resource management,Resource management,SLA violation,virtual machine migration,virtual machines},
  number = {6}
}

@inproceedings{shubham2016,
  title = {An Effective Multi-Objective Workflow Scheduling in Cloud Computing: {{A PSO}} Based Approach},
  shorttitle = {An Effective Multi-Objective Workflow Scheduling in Cloud Computing},
  booktitle = {2016 {{Ninth International Conference}} on {{Contemporary Computing}} ({{IC3}})},
  author = {{Shubham} and Gupta, R. and Gajera, V. and Jana, P. K.},
  year = {2016},
  month = aug,
  pages = {1--6},
  doi = {10.1109/IC3.2016.7880196},
  abstract = {Cloud computing has emerged as prominent paradigm in distributed computing which provides on-demand services to users. It involves challenging areas like workflow scheduling to decide the sequence in which the applications are to be scheduled on several computing resources. Due to NP-complete nature of workflow scheduling, finding an optimal solution is very challenging task. Thus, a meta-heuristic approach such as Particle Swarm Optimization (PSO) can be a promising technique to obtain a near-optimal solution of this problem. Several workflow scheduling algorithms have been developed in recent years but quite a few of them focuses on two or more parameters of scheduling at a time like usage cost, makespan, utilization of resource, load balancing etc. In this paper, we present a PSO based workflow scheduling which consider two such conflicting parameters i.e., makespan and resource utilization. With meticulous experiments on standard workflows we find that our proposed approach outperforms genetic algorithm based workflow scheduling in all cases achieving 100\% results.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/shubham_et_al_2016_an_effective_multi-objective_workflow_scheduling_in_cloud_computing.pdf;/Users/rwb/Zotero/storage/JJ6EXCZD/7880196.html},
  keywords = {cloud computing,Cloud computing,computational complexity,distributed algorithms,distributed computing,makespan,meta-heuristic approach,multiobjective workflow scheduling,NP-complete,on-demand user services,Optimal scheduling,particle swarm optimisation,particle swarm optimization,Particle Swarm Optimization,Processor scheduling,prominent paradigm,PSO based approach,Resource management,resource utilization,Schedules,scheduling,Scheduling,Standards,Workflow scheduling}
}

@article{silva2016,
  title = {Integrating {{Domain}}-Data {{Steering}} with {{Code}}-Profiling {{Tools}} to {{Debug Data}}-Intensive {{Workflows}}},
  author = {Silva, V{\'i}tor and Neves, Leonardo and Souza, Renan and Coutinho, Alvaro and {de Oliveira}, Daniel and Mattoso, Marta},
  year = {2016},
  pages = {5},
  abstract = {Computer simulations may be composed of scientific programs chained in a coherent flow and executed in High Performance Computing environments. These executions may present anomalies associated to the data that flows in parallel among programs. Several parallel code-profiling tools already support performance analysis, such as Tuning and Analysis Utilities (TAU) or provide fine-grained performance statistics such as the System Activity Report (SAR). However, these tools do not associate their results to their corresponding dataflows. Such analysis is fundamental to trace back the data origins of an error. In this paper, we propose to couple a workflow monitoring data approach to parallel code-profiling tools for workflow executions. The goal is to profile and debug parallel workflow executions by querying a database that is able to integrate performance, resource consumption, provenance, and domain data from simulation programs at runtime. We have implemented our data monitoring approach as a software component that was coupled to TAU and SAR code profiling tools. We show how querying the resulting integrated database enables domain-aware runtime steering of performance anomalies by using the astronomy Montage workflow, as a motivating example. We observe that the overhead introduced by our approach is negligible.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/silva_et_al_2016_integrating_domain-data_steering_with_code-profiling_tools_to_debug.pdf},
  language = {en}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/silver_et_al_2016_mastering_the_game_of_go_with_deep_neural_networks_and_tree_search.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{singh2016,
  title = {Cloud Resource Provisioning: Survey, Status and Future Research Directions},
  shorttitle = {Cloud Resource Provisioning},
  author = {Singh, Sukhpal and Chana, Inderveer},
  year = {2016},
  month = dec,
  volume = {49},
  pages = {1005--1069},
  issn = {0219-3116},
  doi = {10.1007/s10115-016-0922-3},
  abstract = {Cloud resource provisioning is a challenging job that may be compromised due to unavailability of the expected resources. Quality of Service (QoS) requirements of workloads derives the provisioning of appropriate resources to cloud workloads. Discovery of best workload\textendash resource pair based on application requirements of cloud users is an optimization problem. Acceptable QoS cannot be provided to the cloud users until provisioning of resources is offered as a crucial ability. QoS parameters-based resource provisioning technique is therefore required for efficient provisioning of resources. This research depicts a broad methodical literature analysis of cloud resource provisioning in general and cloud resource identification in specific. The existing research is categorized generally into various groups in the area of cloud resource provisioning. In this paper, a methodical analysis of resource provisioning in cloud computing is presented, in which resource management, resource provisioning, resource provisioning evolution, different types of resource provisioning mechanisms and their comparisons, benefits and open issues are described. This research work also highlights the previous research, current status and future directions of resource provisioning and management in cloud computing.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/singh_chana_2016_cloud_resource_provisioning.pdf},
  journal = {Knowledge and Information Systems},
  keywords = {_tablet,Autonomic computing,Cloud computing,Resource management,Resource provisioning,Resource provisioning mechanisms,Resource scheduling,Systematic review},
  language = {en},
  number = {3}
}

@article{singh2016a,
  title = {A {{Survey}} on {{Resource Scheduling}} in {{Cloud Computing}}: {{Issues}} and {{Challenges}}},
  shorttitle = {A {{Survey}} on {{Resource Scheduling}} in {{Cloud Computing}}},
  author = {Singh, Sukhpal and Chana, Inderveer},
  year = {2016},
  month = jun,
  volume = {14},
  pages = {217--264},
  issn = {1572-9184},
  doi = {10.1007/s10723-015-9359-2},
  abstract = {Resource scheduling in cloud is a challenging job and the scheduling of appropriate resources to cloud workloads depends on the QoS requirements of cloud applications. In cloud environment, heterogeneity, uncertainty and dispersion of resources encounters problems of allocation of resources, which cannot be addressed with existing resource allocation policies. Researchers still face troubles to select the efficient and appropriate resource scheduling algorithm for a specific workload from the existing literature of resource scheduling algorithms. This research depicts a broad methodical literature analysis of resource management in the area of cloud in general and cloud resource scheduling in specific. In this survey, standard methodical literature analysis technique is used based on a complete collection of 110 research papers out of large collection of 1206 research papers published in 19 foremost workshops, symposiums and conferences and 11 prominent journals. The current status of resource scheduling in cloud computing is distributed into various categories. Methodical analysis of resource scheduling in cloud computing is presented, resource scheduling algorithms and management, its types and benefits with tools, resource scheduling aspects and resource distribution policies are described. The literature concerning to thirteen types of resource scheduling algorithms has also been stated. Further, eight types of resource distribution policies are described. Methodical analysis of this research work will help researchers to find the important characteristics of resource scheduling algorithms and also will help to select most suitable algorithm for scheduling a specific workload. Future research directions have also been suggested in this research work.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/singh_chana_2016_a_survey_on_resource_scheduling_in_cloud_computing.pdf},
  journal = {Journal of Grid Computing},
  keywords = {_tablet,Cloud computing,Cloud resource scheduling,Cloud workloads,Resource distribution policies,Resource management,Resource provisioning,Resource scheduling algorithms,Resource scheduling aspects,Resource scheduling tools},
  language = {en},
  number = {2}
}

@article{sirisha2016,
  title = {Exploring the {{Efficacy}} of {{Branch}} and {{Bound Strategy}} for {{Scheduling Workflows}} on {{Heterogeneous Computing Systems}}},
  author = {Sirisha, D. and Vijayakumari, G.},
  year = {2016},
  month = jan,
  volume = {93},
  pages = {315--323},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2016.07.216},
  abstract = {Computationally complex applications featuring workflows comprises of modules that can be deployed on Heterogeneous Computing Systems (HCS) for accomplishing high performance. The problem of scheduling the workflows on HCS is proven to be NP-Complete. In the present work, Branch and Bound (B\&B) strategy for scheduling the workflows on HCS is proposed to attain globally optimal solutions. The primary merit of the proposed strategy is due to the estimation of the rank functions which are sharper and not complex. The proposed B\&B strategy expands the most promising states first. The sharper ranks aid in converging to the solution quickly by pruning the unpromising states which do not lead to an optimal solution. Therefore, the search space is drastically reduced hence higher performance can be expected. The experimental results reveal that the proposed B\&B scheme is efficient in exploring high potentials of B\&B strategy in finding exact solutions. The performance analysis on a set of benchmark workflows shows that the proposed B\&B strategy has generated optimal schedules for 94.37\% of the cases.},
  file = {/Users/rwb/Zotero/storage/AFQ9UN9A/S1877050916314570.html},
  journal = {Procedia Computer Science},
  keywords = {branch and bound strategy,heterogeneous computing systems,optimal solution,scheduling,workflow s},
  series = {Proceedings of the 6th {{International Conference}} on {{Advances}} in {{Computing}} and {{Communications}}}
}

@misc{sitek2016,
  title = {A {{Hybrid Programming Framework}} for {{Modeling}} and {{Solving Constraint Satisfaction}} and {{Optimization Problems}}},
  author = {Sitek, Pawe\l{} and Wikarek, Jaros\l aw},
  year = {2016},
  doi = {10.1155/2016/5102616},
  abstract = {This paper proposes a hybrid programming framework for modeling and solving of constraint satisfaction problems (CSPs) and constraint optimization problems (COPs). Two paradigms, CLP (constraint logic programming) and MP (mathematical programming), are integrated in the framework. The integration is supplemented with the original method of problem transformation, used in the framework as a presolving method. The transformation substantially reduces the feasible solution space. The framework automatically generates CSP and COP models based on current values of data instances, questions asked by a user, and set of predicates and facts of the problem being modeled, which altogether constitute a knowledge database for the given problem. This dynamic generation of dedicated models, based on the knowledge base, together with the parameters changing externally, for example, the user's questions, is the implementation of the autonomous search concept. The models are solved using the internal or external solvers integrated with the framework. The architecture of the framework as well as its implementation outline is also included in the paper. The effectiveness of the framework regarding the modeling and solution search is assessed through the illustrative examples relating to scheduling problems with additional constrained resources.},
  file = {/Users/rwb/Zotero/storage/SX3JY77Q/abs.html},
  howpublished = {https://www.hindawi.com/journals/sp/2016/5102616/abs/},
  journal = {Scientific Programming},
  language = {en},
  type = {Research Article}
}

@article{smanchat2013,
  title = {Scheduling Parameter Sweep Workflow in the {{Grid}} Based on Resource Competition},
  author = {Smanchat, Sucha and Indrawan, Maria and Ling, Sea and Enticott, Colin and Abramson, David},
  year = {2013},
  month = jul,
  volume = {29},
  pages = {1164--1183},
  issn = {0167-739X},
  doi = {10.1016/j.future.2013.01.005},
  abstract = {Workflow technology has been adopted in scientific domains to orchestrate and automate scientific processes in order to facilitate experimentation. Such scientific workflows often involve large data sets and intensive computation that necessitate the use of the Grid. To execute a scientific workflow in the Grid, tasks within the workflow are assigned to Grid resources. Thus, to ensure efficient execution of the workflow, Grid workflow scheduling is required to manage the allocation of Grid resources. Although many Grid workflow scheduling techniques exist, they are mainly designed for the execution of a single workflow. This is not the case with parameter sweep workflows, which are used for parametric study and optimisation. A parameter sweep workflow is executed numerous times with different input parameters in order to determine the effect of each parameter combination on the experiment. While executing multiple instances of a parameter sweep workflow in parallel can reduce the time required for the overall execution, this parallel execution introduces new challenges to Grid workflow scheduling. Not only is a scheduling algorithm that is able to manage multiple workflow instances required, but this algorithm also needs the ability to schedule tasks across multiple workflow instances judiciously, as tasks may require the same set of Grid resources. Without appropriate resource allocation, resource competition problem could arise. We propose a new Grid workflow scheduling technique for parameter sweep workflow called the Besom scheduling algorithm. The scheduling decision of our algorithm is based on the resource dependencies of tasks in the workflow, as well as conventional Grid resource-performance metrics. In addition, the proposed technique is extended to handle loop structures in scientific workflows without using existing loop-unrolling techniques. The Besom algorithm is evaluated using simulations with a variety of scenarios. A comparison between the simulation results of the Besom algorithm and of the three existing Grid workflow scheduling algorithms shows that the Besom algorithm is able to perform better than the existing algorithms for workflows that have complex structures and that involve overlapping resource dependencies of tasks.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/smanchat_et_al_2013_scheduling_parameter_sweep_workflow_in_the_grid_based_on_resource_competition.pdf;/Users/rwb/Zotero/storage/S6EIUVDV/S0167739X13000198.html},
  journal = {Future Generation Computer Systems},
  keywords = {Grid workflow,Parameter sweep,Resource competition,Workflow scheduler,Workflow scheduling algorithm},
  number = {5},
  series = {Special Section: {{Hybrid Cloud Computing}}}
}

@article{smanchat2015,
  title = {Taxonomies of Workflow Scheduling Problem and Techniques in the Cloud},
  author = {Smanchat, Sucha and Viriyapant, Kanchana},
  year = {2015},
  month = nov,
  volume = {52},
  pages = {1--12},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.04.019},
  abstract = {Scientific workflows, like other applications, benefit from the cloud computing, which offers access to virtually unlimited resources provisioned elastically on demand. In order to efficiently execute a workflow in the cloud, scheduling is required to address many new aspects introduced by cloud resource provisioning. In the last few years, many techniques have been proposed to tackle different cloud environments enabled by the flexible nature of the cloud, leading to the techniques of different designs. In this paper, taxonomies of cloud workflow scheduling problem and techniques are proposed based on analytical review. We identify and explain the aspects and classifications unique to workflow scheduling in the cloud environment in three categories, namely, scheduling process, task and resource. Lastly, review of several scheduling techniques are included and classified onto the proposed taxonomies. We hope that our taxonomies serve as a stepping stone for those entering this research area and for further development of scheduling technique.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/smanchat_viriyapant_2015_taxonomies_of_workflow_scheduling_problem_and_techniques_in_the_cloud.pdf;/Users/rwb/Zotero/storage/9ITZKILQ/S0167739X15001776.html},
  journal = {Future Generation Computer Systems},
  keywords = {Cloud computing,Cloud workflow,Workflow scheduling,Workflow scheduling taxonomy},
  series = {Special {{Section}}: {{Cloud Computing}}: {{Security}}, {{Privacy}} and {{Practice}}}
}

@article{smith2000,
  title = {Bridging the Gap between Planning and Scheduling},
  author = {Smith, David E. and Frank, Jeremy and J{\'o}nsson, Ari K.},
  year = {2000},
  month = mar,
  volume = {15},
  pages = {47--83},
  issn = {02698889},
  doi = {10.1017/S0269888900001089},
  abstract = {Planning research in Artificial Intelligence (AI) has often focused on problems where there are cascading levels of action choice and complex interactions between actions. In contrast, Scheduling research has focused on much larger problems where there is little action choice, but the resulting ordering problem is hard. In this paper, we give an overview of AI planning and scheduling techniques, focusing on their similarities, differences, and limitations. We also argue that many difficult practical problems lie somewhere between planning and scheduling, and that neither area has the right set of tools for solving these vexing problems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2000/smith_et_al_2000_bridging_the_gap_between_planning_and_scheduling.pdf},
  journal = {The Knowledge Engineering Review},
  keywords = {_tablet},
  language = {en},
  number = {1}
}

@article{sochat2018,
  title = {The {{Scientific Filesystem}}},
  author = {Sochat, Vanessa},
  year = {2018},
  month = may,
  volume = {7},
  doi = {10.1093/gigascience/giy023},
  abstract = {AbstractBackground.  Here, we present the Scientific Filesystem (SCIF), an organizational format that supports exposure of executables and metadata for discover},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/sochat_2018_the_scientific_filesystem.pdf;/Users/rwb/Zotero/storage/R92PXAJL/4931737.html},
  journal = {GigaScience},
  language = {en},
  number = {5}
}

@inproceedings{sonmez2010,
  title = {Performance {{Analysis}} of {{Dynamic Workflow Scheduling}} in {{Multicluster Grids}}},
  booktitle = {Proceedings of the 19th {{ACM International Symposium}} on {{High Performance Distributed Computing}}},
  author = {Sonmez, Ozan and Yigitbasi, Nezih and Abrishami, Saeid and Iosup, Alexandru and Epema, Dick},
  year = {2010},
  pages = {49--60},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1851476.1851483},
  abstract = {Scientists increasingly rely on the execution of workflows in grids to obtain results from complex mixtures of applications. However, the inherently dynamic nature of grid workflow scheduling, stemming from the unavailability of scheduling information and from resource contention among the (multiple) workflows and the non-workflow system load, may lead to poor or unpredictable performance. In this paper we present a comprehensive and realistic investigation of the performance of a wide range of dynamic workflow scheduling policies in multicluster grids. We first introduce a taxonomy of grid workflow scheduling policies that is based on the amount of dynamic information used in the scheduling process, and map to this taxonomy seven such policies across the full spectrum of information use. Then, we analyze the performance of these scheduling policies through simulations and experiments in a real multicluster grid. We find that there is no single grid workflow scheduling policy with good performance across all the investigated scenarios. We also find from our real system experiments that with demanding workloads, the limitations of the head-nodes of the grid clusters may lead to performance loss not expected from the simulation results. We show that task throttling, that is, limiting the per-workflow number of tasks dispatched to the system, prevents the head-nodes from becoming overloaded while largely preserving performance, at least for communication-intensive workflows.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/sonmez_et_al_2010_performance_analysis_of_dynamic_workflow_scheduling_in_multicluster_grids.pdf},
  isbn = {978-1-60558-942-8},
  keywords = {grid scheduling,performance evaluation,workflows},
  series = {{{HPDC}} '10}
}

@article{srinivas1994,
  title = {Muiltiobjective {{Optimization Using Nondominated Sorting}} in {{Genetic Algorithms}}},
  author = {Srinivas, N. and Deb, Kalyanmoy},
  year = {1994},
  month = sep,
  volume = {2},
  pages = {221--248},
  issn = {1063-6560},
  doi = {10.1162/evco.1994.2.3.221},
  abstract = {In trying to solve multiobjective optimization problems, many traditional methods scalarize the objective vector into a single objective. In those cases, the obtained solution is highly sensitive to the weight vector used in the scalarization process and demands that the user have knowledge about the underlying problem. Moreover, in solving multiobjective problems, designers may be interested in a set of Pareto-optimal points, instead of a single point. Since genetic algorithms (GAs) work with a population of points, it seems natural to use GAs in multiobjective optimization problems to capture a number of solutions simultaneously. Although a vector evaluated GA (VEGA) has been implemented by Schaffer and has been tried to solve a number of multiobjective problems, the algorithm seems to have bias toward some regions. In this paper, we investigate Goldberg's notion of nondominated sorting in GAs along with a niche and speciation method to find multiple Pareto-optimal points simultaneously. The proof-of-principle results obtained on three problems used by Schaffer and others suggest that the proposed method can be extended to higher dimensional and more difficult multiobjective problems. A number of suggestions for extension and application of the algorithm are also discussed.},
  file = {/Users/rwb/Dropbox/PhD/zotero/1994/srinivas_deb_1994_muiltiobjective_optimization_using_nondominated_sorting_in_genetic_algorithms.pdf},
  journal = {Evol. Comput.},
  keywords = {Multiobjective optimization,nondominated sorting,phenotypic sharing,ranking selection},
  number = {3}
}

@article{stavrinides2017,
  ids = {2017,stavrinides2017a},
  title = {Different Aspects of Workflow Scheduling in Large-Scale Distributed Systems},
  author = {Stavrinides, Georgios L. and Duro, Francisco Rodrigo and Karatza, Helen D. and Blas, Javier Garcia and Carretero, Jesus},
  year = {2017},
  month = jan,
  volume = {70},
  pages = {120--134},
  issn = {1569-190X},
  doi = {10.1016/j.simpat.2016.10.009},
  abstract = {As large-scale distributed systems gain momentum, the scheduling of workflow applications with multiple requirements in such computing platforms has become a crucial area of research. In this paper, we investigate the workflow scheduling problem in large-scale distributed systems, from the Quality of Service (QoS) and data locality perspectives. We present a scheduling approach, considering two models of synchronization for the tasks in a workflow application: (a) communication through the network and (b) communication through temporary files. Specifically, we investigate via simulation the performance of a heterogeneous distributed system, where multiple soft real-time workflow applications arrive dynamically. The applications are scheduled under various tardiness bounds, taking into account the communication cost in the first case study and the I/O cost and data locality in the second. The simulation results provide useful insights into the impact of tardiness bound and data locality on the system performance.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/stavrinides_et_al_2017_different_aspects_of_workflow_scheduling_in_large-scale_distributed_systems.pdf;/Users/rwb/Dropbox/PhD/zotero/2017/stavrinides_et_al_2017_different_aspects_of_workflow_scheduling_in_large-scale_distributed_systems2.pdf;/Users/rwb/Zotero/storage/833927WU/S1569190X16302519.html;/Users/rwb/Zotero/storage/8FK7EGXZ/S1569190X16302519.html;/Users/rwb/Zotero/storage/L3TDNEN9/S1569190X16302519.html},
  journal = {Simulation Modelling Practice and Theory},
  keywords = {Data locality,Large-scale distributed systems,Quality of Service,Ultrascale systems,Workflow scheduling}
}

@article{stein2018,
  title = {The {{Serverless Scheduling Problem}} and {{NOAH}}},
  author = {Stein, Manuel},
  year = {2018},
  month = sep,
  abstract = {The serverless scheduling problem poses a new challenge to Cloud service platform providers because it is rather a job scheduling problem than a traditional resource allocation or request load balancing problem. Traditionally, elastic cloud applications use managed virtual resource allocation and employ request load balancers to orchestrate the deployment. With serverless, the provider needs to solve both the load balancing and the allocation. This work reviews the current Apache OpenWhisk serverless event load balancing and a noncooperative game-theoretic load balancing approach for response time minimization in distributed systems. It is shown by simulation that neither performs well under high system utilization which inspired a noncooperative online allocation heuristic that allows tuning the trade-off between for response time and resource cost of each serverless function.},
  archivePrefix = {arXiv},
  eprint = {1809.06100},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/stein_2018_the_serverless_scheduling_problem_and_noah.pdf;/Users/rwb/Zotero/storage/YSDGU2V2/1809.html},
  journal = {arXiv:1809.06100 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryClass = {cs}
}

@inproceedings{sujana2015,
  title = {Game Multi Objective Scheduling Algorithm for Scientific Workflows in Cloud Computing},
  booktitle = {2015 {{International Conference}} on {{Circuits}}, {{Power}} and {{Computing Technologies}} [{{ICCPCT}}-2015]},
  author = {Sujana, J. A. J. and Revathi, T. and Karthiga, G. and Raj, R. V.},
  year = {2015},
  month = mar,
  pages = {1--6},
  doi = {10.1109/ICCPCT.2015.7159423},
  abstract = {Cloud computing is the latest utility computing that enables the dynamic provisioning for applications over the Internet. Cloud computing provide large number of opportunities to solve large scale scientific problems. Scheduling the tasks in the workflows is an NP-hard problem and in this work we focus on optimizing the scheduling process of the workflow. Scheduling of the workflow applications that satisfy the given constraints for the scientific tasks is an essential requirement for the workflow scheduling. Mapping of each task to suitable resource and allowing the task to satisfy performance constraints is the main aim of this paper. In the present work we propose game theoretic algorithm for multi-objective scheduling of scientific workflows in cloud computing environment. The scheduling problem is formulated as a new sequential cooperative game based on two user objectives that are execution time and economic cost while fulfilling two constraints network bandwidth and storage requirements. We call it as Game Multi Objective (GMO) algorithm. In this paper we apply Game Multi Objective algorithm for minimizing the execution time and cost of an workflow application.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/sujana_et_al_2015_game_multi_objective_scheduling_algorithm_for_scientific_workflows_in_cloud.pdf;/Users/rwb/Zotero/storage/9HJQDC93/7159423.html},
  keywords = {bag of tasks,Bandwidth,cloud computing,Computers,constraints network bandwidth,economic cost,game multiobjective scheduling algorithm,game theoretic algorithm,game theory,Games,GMO,GMO algorithm,Internet,Mathematical model,Multi-objective Scheduling,NP-hard problem,optimisation,Optimization,performance constraint,Processor scheduling,Resource management,scheduling,scheduling process,scientific workflow,sequential cooperative game,storage management,storage requirement,workflow management software,workflow scheduling}
}

@article{sureshbabu2012,
  title = {Reliability {{Optimization Model}} for {{Redundant Systems}} with {{Multiple Constraints}}},
  author = {SureshBabu, S. V. and Maheswar, D. and Ranganath, G.},
  year = {2012},
  month = jan,
  volume = {38},
  pages = {7--14},
  issn = {1877-7058},
  doi = {10.1016/j.proeng.2012.06.002},
  abstract = {The Reliability Optimization Models for Redundant Systems with multiple constraints for one mathematical function is established by applying Lagrangean approach, the related case problem is presented to find the component reliabilities (rj), the number of components in each stage (xj), stage reliability (Rj) and the System Reliability (Rs). The purpose of this Paper is to optimize a class of Integrated Reliability Model for Redundant Systems with weight as additional constraint apart from basic cost constraint. By taking the mathematical function the optimum component reliability, stage reliability, the number of components in each stage and the system reliability are determined after taking the pre-determined values of Cost and Weight. In this work, an attempt is made to develop an integrated reliability redundant model for a Series \textendash{} Parallel configuration subject to the multiple constraints. Generally reliability is treated as the function of Cost but in any given practical situation apart from Cost other constraint like Weight will have hidden impact on the reliability of the system. In this model the Lagrangean technique is implemented to determine the Cost and Weight as constraints. The model has yielded very encouraging results and it can be applied to any type of system, simple or complex. The advantage of this model is very flexible and requires little processing time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/sureshbabu_et_al_2012_reliability_optimization_model_for_redundant_systems_with_multiple_constraints.pdf;/Users/rwb/Zotero/storage/RHFY4EDK/S1877705812019157.html},
  journal = {Procedia Engineering},
  keywords = {Multiple constraints,Redundant system,Reliability Optimization Model,System Reliability},
  series = {{{INTERNATIONAL CONFERENCE ON MODELLING OPTIMIZATION AND COMPUTING}}}
}

@book{sutton2018,
  ids = {sutton2018a},
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {/Users/rwb/Zotero/storage/IP6UKWNV/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf;/Users/rwb/Zotero/storage/MPQFT6JZ/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{szekely,
  ids = {maheswaran},
  title = {Scaling in {{Domains}} with {{Uncertainty}}: {{Criticality}}-{{Sensitive Coordination}}},
  shorttitle = {Scaling in {{Domains}} with {{Uncertainty}}},
  author = {Szekely, Pedro},
  abstract = {ABSTRACT We consider a team of agents that are required to coordinate their actions in order to maximize a global objective. Our domains are characterized by uncertainty, dynamism, and distributed information. Determining appropriate actions becomes},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/maheswaran_et_al_scaling_in_domains_with_uncertainty.pdf;/Users/rwb/Dropbox/PhD/zotero/undefined/szekely_scaling_in_domains_with_uncertainty.pdf;/Users/rwb/Zotero/storage/TENKCIEA/Scaling_in_Domains_with_Uncertainty_Criticality-Sensitive_Coordination.html},
  language = {en}
}

@article{szepesvari,
  title = {Reinforcement {{Learning}}: {{Theory}} and {{Practice}}},
  author = {Szepesvari, Csaba and Thege, Konkoly},
  pages = {10},
  abstract = {Ve consider reinforcement learning methods for the solution of complex sequential optimization problems. In particular, the soundness of tV'lO methods proposed for the solution of partially obsenrable problems will be shown. The first method suggests a state-estimation scheme and requires mild a priori knowledge, \textbackslash vhile the second method assumes that a significant amount of abstract knowl\- edge is available about the decision problem and uses this knowledge to setup a macro-hierarchy to turn the partially observable problem into another one which can already be handled using methods worked out for observable problems. This second method is also illustrated with some experiments on a. real-robot.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/szepesvari_thege_reinforcement_learning.pdf},
  language = {en}
}

@article{tabuada2007,
  title = {Event-{{Triggered Real}}-{{Time Scheduling}} of {{Stabilizing Control Tasks}}},
  author = {Tabuada, P.},
  year = {2007},
  month = sep,
  volume = {52},
  pages = {1680--1685},
  issn = {0018-9286},
  doi = {10.1109/TAC.2007.904277},
  abstract = {In this note, we revisit the problem of scheduling stabilizing control tasks on embedded processors. We start from the paradigm that a real-time scheduler could be regarded as a feedback controller that decides which task is executed at any given instant. This controller has for objective guaranteeing that (control unrelated) software tasks meet their deadlines and that stabilizing control tasks asymptotically stabilize the plant. We investigate a simple event-triggered scheduler based on this feedback paradigm and show how it leads to guaranteed performance thus relaxing the more traditional periodic execution requirements.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/tabuada_2007_event-triggered_real-time_scheduling_of_stabilizing_control_tasks.pdf;/Users/rwb/Zotero/storage/CJMPUS2Z/4303247.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Adaptive control,asymptotic stability,Communication system control,Computer networks,control engineering computing,control unrelated software tasks,Embedded computing,embedded processors,embedded systems,Event-triggered,event-triggered real-time scheduling,event-triggered scheduler,feedback,Feedback,feedback controller,input-to-state stability,Microprocessors,Physics computing,Processor scheduling,real-time scheduler,real-time scheduling,scheduling,Scheduling algorithm,scheduling stabilizing control tasks,Stability,task analysis},
  number = {9}
}

@inproceedings{takizawa2018,
  title = {A {{Scalable Multi}}-{{Granular Data Model}} for {{Data Parallel Workflows}}},
  booktitle = {Proceedings of the {{International Conference}} on {{High Performance Computing}} in {{Asia}}-{{Pacific Region}}},
  author = {Takizawa, Shinichiro and Matsuda, Motohiko and Maruyama, Naoya and Nakamura, Yoshifumi},
  year = {2018},
  pages = {251--260},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3149457.3154483},
  abstract = {Scientific applications consist of many tasks and each task has different requirements for the degree of parallelism and data access pattern. To satisfy these requirements, a task scheduling has to assign required number of processes to each task and task's input has to be decomposed and arranged to these processes by considering data access pattern to exploit data locality. However, hand-writing these code is a troublesome and error-prone work. We propose a multi-view data model where users can specify rules of data decomposition for multi-dimensional data to change data layout on top of processes and define unit of parallel processing by simple directives. Our framework conducts data arrangement and affinity-aware task scheduling transparently from users by following the specified rules. Through a case study of a lattice QCD simulation program, we confirmed that our proposal reduced programming efforts against hand-written MPI code with performance penalties up to 17\%.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/takizawa_et_al_2018_a_scalable_multi-granular_data_model_for_data_parallel_workflows.pdf},
  isbn = {978-1-4503-5372-4},
  series = {{{HPC Asia}} 2018}
}

@article{talbi2016,
  title = {Combining Metaheuristics with Mathematical Programming, Constraint Programming and Machine Learning},
  author = {Talbi, El-Ghazali},
  year = {2016},
  month = may,
  volume = {240},
  pages = {171--215},
  issn = {1572-9338},
  doi = {10.1007/s10479-015-2034-y},
  abstract = {During the last years, interest on hybrid metaheuristics has risen considerably in the field of optimization and machine learning. The best results found for many optimization problems in science and industry are obtained by hybrid optimization algorithms. Combinations of optimization tools such as metaheuristics, mathematical programming, constraint programming and machine learning, have provided very efficient optimization algorithms. Four different types of combinations are considered in this paper: (1) Combining metaheuristics with complementary metaheuristics. (2) Combining metaheuristics with exact methods from mathematical programming approaches which are mostly used in the operations research community. (3) Combining metaheuristics with constraint programming approaches developed in the artificial intelligence community. (4) Combining metaheuristics with machine learning and data mining techniques.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/talbi_2016_combining_metaheuristics_with_mathematical_programming,_constraint_programming.pdf},
  journal = {Annals of Operations Research},
  keywords = {Constraint programming,Data mining,Hybrid metaheuristics,Machine learning,Mathematical programming,Matheuristics},
  language = {en},
  number = {1}
}

@inproceedings{tang2018,
  title = {A {{Group}}-Based {{Approach}} to {{Improve Multifactorial Evolutionary Algorithm}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Tang, Jing and Chen, Yingke and Deng, Zixuan and Xiang, Yanping and Paul Joy, Colin},
  year = {2018},
  month = jul,
  pages = {3870--3876},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/538},
  abstract = {Multifactorial evolutionary algorithm (MFEA) exploits the parallelism of population-based evolutionary algorithm and provides an efficient way to evolve individuals for solving multiple tasks concurrently. Its efficiency is derived by implicitly transferring the genetic information among tasks. However, MFEA doesn't distinguish the information quality in the transfer compromising the algorithm performance. We propose a group-based MFEA that groups tasks of similar types and selectively transfers the genetic information only within the groups. We also develop a new selection criterion and an additional mating selection mechanism in order to strengthen the effectiveness and efficiency of the improved MFEA. We conduct the experiments in both the cross-domain and intra-domain problems.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/tang_et_al_2018_a_group-based_approach_to_improve_multifactorial_evolutionary_algorithm.pdf},
  isbn = {978-0-9992411-2-7},
  keywords = {_tablet},
  language = {en}
}

@inproceedings{tao2009,
  title = {{{QoS Constrained Grid Workflow Scheduling Optimization Based}} on a {{Novel PSO Algorithm}}},
  booktitle = {2009 {{Eighth International Conference}} on {{Grid}} and {{Cooperative Computing}}},
  author = {Tao, Q. and Chang, H. and Yi, Y. and Gu, C. and Yu, Y.},
  year = {2009},
  month = aug,
  pages = {153--159},
  doi = {10.1109/GCC.2009.39},
  abstract = {Currently, computational grid, service grid and data grid are becoming richer and more complex. Grid infrastructure composed of heterogeneous resource is widely distributed, workflow scheduling problem in grid environment described by directed acrylic graph (DAG) becomes an important and difficult problem. Meanwhile, the research on workflow scheduling problem in grid environment mainly focuses on the time and cost constrained optimization, but the key problems about stability, flexibility, security and load balancing arenpsilat considered adequately. Aiming at these problems, we redefine parameters of quality of service (QoS) and the model of grid workflow scheduling, and put forward a rotary hybrid discrete particle swarm optimization (RHCurrently, computational grid, service grid and data grid are becoming richer and more complex. Grid infrastructure composed of heterogeneous resource is widely distributed, workflow scheduling problem in grid environment described by directed acrylic graph (DAG) becomes an important and difficult problem. Meanwhile, the research on workflow scheduling problem in grid environment mainly focuses on the time and cost constrained optimization, but the key problems about stability, flexibility, security and load balancing arenpsilat considered adequately. Aiming at these problems, we redefine parameters of quality of service (QoS) and the model of grid workflow scheduling, and put forward a rotary hybrid discrete particle swarm optimization (RHDPSO) algorithm, in which double extremums are disturbed by the method of random time sequence based on rotation discretization, to overcome premature convergence and local optimum. The simulation results show that the RHDPSO algorithm has fast convergence, high precision and strong robustness, and can effectively restrain premature convergence, compared with DPSO. The performance of our algorithm is very promising, scheduling, and put forward a rotary hybrid discrete particle swarm optimization- (RHDPSO) algorithm, in which double extremums are disturbed by the method of random time sequence based on rotation discretization, to overcome premature convergence and local optimum. The simulation results show that the RHDPSO algorithm has fast convergence, high precision and strong robustness, and can effectively restrain premature convergence, compared with DPSO. The performance of our algorithm is very promising.DPSO) algorithm, in which double extremums are disturbed by the method of random time sequence based on rotation discretization, to overcome premature convergence and local optimum. The simulation results show that the RHDPSO algorithm has fast convergence, high precision and strong robustness, and can effectively restrain premature convergence, compared with DPSO. The performance of our algorithm is very promising, scheduling, and put forward a rotary hybrid discrete particle swarm optimization (RHDPSO) algorithm, in which double extremums are disturbed by the method of random time sequence based on rotation discretization, to overcome premature convergence and local optimum. The simulation results show that the RHDPSO algorithm has fast convergence, high precision and strong robustness, and can effectively restrain premature convergence, compared with DPSO. The performance of our algorithm is very promising.},
  file = {/Users/rwb/Zotero/storage/ME59P2UQ/5280159.html},
  keywords = {computational grid,Constraint optimization,Convergence,cost constrained optimization,Cost function,DAG,data grid,directed acrylic graph,directed graphs,discrete particle swarm optimization,double extremums disturbing,grid computing,Grid computing,grid workflow,heterogeneous resource,Logistic sequence,multi-QoS,particle swarm optimisation,Particle swarm optimization,Processor scheduling,QoS constrained grid workflow scheduling,quality of service,Quality of service,RHDPSO,Robustness,scheduling,Scheduling algorithm,service grid,Time factors,workflow management software}
}

@phdthesis{terblanche2017,
  title = {{Resource constrained project scheduling models and algorithms applied to underground mining}},
  author = {Terblanche, S. E.},
  year = {2017},
  month = dec,
  abstract = {ENGLISH ABSTRACT: The resource constrained project scheduling problem (RCSP) involves the scheduling of a number of activities over time, where each activity consumes a unit of some resource per time period. 
For a feasible solution to exist, the total resource consumption per time period must be less than 
or equal to the available resources. In addition, the order in which activities may be scheduled is determined by a precedence graph. The nodes of this directed graph represent the various activities and each directed edge a precedence relationship. A multitude of model formulations for the RCSP exist in the literature as do various solution approaches. Some of the frequently 
applied objective functions include the minimisation of the makespan, the minimisation of a tardiness penalty cost, and the maximisation of net present value. 
The advent of practical computer technology during the late 1950s has meant that various industrial problems can now be solved by computer algorithms. It soon became clear, however, that certain types of problems are inherently difficult and in some cases even impossible to solve. 
Even today scheduling problems exist which have no more than 60 tasks to be scheduled, but 
which cannot be solved to optimality within reasonable time using the latest algorithmic and computer technology. In this dissertation, the challenges of underground mine planning are addressed by employing 
RCSP models and algorithms. Underground mine planning entails the scheduling of mining activities in a manner that the most economical value is derived, while satisfying constraints related to resource requirements and physical limitations due to the properties of the mine infrastructure. 
Modelling extensions that address mining-specific requirements are of specific interest. For instance, the modelling of transfer delay constraints are especially useful in a mechanised mining environment where the movement of large machinery from one point to another may cause significant delays in a mine production schedule. Other mining-specific requirements include 
the modelling of uncertainty in resource requirements as well as the formulation of scheduling 
models that facilitate selective scheduling. 
Details of existing RCSP formulations in the literature are provided and results from empirical tests are presented to evaluate the suitability of adopting a resource  
flow-based RCSP formulation to solve the underground mine scheduling optimisation problem. Modifications to the resource flow formulation are proposed for the purpose of accommodating the maximisation of 
net present value. Due to the computational complexity of the underground mine scheduling problem, variable and constraint reduction approaches are suggested. In addition, a Benders decomposition approach is described which is capable of improving the computation of feasible 
solutions for large problem instances. 
Computational results presented in this dissertation are based on both randomly generated data and data from a real South African underground mine. Based on these results it is found that the best performing model reformulation involves the use of a resource  
ow-based model in conjunction with a constraint aggregation and graph reduction approach. The Benders decomposition approach, implemented within a branch-and-cut framework, scales well for problem instances with a large number of activities and resources. This is a significant contribution within the context of mining, especially considering the large number of resources that have to be accommodated when solving underground mine scheduling optimisation problems.},
  copyright = {Stellenbosch University},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/terblanche_2017_resource_constrained_project_scheduling_models_and_algorithms_applied_to.pdf;/Users/rwb/Zotero/storage/4XVWTT4Z/102979.html},
  language = {en\_ZA},
  school = {Stellenbosch : Stellenbosch University},
  type = {{Thesis}}
}

@article{terekhov2014,
  title = {Integrating {{Queueing Theory}} and {{Scheduling}} for {{Dynamic Scheduling Problems}}},
  author = {Terekhov, D. and Tran, T. T. and Down, D. G. and Beck, J.C.},
  year = {2014},
  month = jul,
  volume = {50},
  pages = {535--572},
  issn = {1076-9757},
  doi = {10.1613/jair.4278},
  abstract = {Dynamic scheduling problems consist of both challenging combinatorics, as found in classical scheduling problems, and stochastics due to uncertainty about the arrival times, resource requirements, and processing times of jobs. To address these two challenges, we investigate the integration of queueing theory and scheduling. The former reasons about long-run stochastic system characteristics, whereas the latter typically deals with shortterm combinatorics. We investigate two simple problems to isolate the core differences and potential synergies between the two approaches: a two-machine dynamic flowshop and a flexible queueing network. We show for the first time that stability, a fundamental characteristic in queueing theory, can be applied to approaches that periodically solve combinatorial scheduling problems. We empirically demonstrate that for a dynamic flowshop, the use of combinatorial reasoning has little impact on schedule quality beyond queueing approaches. In contrast, for the more complicated flexible queueing network, a novel algorithm that combines long-term guidance from queueing theory with short-term combinatorial decision making outperforms all other tested approaches. To our knowledge, this is the first time that such a hybrid of queueing theory and scheduling techniques has been proposed and evaluated.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/terekhov_et_al_2014_integrating_queueing_theory_and_scheduling_for_dynamic_scheduling_problems.pdf},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {_tablet},
  language = {en}
}

@article{terekhov2014a,
  title = {Queueing-Theoretic Approaches for Dynamic Scheduling: {{A}} Survey},
  shorttitle = {Queueing-Theoretic Approaches for Dynamic Scheduling},
  author = {Terekhov, Daria and Down, Douglas G. and Beck, J. Christopher},
  year = {2014},
  month = jul,
  volume = {19},
  pages = {105--129},
  issn = {1876-7354},
  doi = {10.1016/j.sorms.2014.09.001},
  abstract = {Within the combinatorial scheduling community, there has been an increasing interest in modelling and solving scheduling problems in dynamic environments. Such problems have also been considered in the field of queueing theory, but very few papers take advantage of developments in both areas, and literature surveys on dynamic scheduling usually make no mention of queueing approaches. In this paper, we provide an overview of queueing-theoretic models and methods that are relevant to scheduling in dynamic settings. This paper provides a context for investigating the integration of queueing theory and scheduling approaches with the goal of more effectively solving scheduling problems arising in dynamic environments.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/terekhov_et_al_2014_queueing-theoretic_approaches_for_dynamic_scheduling.pdf;/Users/rwb/Zotero/storage/GMW37AGH/S1876735414000233.html},
  journal = {Surveys in Operations Research and Management Science},
  keywords = {_tablet},
  number = {2}
}

@incollection{thain2003,
  title = {Condor and the {{Grid}}},
  booktitle = {Wiley {{Series}} in {{Communications Networking}} \& {{Distributed Systems}}},
  author = {Thain, Douglas and Tannenbaum, Todd and Livny, Miron},
  editor = {Berman, Fran and Fox, Geoffrey and Hey, Tony},
  year = {2003},
  month = mar,
  pages = {299--335},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/0470867167.ch11},
  abstract = {Since 1984, the Condor project has helped ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must reflect the sociology of communities. Throughout, we reflect on the lessons of experience and chart the course travelled by research ideas as they grow into production systems.},
  file = {/Users/rwb/Zotero/storage/D444QDF7/thain_et_al_2003_condor_and_the_grid.pdf},
  isbn = {978-0-470-85319-1 978-0-470-86716-7},
  keywords = {_tablet},
  language = {en}
}

@article{thain2005,
  title = {Distributed Computing in Practice: The {{Condor}} Experience},
  shorttitle = {Distributed Computing in Practice},
  author = {Thain, Douglas and Tannenbaum, Todd and Livny, Miron},
  year = {2005},
  month = feb,
  volume = {17},
  pages = {323--356},
  issn = {1532-0626, 1532-0634},
  doi = {10.1002/cpe.938},
  abstract = {Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.},
  file = {/Users/rwb/Zotero/storage/KFRW3AGD/thain_et_al_2005_distributed_computing_in_practice.pdf},
  journal = {Concurrency and Computation: Practice and Experience},
  keywords = {_tablet},
  language = {en},
  number = {2-4}
}

@book{thubui2008,
  title = {Multi-{{Objective Optimization}} in {{Computational Intelligence}}: {{Theory}} and {{Practice}}},
  shorttitle = {Multi-{{Objective Optimization}} in {{Computational Intelligence}}},
  editor = {Thu Bui, Lam and Alam, Sameer},
  year = {2008},
  publisher = {{IGI Global}},
  doi = {10.4018/978-1-59904-498-9},
  abstract = {This chapter is devoted to summarize common concepts related to multi-objective optimization (MO). An overview of ``traditional'' as well as CI-based MO is given. Further, all aspects of performance assessment for MO techniques are discussed. Finally, challenges facing MO techniques are addressed. All of these description and analysis give the readers basic knowledge for understandings the rest of the book.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/thu_bui_alam_2008_multi-objective_optimization_in_computational_intelligence.pdf},
  isbn = {978-1-59904-498-9 978-1-59904-500-9},
  language = {en}
}

@article{tolosana-calasanz2017,
  title = {Computational Resource Management for Data-driven Applications with Deadline Constraints},
  author = {Tolosana-Calasanz, Rafael and Diaz-Montes, Javier and Rana, Omer F. and Parashar, Manish and Xydas, Erotokritos and Marmaras, Charalampos and Papadopoulos, Panagiotis and Cipcigan, Liana},
  year = {2017},
  month = apr,
  volume = {29},
  issn = {1532-0634},
  doi = {10.1002/cpe.4018},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/tolosanacalasanz_et_al_2017_computational_resource_management_for_datadriven_applications_with_deadline.pdf;/Users/rwb/Zotero/storage/6JPWD54K/cpe.html},
  journal = {Concurrency and Computation: Practice and Experience},
  language = {en},
  number = {8}
}

@article{tompkins,
  title = {Optimization {{Techniques}} for {{Task Allocation}} and {{Scheduling}} in {{Distributed Multi}}-{{Agent Operations}}},
  author = {Tompkins, Mark F},
  pages = {107},
  abstract = {This thesis examines scenarios where multiple autonomous agents collaborate in order to accomplish a global objective. In the environment that we consider, there is a network of agents, each of which offers different sets of capabilities or services that can be used to perform various tasks. In this environment, an agent formulates a problem, divides it into a precedence-constrained set of sub-problems, and determines the optimal allocation of these sub-problems/tasks to other agents so that they are completed in the shortest amount of time. The resulting schedule is constrained by the execution delay of each service, job release times and precedence relations, as well as communication delays between agents. A Mixed Integer-Linear Programming (MILP) approach is presented in the context of a multi-agent problem-solving framework that enables optimal makespans to be computed for complex classifications of scheduling problems that take many different parameters into account. While the algorithm presented takes exponential time to solve and thus is only feasible to use for limited numbers of agents and jobs, it provides a flexible alternative to existing heuristics that model only limited sets of parameters, or settle for approximations of the optimal solution. Complexity results of the algorithm are studied for various scenarios and inputs, as well as recursive applications of the algorithm for hierarchical decompositions of large problems, and optimization of multiple objective functions using Multiple Objective Linear Programming (MOLP) techniques.},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/tompkins_optimization_techniques_for_task_allocation_and_scheduling_in_distributed.pdf},
  language = {en}
}

@article{topaloglu2006,
  title = {A Multi-Objective Programming Model for Scheduling Emergency Medicine Residents},
  author = {Topaloglu, Seyda},
  year = {2006},
  month = nov,
  volume = {51},
  pages = {375--388},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2006.08.003},
  abstract = {Scheduling emergency medicine residents (EMRs) is a complex task, which considers a large number of rules (often conflicting) related to various aspec\ldots},
  file = {/Users/rwb/Zotero/storage/IFDC2GKS/S0360835206001021.html},
  journal = {Computers \& Industrial Engineering},
  language = {en},
  number = {3}
}

@article{topaloglu2010,
  title = {Nurse Scheduling Using Fuzzy Modeling Approach},
  author = {Topaloglu, Seyda and Selim, Hasan},
  year = {2010},
  month = jun,
  volume = {161},
  pages = {1543--1563},
  issn = {0165-0114},
  doi = {10.1016/j.fss.2009.10.003},
  abstract = {Nurse scheduling is a complex scheduling problem and involves generating a schedule for each nurse that consists of shift duties and days off within a\ldots},
  file = {/Users/rwb/Zotero/storage/DJJLAF94/S0165011409004151.html},
  journal = {Fuzzy Sets and Systems},
  language = {en},
  number = {11}
}

@inproceedings{topcuoglu1999,
  ids = {topcuoglu1999a},
  title = {Task Scheduling Algorithms for Heterogeneous Processors},
  booktitle = {Heterogeneous {{Computing Workshop}}, 1999. ({{HCW}} '99) {{Proceedings}}. {{Eighth}}},
  author = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
  year = {1999},
  pages = {3--14},
  doi = {10.1109/HCW.1999.765092},
  abstract = {Scheduling computation tasks on processors is the key issue for high-performance computing. Although a large number of scheduling heuristics have been presented in the literature, most of them target only homogeneous resources. The existing algorithms for heterogeneous domains are not generally efficient because of their high complexity and/or the quality of the results. We present two low-complexity efficient heuristics, the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm for scheduling directed acyclic weighted task graphs (DAGs) on a bounded number of heterogeneous processors. We compared the performances of these algorithms against three previously proposed heuristics. The comparison study showed that our algorithms outperform previous approaches in terms of performance (schedule length ratio and speedup) and cost (time-complexity)},
  file = {/Users/rwb/Dropbox/PhD/zotero/1999/topcuoglu_et_al_1999_task_scheduling_algorithms_for_heterogeneous_processors.pdf;C\:\\Users\\gerald\\Dropbox\\library\\undefined\\1999\\topcuoglu_et_al_1999_task_scheduling_algorithms_for_heterogeneous_processors.pdf;Dropbox/library/1999/Topcuoglu et al/topcuoglu_et_al_1999_task_scheduling_algorithms_for_heterogeneous_processors.pdf;/Users/rwb/Zotero/storage/HFI77I4S/765092.html;/Users/rwb/Zotero/storage/KA6U9BU8/765092.html},
  keywords = {algorithm performance,computational complexity,Computational efficiency,cost,Costs,Critical-Path-on-a-Processor algorithm,directed acyclic weighted task graph scheduling,directed graphs,distributed algorithms,Heterogeneous Earliest-Finish-Time algorithm,heterogeneous processors,heuristic programming,high-performance computing,low-complexity efficient heuristics,processor scheduling,Processor scheduling,schedule length ratio,Scheduling algorithm,software performance evaluation,speedup,task scheduling algorithms,time complexity,Unread}
}

@article{topcuoglu2002,
  ids = {topcuoglu2002},
  title = {Performance-Effective and Low-Complexity Task Scheduling for Heterogeneous Computing},
  author = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
  year = {2002},
  month = mar,
  volume = {13},
  pages = {260--274},
  issn = {1045-9219},
  doi = {10.1109/71.993206},
  abstract = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2002/topcuoglu_et_al_2002_performance-effective_and_low-complexity_task_scheduling_for_heterogeneous.pdf;/Users/rwb/Zotero/storage/AFWJYXGV/993206.html;/Users/rwb/Zotero/storage/MSZ79E2H/993206.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {application scheduling problem,Critical-Path-on-a-Processor algorithm,DAG scheduling,directed graphs,heterogeneous computing environments,Heterogeneous Earliest-Finish-Time algorithm,list scheduling,parametric graph generator,processor scheduling,Processor scheduling,scheduling costs,task graphs,time metrics,weighted directed acyclic graphs,workstation clusters},
  number = {3}
}

@article{topcuoglu2002a,
  title = {Performance-Effective and Low-Complexity Task Scheduling for Heterogeneous Computing},
  author = {Topcuoglu, H. and Hariri, S. and {Min-You Wu}},
  year = {2002},
  month = mar,
  volume = {13},
  pages = {260--274},
  issn = {2161-9883},
  doi = {10.1109/71.993206},
  abstract = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2002/topcuoglu_et_al_2002_performance-effective_and_low-complexity_task_scheduling_for_heterogeneous2.pdf;/Users/rwb/Zotero/storage/KNX2BV27/993206.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {application scheduling problem,Critical-Path-on-a-Processor algorithm,DAG scheduling,directed graphs,heterogeneous computing environments,Heterogeneous Earliest-Finish-Time algorithm,list scheduling,parametric graph generator,processor scheduling,Processor scheduling,scheduling costs,task graphs,time metrics,weighted directed acyclic graphs,workstation clusters},
  number = {3}
}

@article{tovar2018,
  title = {A {{Job Sizing Strategy}} for {{High}}-{{Throughput Scientific Workflows}}},
  author = {Tovar, Benjamin and {da Silva}, Rafael Ferreira and Juve, Gideon and Deelman, Ewa and Allcock, William and Thain, Douglas and Livny, Miron},
  year = {2018},
  month = feb,
  volume = {29},
  pages = {240--253},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2017.2762310},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  number = {2}
}

@article{tran2018,
  title = {Multi-Stage Resource-Aware Scheduling for Data Centers with Heterogeneous Servers},
  author = {Tran, Tony T. and Padmanabhan, Meghana and Zhang, Peter Yun and Li, Heyse and Down, Douglas G. and Beck, J. Christopher},
  year = {2018},
  month = apr,
  volume = {21},
  pages = {251--267},
  issn = {1099-1425},
  doi = {10.1007/s10951-017-0537-x},
  abstract = {This paper presents a three-stage algorithm for resource-aware scheduling of computational jobs in a large-scale heterogeneous data center. The algorithm aims to allocate job classes to machine configurations to attain an efficient mapping between job resource request profiles and machine resource capacity profiles. The first stage uses a queueing model that treats the system in an aggregated manner with pooled machines and jobs represented as a fluid flow. The latter two stages use combinatorial optimization techniques to solve a shorter-term, more accurate representation of the problem using the first-stage, long-term solution for heuristic guidance. In the second stage, jobs and machines are discretized. A linear programming model is used to obtain a solution to the discrete problem that maximizes the system capacity given a restriction on the job class and machine configuration pairings based on the solution of the first stage. The final stage is a scheduling policy that uses the solution from the second stage to guide the dispatching of arriving jobs to machines. We present experimental results of our algorithm on both Google workload trace data and generated data and show that it outperforms existing schedulers. These results illustrate the importance of considering heterogeneity of both job and machine configuration profiles in making effective scheduling decisions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/tran_et_al_2018_multi-stage_resource-aware_scheduling_for_data_centers_with_heterogeneous.pdf},
  journal = {Journal of Scheduling},
  keywords = {_tablet,Dynamic scheduling,Heterogeneous servers,Resource-aware scheduling},
  language = {en},
  number = {2}
}

@inproceedings{tremblay2018,
  title = {Training {{Deep Networks}} with {{Synthetic Data}}: {{Bridging}} the {{Reality Gap}} by {{Domain Randomization}}},
  shorttitle = {Training {{Deep Networks}} with {{Synthetic Data}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Tremblay, Jonathan and Prakash, Aayush and Acuna, David and Brophy, Mark and Jampani, Varun and Anil, Cem and To, Thang and Cameracci, Eric and Boochoon, Shaad and Birchfield, Stan},
  year = {2018},
  month = jun,
  pages = {1082--10828},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPRW.2018.00143},
  abstract = {We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator\textemdash such as lighting, pose, object textures, etc.\textemdash are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artisticallygenerated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of handannotated real-world data or to generate high-fidelity synthetic worlds\textemdash both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/Tremblay et al_2018_Training Deep Networks with Synthetic Data.pdf},
  isbn = {978-1-5386-6100-0},
  keywords = {_tablet},
  language = {en}
}

@article{trojet2011,
  title = {Project Scheduling under Resource Constraints: {{Application}} of the Cumulative Global Constraint in a Decision Support Framework},
  shorttitle = {Project Scheduling under Resource Constraints},
  author = {Trojet, Mariem and H'Mida, Fehmi and Lopez, Pierre},
  year = {2011},
  month = sep,
  volume = {61},
  pages = {357--363},
  issn = {03608352},
  doi = {10.1016/j.cie.2010.08.014},
  abstract = {This paper concerns project scheduling under resource constraints. Traditionally, the objective is to find a unique solution that minimizes the project makespan, while respecting the precedence constraints and the resource constraints. This work focuses on developing a model and a decision support framework for industrial application of the cumulative global constraint. For a given project scheduling, the proposed approach allows the generation of different optimal solutions relative to the alternate availability of outsourcing and resources. The objective is to provide a decision-maker an assistance to construct, choose, and define the appropriate scheduling program taking into account the possible capacity resources. The industrial problem under consideration is modeled as a Constraint Satisfaction Problem (CSP). It is implemented under the constraint programming language CHIP V5. The provided solutions determine values for the various variables associated to the tasks realized on each resource, as well as the curves with the profile of the total consumption of resources on time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2011/trojet_et_al_2011_project_scheduling_under_resource_constraints.pdf},
  journal = {Computers \& Industrial Engineering},
  language = {en},
  number = {2}
}

@article{truong2007,
  title = {Performance Metrics and Ontologies for {{Grid}} Workflows},
  author = {Truong, Hong-Linh and Dustdar, Schahram and Fahringer, Thomas},
  year = {2007},
  month = jul,
  volume = {23},
  pages = {760--772},
  issn = {0167-739X},
  doi = {10.1016/j.future.2007.01.003},
  abstract = {Many Grid workflow middleware services require knowledge about the performance behavior of Grid applications/services in order to effectively select, compose, and execute workflows in dynamic and complex Grid systems. To provide performance information for building such knowledge, Grid workflow performance tools have to select, measure, and analyze various performance metrics of workflows. However, there is a lack of a comprehensive study of performance metrics which can be used to evaluate the performance of a workflow executed in the Grid. Moreover, given the complexity of both Grid systems and workflows, semantics of essential performance-related concepts and relationships, and associated performance data in Grid workflows should be well described. In this paper, we analyze performance metrics that performance monitoring and analysis tools should provide during the evaluation of the performance of Grid workflows. Performance metrics are associated with multiple levels of abstraction. We introduce an ontology for describing performance data of Grid workflows and illustrate how the ontology can be utilized for monitoring and analyzing the performance of Grid workflows.},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\Future Generation Computer Systems\\2007\\truong_et_al_2007_performance_metrics_and_ontologies_for_grid_workflows.pdf;Dropbox/library/2007/Truong et al/truong_et_al_2007_performance_metrics_and_ontologies_for_grid_workflows.pdf;/Users/rwb/Zotero/storage/FDAMHMMT/S0167739X07000027.html},
  journal = {Future Generation Computer Systems},
  keywords = {Grid computing,Grid workflows,Performance metrics and ontology,Performance monitoring and analysis,Unread},
  number = {6}
}

@article{tsai2015,
  ids = {tsai2015},
  title = {Adaptive Dual-Criteria Task Group Allocation for Clustering-Based Multi-Workflow Scheduling on Parallel Computing Platform},
  author = {Tsai, Ying-Lin and Liu, Hsiao-Ching and Huang, Kuo-Chan},
  year = {2015},
  month = oct,
  volume = {71},
  pages = {3811--3831},
  issn = {1573-0484},
  doi = {10.1007/s11227-015-1469-x},
  abstract = {Workflow scheduling has long been an important research topic in the field of parallel computing. Clustering-based methods are one of the major types of workflow scheduling approaches which have been shown superior to other kinds of methods in many cases due to their advantage of minimizing inter-task communication costs. Most previous research dealt with single workflow scheduling and focused on how to cluster the tasks within a workflow into a set of task groups. Recent research showed that utilizing idle time gaps between scheduled tasks is a promising direction for efficient multiple workflow scheduling. Since executing multiple workflows simultaneously is an inevitable need in modern shared parallel computing platforms, efficient task group allocation becomes a critical issue. In this paper, we studied such issue and proposed an innovative dual-criteria task group allocation method which considers both task group's finish time and potential resource utilization to effectively improve overall multi-workflow execution performance. In addition, an adaptive task group rearrangement mechanism was adopted to further improve performance. The proposed method has been evaluated with a series of simulation experiments and compared to previous approaches. The experimental results show that our method outperforms previous approaches across different workload conditions and workflow properties in terms of average makespan. The performance improvement ranges from 5 to 29 \% for different conditions, achieving the largest performance improvement for workflows of smaller CCR.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/tsai_et_al_2015_adaptive_dual-criteria_task_group_allocation_for_clustering-based.pdf;/Users/rwb/Dropbox/PhD/zotero/2015/tsai_et_al_2015_adaptive_dual-criteria_task_group_allocation_for_clustering-based2.pdf},
  journal = {The Journal of Supercomputing},
  keywords = {Clustering-based scheduling,Parallel computing,Task allocation,Workflow scheduling},
  language = {en},
  number = {10}
}

@article{ullman1975,
  title = {{{NP}}-Complete {{Scheduling Problems}}},
  author = {Ullman, J. D.},
  year = {1975},
  month = jun,
  volume = {10},
  pages = {384--393},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(75)80008-0},
  journal = {J. Comput. Syst. Sci.},
  number = {3}
}

@inproceedings{uselton2010,
  title = {Parallel {{I}}/{{O}} Performance: {{From}} Events to Ensembles},
  shorttitle = {Parallel {{I}}/{{O}} Performance},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel Distributed Processing}} ({{IPDPS}})},
  author = {Uselton, A. and Howison, M. and Wright, N. J. and Skinner, D. and Keen, N. and Shalf, J. and Karavanic, K. L. and Oliker, L.},
  year = {2010},
  month = apr,
  pages = {1--11},
  doi = {10.1109/IPDPS.2010.5470424},
  abstract = {Parallel I/O is fast becoming a bottleneck to the research agendas of many users of extreme scale parallel computers. The principle cause of this is the concurrency explosion of high-end computation, coupled with the complexity of providing parallel file systems that perform reliably at such scales. More than just being a bottleneck, parallel I/O performance at scale is notoriously variable, being influenced by numerous factors inside and outside the application, thus making it extremely difficult to isolate cause and effect for performance events. In this paper, we propose a statistical approach to understanding I/O performance that moves from the analysis of performance events to the exploration of performance ensembles. Using this methodology, we examine two I/O-intensive scientific computations from cosmology and climate science, and demonstrate that our approach can identify application and middleware performance deficiencies - resulting in more than 4\texttimes{} run time improvement for both examined applications.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/uselton_et_al_2010_parallel_i-o_performance.pdf;/Users/rwb/Zotero/storage/UJVAR8D7/5470424.html},
  keywords = {Concurrent computing,Explosions,extreme scale parallel computers,File systems,High performance computing,high-end computation,Laboratories,Large-scale systems,Middleware,Monitoring,parallel file systems,parallel I/O performance,parallel processing,Performance analysis,performance evaluation,statistical approach,Supercomputers}
}

@article{vahi,
  title = {Workflows Using {{Pegasus}}: {{Enabling Dark Energy Survey Pipelines}}},
  author = {Vahi, Karan and Wang, Michael H and Chang, Chihway and Dodelson, Scott and Deelman, Ewa},
  pages = {4},
  abstract = {Workflows are a key technology for enabling complex scientific applications. They capture the inter-dependencies between processing steps in data analysis and simulation pipelines, as well as the mechanisms to execute those steps reliably and efficiently in a distributed computing environment. They also enable scientists to capture complex processes to promote method sharing and reuse and provide provenance information necessary for the verification of scientific results and scientific reproducibility. We describe a weak-lensing pipeline that is modelled as a Pegasus workflow with pipeline codes available as a Singularity container. This has enabled us to make this analysis widely available and easily replicable to the astronomy community. Using Pegasus, we have executed various steps of pipelines on different compute sites with varying infrastructures, with Pegasus seamless managing the data across the various compute clusters in a transparent manner.},
  file = {/Users/rwb/Zotero/storage/NLTF9Y2V/Vahi et al. - Workows using Pegasus Enabling Dark Energy Surve.pdf},
  language = {en}
}

@inproceedings{valouxis2013,
  title = {{{DAG Scheduling}} Using {{Integer Programming}} in Heterogenous Parallel Execution Environments},
  booktitle = {In Proceedings of the 6th {{Multidisciplinary International Conference}} on {{Scheduling}} : {{Theory}} and {{Applications}} ({{MISTA}} 2013), 27 - 30 {{Aug}} 2013, {{Ghent}}, {{Belgium}}},
  author = {Valouxis, C. and Gogos, C. and Alefragis, P. and Goulas, G. and Voros, N. and Housos, E.},
  editor = {Kendall, G. and McCollum, B. and Berghe, G. Venden},
  year = {2013},
  pages = {392--401},
  abstract = {A computer program can be represented by a Directed Acyclic Graph (DAG) in order to capture the dependencies between the individual tasks that should be executed each time the program runs. This paper proposes a mathematical model of Integer Programming that can be applied in order to schedule the tasks in the presence of multiple processors serving as the execution environment. The target is to minimize the overall execution time of the DAG known as schedule length or makespan. An approach called MATH using the full model is applied to small sized problems and then a more elaborate approach called MATHL is presented where the DAG is partitioned to levels. Levels are formed according to the hops needed for each node to be reached starting from the source node. Hence sub-problems have manageable size and can be solved in a timely manner. Consecutive optimal solutions for each level result in a high quality schedule for the overall problem even for cases consisting of several hundreds of nodes. Results show that this method constantly gives very good results and it is compared favorably with other approaches to the problem that can be found in the bibliography.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/valouxis_et_al_2013_dag_scheduling_using_integer_programming_in_heterogenous_parallel_execution.pdf}
}

@book{vanderbei2008,
  title = {Linear Programming: Foundations and Extensions},
  shorttitle = {Linear Programming},
  author = {Vanderbei, Robert J.},
  year = {2008},
  edition = {3rd ed},
  publisher = {{Springer}},
  address = {{Boston}},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\Springer\\2008\\vanderbei_2008_linear_programming.pdf},
  isbn = {978-0-387-74387-5},
  keywords = {Linear programming,Mathematical optimization},
  language = {en},
  lccn = {T57.74 .V36 2008},
  series = {International Series in Operations Research and Management Science}
}

@inproceedings{vanhautegem2018,
  title = {Analysis of {{VAS}}, {{WAS}} and {{XAS Scheduling Algorithms}} for {{Fiber}}-{{Loop Optical Buffers}}},
  booktitle = {Queueing {{Theory}} and {{Network Applications}}},
  author = {Van Hautegem, Kurt and Pinto, Mario and Bruneel, Herwig and Rogiest, Wouter},
  editor = {Takahashi, Yutaka and {Phung-Duc}, Tuan and Wittevrongel, Sabine and Yue, Wuyi},
  year = {2018},
  pages = {216--226},
  publisher = {{Springer International Publishing}},
  abstract = {In optical packet/burst switched networks fiber loops provide a viable and compact means of contention resolution. For fixed size packets it is known that a basic void-avoiding schedule (VAS) can vastly outperform a more classical pre-reservation algorithm as FCFS. In this contribution we propose two novel forward-looking algorithms, WAS and XAS, that outperform VAS in the setting of a uniform distributed packet size and a restricted buffer size. This paper presents results obtained by Monte Carlo simulation, showing that improvements of more than \textbackslash (20\textbackslash\%\textbackslash ) in packet loss in specific settings are obtainable. In other settings and for other performance measures similar improvements are within reach.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/van_hautegem_et_al_2018_analysis_of_vas,_was_and_xas_scheduling_algorithms_for_fiber-loop_optical.pdf},
  isbn = {978-3-319-93736-6},
  keywords = {_tablet,Contention resolution,Fiber loops,Optical buffering,Scheduling},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{vanspilbeeck2018,
  title = {On the {{Impact}} of {{Job Size Variability}} on {{Heterogeneity}}-{{Aware Load Balancing}}},
  booktitle = {Queueing {{Theory}} and {{Network Applications}}},
  author = {Van Spilbeeck, Ignace and Van Houdt, Benny},
  editor = {Takahashi, Yutaka and {Phung-Duc}, Tuan and Wittevrongel, Sabine and Yue, Wuyi},
  year = {2018},
  pages = {193--215},
  publisher = {{Springer International Publishing}},
  abstract = {Load balancing is one of the key components in many distributed systems as it heavily impacts performance and resource utilization. We consider a heterogeneous system where each server belongs to one of K classes and the speed of the server depends on its class. Arriving jobs are immediately dispatched to a server class in a randomized manner, i.e., with probability \textbackslash (p\_k\textbackslash ) a job is assigned to class k. Within each class a power of d choices rule is used to select the server that executes the job.For large systems and exponential job size durations the optimal probabilities \textbackslash (p\_k\textbackslash ) to minimize the mean response time can be determined easily via convex optimization. In this paper we develop a mean field model (validated by simulation) to investigate how these optimal probabilities \textbackslash (p\_k\textbackslash ) are affected by the higher moments and in particular by the variability of the job size distribution when the service discipline at each server is first-come-first-served. The main insight provided is that optimizing the probabilities \textbackslash (p\_k\textbackslash ) based on the higher moments is much more involved and provides only a non negligible gain for very specific system load regions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/van_spilbeeck_van_houdt_2018_on_the_impact_of_job_size_variability_on_heterogeneity-aware_load_balancing.pdf},
  isbn = {978-3-319-93736-6},
  keywords = {_tablet,Distributed computing,Performance analysis,Processor scheduling},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{vinothina2012,
  title = {A {{Survey}} on {{Resource Allocation Strategies}} in                      {{Cloud Computing}}},
  author = {Vinothina, V and Lecturer, Sr and Sridaran, Dr R},
  year = {2012},
  volume = {3},
  pages = {8},
  abstract = {Cloud computing has become a new age technology that has got huge potentials in enterprises and markets. Clouds can make it possible to access applications and associated data from anywhere. Companies are able to rent resources from cloud for storage and other computational purposes so that their infrastructure cost can be reduced significantly. Further they can make use of company-wide access to applications, based on payas-you-go model. Hence there is no need for getting licenses for individual products. However one of the major pitfalls in cloud computing is related to optimizing the resources being allocated. Because of the uniqueness of the model, resource allocation is performed with the objective of minimizing the costs associated with it. The other challenges of resource allocation are meeting customer demands and application requirements. In this paper, various resource allocation strategies and their challenges are discussed in detail. It is believed that this paper would benefit both cloud users and researchers in overcoming the challenges faced.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/vinothina_et_al_2012_a_survey_on_resource_allocation_strategies_in_cloud.pdf},
  journal = {International Journal of Advanced Computer Science and Applications},
  language = {en},
  number = {6}
}

@article{vinyals2017,
  title = {{{StarCraft II}}: {{A New Challenge}} for {{Reinforcement Learning}}},
  shorttitle = {{{StarCraft II}}},
  author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and {van Hasselt}, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
  year = {2017},
  month = aug,
  abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
  archivePrefix = {arXiv},
  eprint = {1708.04782},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/vinyals_et_al_2017_starcraft_ii.pdf;/Users/rwb/Zotero/storage/9L6WN6E5/1708.html},
  journal = {arXiv:1708.04782 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{wakefield2007,
  title = {An {{Analysis}} of {{Quality}} of {{Service Metrics}} and {{Frameworks}} in a {{Grid Computing Environment}}},
  author = {Wakefield, Russ and Collins, Ft},
  year = {2007},
  pages = {11},
  abstract = {One of the fastest emerging technologies within the high performance computing environment is gridbased technologies. Quality of service has increasingly become more important to applications running within the grid environment as the landscape changes from hand-carved applications to middleware-based system running many applications within the virtual organization. This paper offers an analysis of the metrics used by middleware packages by comparing them to standard or generic QoS metrics. The paper also examines the frameworks that implement QoS within these middleware packages and the features required within the framework to enforce QoS-based service. Conclusions compare the feature sets of both the metrics and the frameworks.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/wakefield_collins_2007_an_analysis_of_quality_of_service_metrics_and_frameworks_in_a_grid_computing.pdf},
  language = {en}
}

@inproceedings{wang2012,
  title = {Multi-Objective {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {{{JMLR}}: {{Workshop}} and {{Conference Proceedings}}},
  author = {Wang, Weijia and Sebag, Mich{\`e}le},
  year = {2012},
  pages = {17},
  abstract = {Concerned with multi-objective reinforcement learning (MORL), this paper presents MOMCTS, an extension of Monte-Carlo Tree Search to multi-objective sequential decision making. The known multi-objective indicator referred to as hyper-volume indicator is used to define an action selection criterion, replacing the UCB criterion in order to deal with multi-dimensional rewards. MO-MCTS is firstly compared with an existing MORL algorithm on the artificial Deep Sea Treasure problem. Then a scalability study of MOMCTS is made on the NP-hard problem of grid scheduling, showing that the performance of MO-MCTS matches the non RL-based state of the art albeit with a higher computational cost.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2012/wang_sebag_2012_multi-objective_monte-carlo_tree_search.pdf},
  language = {en}
}

@inproceedings{wang2014,
  title = {Multiple {{DAGs Dynamic Workflow Scheduling Based}} on the {{Primary Backup Algorithm}} in {{Cloud Computing System}}},
  booktitle = {2014 {{Ninth International Conference}} on {{Broadband}} and {{Wireless Computing}}, {{Communication}} and {{Applications}}},
  author = {Wang, Y. and Jia, C. and Xu, Y.},
  year = {2014},
  month = nov,
  pages = {177--182},
  doi = {10.1109/BWCCA.2014.62},
  abstract = {For reliable scheduling problem about multiple DAGs scientific workflow in a cloud computing environment, we propose a competitive, dynamic and multiple DAG scheduling algorithm which takes link communication processor into consideration (CCRH). Firstly, algorithm uses communication competition model to describe the communication between the processors. It calculates the earliest completion time of the primary backup task. It defines the scheduling processor unit and uses dynamic hierarchical approach for multiple DAGs workflow tasks. It is In order to calculate the unfair degree of each DAG factor. When multiple tasks in DAG have large difference weights, the algorithm is an effective solution for previous DAGs would increase the execution time span which caused by the remaining tasks scheduling. Simulation results show that under the premise of ensuring reliable scheduling, the algorithm not only can improve the fairness of the multiple DAGs scheduling, but also effectively short the average multiple DAGs scheduling time, and make the robustness of algorithm is more better.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/wang_et_al_2014_multiple_dags_dynamic_workflow_scheduling_based_on_the_primary_backup_algorithm.pdf;/Users/rwb/Zotero/storage/R2MVI6B3/7016065.html},
  keywords = {CCRH,cloud computing,Cloud computing,cloud computing system,DAG factor,DAGs scheduling time,dynamic hierarchical approach,Dynamic scheduling,Heuristic algorithms,link communication processor,multiple DAG dynamic workflow scheduling,multiple DAGs,primary backup algorithm,primary backup task,processor scheduling,reliability,Reliability,reliable scheduling,Scheduling algorithms,scheduling processor unit}
}

@inproceedings{wang2015,
  title = {Maximize {{Throughput Scheduling}} and {{Cost}}-{{Fairness Optimization}} for {{Multiple DAGs}} with {{Deadline Constraint}}},
  booktitle = {Algorithms and {{Architectures}} for {{Parallel Processing}}},
  author = {Wang, Wei and Wu, Qingbo and Tan, Yusong and Wu, Fuhui},
  editor = {Wang, Guojun and Zomaya, Albert and Martinez, Gregorio and Li, Kenli},
  year = {2015},
  pages = {621--634},
  publisher = {{Springer International Publishing}},
  abstract = {More and more application workflows are computed in cloud and most of them can be expressed by Directed Acyclic Graph (DAG). As Cloud resource providers, they should guarantee as many as possible DAGs be accomplished within their deadline when they face the overstep request of computer resource. In this paper, we define the urgency of DAG and introduce the MTMD (Maximize Throughput of Multi-DAG with Deadline) algorithm to improve the ratio of DAGs which can be accomplished within deadline. The urgency of DAG is changing among execution and determine the execution order of tasks. We can detect DAGs which will exceed the deadline by this algorithm and abandon these DAGs timely. Based on the MTMD algorithm, we put forward the CFS (Cost Fairness Scheduling) algorithm to reduce the unfairness of cost between different DAGs. The simulation results show that the MTMD algorithm outperforms three other algorithms and the CFS algorithm reduces the cost of all DAGs by 12.1 \% on average and reduces the unfairness among DAGs by 54.5 \% on average.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/wang_et_al_2015_maximize_throughput_scheduling_and_cost-fairness_optimization_for_multiple_dags.pdf},
  isbn = {978-3-319-27122-4},
  keywords = {Deadline constrained scheduling,Multiple DAGs,Workflow scheduling},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wang2017,
  title = {Fairness Scheduling with Dynamic Priority for Multi Workflow on Heterogeneous Systems},
  booktitle = {2017 {{IEEE}} 2nd {{International Conference}} on {{Cloud Computing}} and {{Big Data Analysis}} ({{ICCCBDA}})},
  author = {Wang, Y. and Cao, Shijie and Wang, G. and Feng, Zhen and Zhang, Chi and Guo, He},
  year = {2017},
  month = apr,
  pages = {404--409},
  doi = {10.1109/ICCCBDA.2017.7951947},
  abstract = {In Heterogeneous Computing Systems, completion time and overall fairness are both crucial to multi workflow scheduling. Unfortunately, most dynamic workflow scheduling algorithms fail to take fairness into account or adapt unreasonable fairness policy, which will probably result in some DAGs' scheduling failure in certain scenarios when they can't complete all the tasks before deadline. Aiming at the need to solve the flaws mentioned above, we propose a novel dynamic workflow scheduling algorithm named FSDP (Fairness Scheduling with Dynamic Priority for Multi Workflow). The algorithm focuses on the deadline of each workflow to achieve a more reasonable fairness when allocating the processors so that those urgent workflows can acquire a higher priority. Also, a new metric, urgency, is proposed to update priority. Experimental results show that it is possible to meet different requirements of dynamic workflows.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/wang_et_al_2017_fairness_scheduling_with_dynamic_priority_for_multi_workflow_on_heterogeneous.pdf;/Users/rwb/Zotero/storage/AG3DKCLJ/7951947.html},
  keywords = {completion time,Computers,DAG,DAG scheduling failure,directed acyclic graph,directed graphs,dynamic priority,dynamic workflow,dynamic workflow scheduling algorithm,Electronic mail,fairness,fairness scheduling,heterogeneous computing,heterogeneous computing systems,multiworkflow scheduling,overall fairness,processor allocation,processor scheduling,Schedules}
}

@inproceedings{wang2017a,
  title = {Using {{Integer Programming}} for {{Workflow Scheduling}} in the {{Cloud}}},
  booktitle = {2017 {{IEEE}} 10th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Wang, Yi and Xia, Ye and Chen, Shigang},
  year = {2017},
  month = jun,
  publisher = {{IEEE}},
  address = {{Honolulu, CA, USA}},
  doi = {10.1109/cloud.2017.26},
  abstract = {We study a fundamental problem of how to schedule complex workflows in the cloud for applications such as data analytics. One of the main challenges is that such workflow scheduling problems involve many constraints, requirements and varied objectives and it is extremely difficult to find high-quality solutions. To meet the challenge, we explore using mixed integer programming (MIP) to formulate and solve complex workflow scheduling problems. To illustrate the MIPbased method, we formulate three related workflow scheduling problems in MIP. They are fairly generic, comprehensive and are expected to be useful for a wide range of workflow scheduling scenarios. Using results from numerical experiments, we demonstrate that, for problems up to certain size, the MIP approach is entirely applicable and more advantageous over heuristic algorithms.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/wang_et_al_2017_using_integer_programming_for_workflow_scheduling_in_the_cloud.pdf},
  isbn = {978-1-5386-1993-3},
  language = {en}
}

@article{wang2018,
  title = {List-{{Scheduling}} versus {{Cluster}}-{{Scheduling}}},
  author = {Wang, Huijun and Sinnen, Oliver},
  year = {2018},
  month = aug,
  volume = {29},
  pages = {1736--1749},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2018.2808959},
  abstract = {In scheduling theory and parallel computing practice, programs are often represented as directed acyclic graphs. Finding a makespan-minimising schedule for such a graph on a given number of homogenous processors (PIprec; cijICmax) is an NP-hard optimisation problem. Among the many proposed heuristics, the two dominant approaches are list-scheduling and cluster-scheduling (based on clustering), whereby clustering targets an unlimited number of processors at its core. Given their heuristic nature, many experimental comparisons exist. However, their overwhelming majority compares algorithms within but not across categories. Hence it is not clear how cluster-scheduling, for a limited number of processors, performs relative to list-scheduling or how list-scheduling, for an unlimited number of processors, performs against clustering. This study addresses these open questions by comparing a large set of representative algorithms from the two approaches in an extensive experimental evaluation. The algorithms are discussed and studied in a modular nature, categorizing algorithms into components. Some of the included algorithms are previously unpublished combinations of these techniques. This approach also permits to study the separate merit of techniques like task insertion or lookahead. The results show that simple low-complexity algorithms are surprisingly competitive and that more sophisticated algorithms only exhibit their strengths under certain conditions.},
  file = {/Users/rwb/Zotero/storage/R997VYHY/8301529.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {cluster-scheduling,clustering,Clustering algorithms,clustering targets,computational complexity,DAG,directed acyclic graphs,directed graphs,experiment,extensive experimental evaluation,Heuristic algorithms,homogenous processors,list-scheduling,low-complexity algorithms,makespan-minimising schedule,minimisation,NP-hard optimisation problem,parallel computing practice,parallel processing,pattern clustering,processor scheduling,Processor scheduling,Program processors,Schedules,scheduling,Scheduling,Task analysis,task insertion,Task scheduling with communication delay},
  number = {8}
}

@article{wang2018a,
  title = {List-{{Scheduling}} versus {{Cluster}}-{{Scheduling}}},
  author = {Wang, Huijun and Sinnen, Oliver},
  year = {2018},
  month = aug,
  volume = {29},
  pages = {1736--1749},
  issn = {2161-9883},
  doi = {10.1109/TPDS.2018.2808959},
  abstract = {In scheduling theory and parallel computing practice, programs are often represented as directed acyclic graphs. Finding a makespan-minimising schedule for such a graph on a given number of homogenous processors (PIprec; cijICmax) is an NP-hard optimisation problem. Among the many proposed heuristics, the two dominant approaches are list-scheduling and cluster-scheduling (based on clustering), whereby clustering targets an unlimited number of processors at its core. Given their heuristic nature, many experimental comparisons exist. However, their overwhelming majority compares algorithms within but not across categories. Hence it is not clear how cluster-scheduling, for a limited number of processors, performs relative to list-scheduling or how list-scheduling, for an unlimited number of processors, performs against clustering. This study addresses these open questions by comparing a large set of representative algorithms from the two approaches in an extensive experimental evaluation. The algorithms are discussed and studied in a modular nature, categorizing algorithms into components. Some of the included algorithms are previously unpublished combinations of these techniques. This approach also permits to study the separate merit of techniques like task insertion or lookahead. The results show that simple low-complexity algorithms are surprisingly competitive and that more sophisticated algorithms only exhibit their strengths under certain conditions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/wang_sinnen_2018_list-scheduling_versus_cluster-scheduling.pdf;/Users/rwb/Zotero/storage/NPWUL3V9/8301529.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {cluster-scheduling,clustering,Clustering algorithms,clustering targets,computational complexity,DAG,directed acyclic graphs,directed graphs,experiment,extensive experimental evaluation,Heuristic algorithms,homogenous processors,list-scheduling,low-complexity algorithms,makespan-minimising schedule,minimisation,NP-hard optimisation problem,parallel computing practice,parallel processing,pattern clustering,processor scheduling,Processor scheduling,Program processors,Schedules,scheduling,Scheduling,Task analysis,task insertion,Task scheduling with communication delay},
  number = {8}
}

@article{wang2018b,
  title = {Machine {{Learning}} for {{Networking}}: {{Workflow}}, {{Advances}} and {{Opportunities}}},
  shorttitle = {Machine {{Learning}} for {{Networking}}},
  author = {Wang, M. and Cui, Y. and Wang, X. and Xiao, S. and Jiang, J.},
  year = {2018},
  month = mar,
  volume = {32},
  pages = {92--99},
  issn = {0890-8044},
  doi = {10.1109/MNET.2017.1700200},
  abstract = {Recently, machine learning has been used in every possible field to leverage its amazing power. For a long time, the networking and distributed computing system is the key infrastructure to provide efficient computational resources for machine learning. Networking itself can also benefit from this promising technology. This article focuses on the application of MLN, which can not only help solve the intractable old network questions but also stimulate new network applications. In this article, we summarize the basic workflow to explain how to apply machine learning technology in the networking domain. Then we provide a selective survey of the latest representative advances with explanations of their design principles and benefits. These advances are divided into several network design objectives and the detailed information of how they perform in each step of MLN workflow is presented. Finally, we shed light on the new opportunities in networking design and community building of this new inter-discipline. Our goal is to provide a broad research guideline on networking with machine learning to help motivate researchers to develop innovative algorithms, standards and frameworks.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/wang_et_al_2018_machine_learning_for_networking.pdf;/Users/rwb/Zotero/storage/F6WLY5UQ/8121867.html},
  journal = {IEEE Network},
  keywords = {community building,computer network management,Data models,Decision making,distributed computing system,Feature extraction,Hidden Markov models,learning (artificial intelligence),Machine learning algorithms,machine learning for networking,MLN workflow,networking design,Predictive models,telecommunication computing,Training},
  number = {2}
}

@inproceedings{wang2018c,
  title = {A {{Multi}}-Stage {{Dynamic Game}}-{{Theoretic Approach}} for {{Multi}}-{{Workflow Scheduling}} on {{Heterogeneous Virtual Machines}} from {{Multiple Infrastructure}}-as-a-{{Service Clouds}}},
  booktitle = {Services {{Computing}} \textendash{} {{SCC}} 2018},
  author = {Wang, Yuandou and Jiang, Jiajia and Xia, Yunni and Wu, Quanwang and Luo, Xin and Zhu, Qingsheng},
  editor = {Ferreira, Jo{\~a}o Eduardo and Spanoudakis, George and Ma, Yutao and Zhang, Liang-Jie},
  year = {2018},
  pages = {137--152},
  publisher = {{Springer International Publishing}},
  abstract = {Distributed computing systems such as clouds continue to evolve to support various types of scientific applications, especially scientific workflows, with dependable, consistent, pervasive, and inexpensive access to geographically-distributed computational capabilities. Scheduling multiple workflows on distributed computing systems like Infrastructure-as-a-Service (IaaS) clouds is well recognized as a fundamental NP-complete problem that is critical to meeting various types of Quality-of-Service (QoS) requirements. In this paper, we propose a multi-objective optimization workflow scheduling approach based on dynamic game-theoretic model aiming at reducing workflow make-spans, reducing total cost, and maximizing system fairness in terms of workload distribution among heterogeneous cloud virtual machines (VMs). We conduct extensive case studies as well based on various well-known scientific workflow templates and real-world third-party commercial IaaS clouds. Experimental results clearly suggest that our proposed approach outperform traditional ones by achieving lower workflow make-spans, lower cost, and better system fairness.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/wang_et_al_2018_a_multi-stage_dynamic_game-theoretic_approach_for_multi-workflow_scheduling_on.pdf},
  isbn = {978-3-319-94376-3},
  keywords = {Dynamic game,Multi-clouds system,Multi-objective optimization,QoS,Workflow scheduling},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{wang2019,
  title = {Multi-{{Objective Workflow Scheduling With Deep}}-{{Q}}-{{Network}}-{{Based Multi}}-{{Agent Reinforcement Learning}}},
  author = {Wang, Yuandou and Liu, Hang and Zheng, Wanbo and Xia, Yunni and Li, Yawen and Chen, Peng and Guo, Kunyin and Xie, Hong},
  year = {2019},
  volume = {7},
  pages = {39974--39982},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2902846},
  abstract = {Cloud Computing provides an effective platform for executing large-scale and complex workflow applications with a pay-as-you-go model. Nevertheless, various challenges, especially its optimal scheduling for multiple conflicting objectives, are yet to be addressed properly. The existing multi-objective workflow scheduling approaches are still limited in many ways, e.g., encoding is restricted by prior experts' knowledge when handling a dynamic real-time problem, which strongly influences the performance of scheduling. In this paper, we apply a deep-Q-network model in a multi-agent reinforcement learning setting to guide the scheduling of multi-workflows over infrastructure-as-a-service clouds. To optimize multi-workflow completion time and user's cost, we consider a Markov game model, which takes the number of workflow applications and heterogeneous virtual machines as state input and the maximum completion time and cost as rewards. The game model is capable of seeking for correlated equilibrium between make-span and cost criteria without prior experts' knowledge and converges to the correlated equilibrium policy in a dynamic real-time environment. To validate our proposed approach, we conduct extensive case studies based on multiple well-known scientific workflow templates and Amazon EC2 cloud. The experimental results clearly suggest that our proposed approach outperforms traditional ones, e.g., non-dominated sorting genetic algorithm-II, multi-objective particle swarm optimization, and game-theoretic-based greedy algorithms, in terms of optimality of scheduling plans generated.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2019/wang_et_al_2019_multi-objective_workflow_scheduling_with_deep-q-network-based_multi-agent.pdf;/Users/rwb/Zotero/storage/TU5AWFBP/8676306.html},
  journal = {IEEE Access},
  keywords = {Amazon EC2 cloud,cloud computing,Cloud computing,Cloud Computing,complex workflow applications,correlated equilibrium policy,deep-Q-network (DQN),deep-Q-network model,deep-Q-network-based multiagent reinforcement,dynamic real-time problem,game theory,game-theoretic-based greedy algorithms,Games,genetic algorithms,greedy algorithms,infrastructure-as-a-service (IaaS) cloud,infrastructure-as-a-service clouds,learning (artificial intelligence),Markov game model,Markov processes,maximum completion time,multi-agent reinforcement learning (MARL),multi-agent systems,Multi-objective workflow scheduling,multiobjective particle swarm optimization,multiobjective workflow scheduling,multiple conflicting objectives,multiworkflow completion time,optimal scheduling,Optimal scheduling,Pareto optimisation,particle swarm optimisation,pay-as-you-go model,quality-of-service (QoS),real-time environment,Reinforcement learning,scheduling,Scheduling,scheduling approaches,scheduling plans,scientific workflow templates,Task analysis,virtual machines,workflow management software}
}

@article{wei2018,
  title = {{{DRL}}-{{Scheduling}}: {{An Intelligent QoS}}-{{Aware Job Scheduling Framework}} for {{Applications}} in {{Clouds}}},
  shorttitle = {{{DRL}}-{{Scheduling}}},
  author = {Wei, Y. and Pan, L. and Liu, S. and Wu, L. and Meng, X.},
  year = {2018},
  volume = {6},
  pages = {55112--55125},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2872674},
  abstract = {As an increasing number of traditional applications migrated to the cloud, achieving resource management and performance optimization in such a dynamic and uncertain environment becomes a big challenge for cloud-based application providers. In particular, job scheduling is a non-trivial task, which is responsible for allocating massive job requests submitted by users to the most suitable resources and satisfying user QoS requirements as much as possible. Inspired by recent success of using deep reinforcement learning techniques to solve AI control problems, in this paper, we propose an intelligent QoS-aware job scheduling framework for application providers. A deep reinforcement learning-based job scheduler is the key component of the framework. It is able to learn to make appropriate online job-to-VM decisions for continuous job requests directly from its experiences without any prior knowledge. Experimental results using synthetic workloads and real-world NASA workload traces show that compared with other baseline solutions, our proposed job scheduling approach can efficiently reduce average job response time (e.g., reduced by 40.4\% compared with the best baseline for NASA traces), guarantee the QoS at a high level (e.g., job success rate is higher than 93\% for all simulated changing workload scenarios), and adapt to different workload conditions.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/wei_et_al_2018_drl-scheduling.pdf;/Users/rwb/Zotero/storage/SIL4QYSF/8476582.html},
  journal = {IEEE Access},
  keywords = {average job response time,cloud computing,Cloud computing,cloud-based application providers,deep Q-Learning,deep reinforcement learning-based job scheduler,DRL-scheduling,intelligent QoS-aware job scheduling framework,job scheduling,Job shop scheduling,learning (artificial intelligence),Machine learning,online job-to-VM decisions,Processor scheduling,QoS,quality of service,Quality of service,reinforcement learning,resource allocation,resource management,scheduling,Task analysis,user QoS requirements,virtual machines}
}

@article{weil1995,
  title = {Constraint Programming for Nurse Scheduling},
  author = {Weil, G. and Heus, K. and Francois, P. and Poujade, M.},
  year = {1995},
  month = jul,
  volume = {14},
  pages = {417--422},
  issn = {0739-5175},
  doi = {10.1109/51.395324},
  abstract = {Nurse scheduling is a difficult, multifaceted problem. Here, the authors have presented the efficiency of Constraint Programming for solving this problem. The results obtained are very satisfactory for response time and for flexibility. The advantages of implementing this method are multiple: 1) it saves much time for the head nurse in the generation of schedules (the authors met head nurses for whom the task of scheduling takes a full working day); 2) the proposed system is not a rigid tool for schedule generation, but it is designed to help the decision maker in decisions and negotiations; 3) the proposed system is a flexible tool with respect to individual requests and for overcoming unforeseen absences; 4) it is very easy to manage constraints whether, for example, to define new constraints, activating or deactivating particular constraints, or modifying an already defined constraint. Ilog-Solver is a powerful tool for constraint programming. It provides the user with several types of variables, and the possibility of defining a specific constraint for the problem. The integration of object programming provided by Ilog-Solver allows better representation and saves much memory.{$<$}{$>$}},
  file = {/Users/rwb/Zotero/storage/LAACYA47/395324.html},
  journal = {IEEE Engineering in Medicine and Biology Magazine},
  keywords = {Artificial intelligence,Biomedical imaging,constraint programming,difficult multifaceted problem,Head,head nurse's time saving,Hospitals,human resource management,Humans,Ilog-Solver,individual requests,Interference constraints,Libraries,medical administrative data processing,Medical services,nurse scheduling,personnel,Personnel,programming,Resource management,response time,scheduling,unforeseen absences},
  number = {4}
}

@inproceedings{wenhao2010,
  title = {A Community Cloud Oriented Workflow System Framework and Its Scheduling Strategy},
  booktitle = {2010 {{IEEE}} 2nd {{Symposium}} on {{Web Society}}},
  author = {Wenhao, Li},
  year = {2010},
  month = aug,
  pages = {316--325},
  issn = {2158-6993},
  doi = {10.1109/SWS.2010.5607434},
  abstract = {The problem of solving fast collaboration among many related enterprises has been an important research issue in the Web Society research area. The recent proposed Cloud Computing application model called the Community Cloud Computing has provided a good solution for this problem. This paper proposed a novel workflow system framework for the Community Cloud which meets the need of the process-based fast collaboration well, and a community cloud oriented task scheduling strategy called the Unified Scheduling Strategy for Instance-intensive Workflows (USS-I) to ensure the fast collaboration mechanism work in high efficiency.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2010/wenhao_2010_a_community_cloud_oriented_workflow_system_framework_and_its_scheduling_strategy.pdf;/Users/rwb/Zotero/storage/WNNWQ4P3/5607434.html},
  keywords = {Algorithm design and analysis,Cloud computing application model,Clouds,Collaboration,Communities,community cloud oriented task scheduling strategy,community cloud oriented workflow system framework,instance-intensive workflows,Internet,process-based fast collaboration,Quality of service,scheduling,Scheduling,Scheduling algorithm,unified scheduling strategy,Web society research area,workflow management software}
}

@article{wieczorek2009,
  ids = {wieczorek2009a},
  title = {Towards a General Model of the Multi-Criteria Workflow Scheduling on the Grid},
  author = {Wieczorek, Marek and Hoheisel, Andreas and Prodan, Radu},
  year = {2009},
  month = mar,
  volume = {25},
  pages = {237--256},
  issn = {0167-739X},
  doi = {10.1016/j.future.2008.09.002},
  abstract = {Workflow scheduling on the Grid becomes more challenging when multiple scheduling criteria are considered. Existing studies provide different approaches to the multi-criteria Grid workflow scheduling problem, and address different variants of the problem. A profound understanding of the problem's nature can be an important step towards more generic scheduling approaches. Based on the related work and on our own experience, we propose several novel taxonomies of the problem, considering five facets: workflow model, scheduling criteria, scheduling process, resource model, and task model. We make a survey of the existing related work, and classify it according to the proposed taxonomies, identifying the most common use cases and the areas that have not been sufficiently explored yet.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2009/wieczorek_et_al_2009_towards_a_general_model_of_the_multi-criteria_workflow_scheduling_on_the_grid.pdf;/Users/rwb/Zotero/storage/ANG5TNKL/S0167739X08001398.html;/Users/rwb/Zotero/storage/XIHLNT9F/S0167739X08001398.html},
  journal = {Future Generation Computer Systems},
  keywords = {Grid computing,Multi-criteria scheduling,Taxonomy,Unread,Workflow},
  number = {3}
}

@article{wolstencroft2013,
  title = {The {{Taverna Workflow Suite}}: {{Designing}} and {{Executing Workflows}} of {{Web Services}} on the {{Desktop}}, {{Web}} or in the {{Cloud}}},
  shorttitle = {The {{Taverna Workflow Suite}}},
  author = {Wolstencroft, Katy and Haines, Robert and Fellows, Donal and Williams, Alan and Withers, David and Owen, Stuart and {Soiland-Reyes}, Stian and Dunlop, Ian and Nenadic, Aleksandra and Fisher, Paul and Bhagat, Jiten and Belhajjame, Khalid and Bacall, Finn and Hardisty, Alex and Hidalga, Abraham and {Balcazar-Vargas}, Maria and Sufi, Shoaib and Goble, Carole},
  year = {2013},
  month = may,
  volume = {41},
  doi = {10.1093/nar/gkt328},
  abstract = {The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can
be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments),
using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses
(for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through
Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range
of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be
shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and
the Taverna Server.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/wolstencroft_et_al_2013_the_taverna_workflow_suite.pdf},
  journal = {Nucleic acids research}
}

@inproceedings{wong2007,
  title = {Evaluating the {{EASY}}-Backfill Job Scheduling of Static Workloads on Clusters},
  booktitle = {2007 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Wong, A. K. L. and Goscinski, A. M.},
  year = {2007},
  month = sep,
  pages = {64--73},
  doi = {10.1109/CLUSTR.2007.4629218},
  abstract = {This research aims at improving our understanding of backfilling job scheduling algorithms. The most frequently used algorithm, EASY-backfilling, was selected for a performance evaluation by scheduling static workloads of parallel jobs on a computer cluster. To achieve the aim, we have developed a batch job scheduler for Linux clusters, implemented several scheduling algorithms including ARCA and EASY-Backfilling, and carried out their performance evaluation by running well known MPI applications on a real cluster. Our performance evaluation carried out for EASY-Backfilling serves two purposes. First, the performance results obtained from our evaluation can be used to validate other researcherspsila results generated by simulation, and second, the methodology used in our evaluation has alleviated many problems existed in the simulations presented in the current literature.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/wong_goscinski_2007_evaluating_the_easy-backfill_job_scheduling_of_static_workloads_on_clusters.pdf;/Users/rwb/Zotero/storage/IXZSJ8HA/4629218.html},
  keywords = {application program interfaces,ARCA,batch job scheduler,computer cluster,Computers,Delay,Dynamic scheduling,EASY-backfill job scheduling,EASY-Backfilling,Linux,Linux clusters,message passing,MPI,performance evaluation,Performance evaluation,Processor scheduling,scheduling,Scheduling algorithm,Time factors,workstation clusters}
}

@article{woo2014,
  title = {A {{Job Shop Scheduling Game}} with {{GA}}-Based {{Evaluation}}},
  author = {Woo, Kim Jun},
  year = {2014},
  month = sep,
  volume = {8},
  pages = {2627--2634},
  issn = {1935-0090, 2325-0399},
  doi = {10.12785/amis/080562},
  abstract = {The job shop scheduling problem is one of the well-known hardest combinatorial optimization problems, and solving the problem can be complex and time-consuming task. Hence, many undergraduates can not fully experience the scheduling procedures. This paper aims to introduce a computer game designed to enable the players to create and solve the job shop scheduling problems, and the players can learn the topics such as scheduling and optimization via a sense-making experiences provided by the game playing. The game program provides a simple graphic user interface similar with traditional board games played by manipulating blocks, and the players can conveniently search the good schedules in a trial-and-error manner. Moreover, the completed schedules are evaluated by taking the schedules generated by the genetic algorithm into account. For reasonable evaluation, the game program adopts two search strategies called forward and backward search to generate schedules with a wide range of makespans. The integrated job shop scheduling game introduced is well-organized to support overall procedures for job shop scheduling, and the genetic algorithm based evaluation can make the players to compete for better schedules. As a result, both purposes, entertainment and education, can be simultaneously served by the game playing.},
  file = {/Users/rwb/Zotero/storage/TP76IDPH/woo_2014_a_job_shop_scheduling_game_with_ga-based_evaluation.pdf},
  journal = {Applied Mathematics \& Information Sciences},
  keywords = {_tablet},
  language = {en},
  number = {5}
}

@inproceedings{wu2015,
  title = {Hierarchical {{DAG Scheduling}} for {{Hybrid Distributed Systems}}},
  booktitle = {2015 {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Wu, W. and Bouteiller, A. and Bosilca, G. and Faverge, M. and Dongarra, J.},
  year = {2015},
  month = may,
  pages = {156--165},
  doi = {10.1109/IPDPS.2015.56},
  abstract = {Accelerator-enhanced computing platforms have drawn a lot of attention due to their massive peak commutational capacity. Despite significant advances in the programming interfaces to such hybrid architectures, traditional programming paradigms struggle with mapping the resulting multi-dimensional heterogeneity and the expression of algorithm parallelism, resulting in sub-optimal effective performance. Task-based programming paradigms have the capability to alleviate some of the programming challenges on distributed hybrid many-core architectures. In this paper we take this concept a step further by showing that the potential of task-based programming paradigms can be greatly increased with minimal modification of the underlying runtime combined with the right algorithmic changes. We propose two novel recursive algorithmic variants for one-sided factorizations and describe the changes to the PaRSEC task-scheduling runtime to build a framework where the task granularity is dynamically adjusted to adapt the degree of available parallelism and kernel efficiency according to runtime conditions. Based on an extensive set of results, we show that, with one-sided factorizations, i.e. Colicky, and QR, a carefully written algorithm, supported by an adaptive tasks-based runtime, is capable of reaching a degree of performance and scalability never achieved before in distributed hybrid environments.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/wu_et_al_2015_hierarchical_dag_scheduling_for_hybrid_distributed_systems.pdf;/Users/rwb/Zotero/storage/6T3UBRRL/7161505.html},
  keywords = {accelerator-enhanced computing platforms,adaptive tasks-based runtime,algorithm parallelism,dense linear algebra,distributed hybrid environments,distributed hybrid many-core architectures,GPU,Graphics processing units,heterogeneous architecture,hierarchical DAG scheduling,hybrid architectures,hybrid distributed systems,Kernel,Linear algebra,massive peak commutational capacity,multidimensional heterogeneity,multiprocessing systems,one-sided factorizations,parallel algorithms,parallel programming,PaRSEC runtime,PaRSEC task-scheduling runtime,processor scheduling,Processor scheduling,Programming,programming interfaces,recursive algorithmic variants,Runtime,sub-optimal effective performance,task granularity,task-based programming paradigms,traditional programming paradigms}
}

@article{wu2015a,
  title = {Workflow Scheduling in Cloud: A Survey},
  shorttitle = {Workflow Scheduling in Cloud},
  author = {Wu, Fuhui and Wu, Qingbo and Tan, Yusong},
  year = {2015},
  month = sep,
  volume = {71},
  pages = {3373--3418},
  issn = {1573-0484},
  doi = {10.1007/s11227-015-1438-4},
  abstract = {To program in distributed computing environments such as grids and clouds, workflow is adopted as an attractive paradigm for its powerful ability in expressing a wide range of applications, including scientific computing, multi-tier Web, and big data processing applications. With the development of cloud technology and extensive deployment of cloud platform, the problem of workflow scheduling in cloud becomes an important research topic. The challenges of the problem lie in: NP-hard nature of task-resource mapping; diverse QoS requirements; on-demand resource provisioning; performance fluctuation and failure handling; hybrid resource scheduling; data storage and transmission optimization. Consequently, a number of studies, focusing on different aspects, emerged in the literature. In this paper, we firstly conduct taxonomy and comparative review on workflow scheduling algorithms. Then, we make a comprehensive survey of workflow scheduling in cloud environment in a problem\textendash solution manner. Based on the analysis, we also highlight some research directions for future investigation.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/wu_et_al_2015_workflow_scheduling_in_cloud.pdf},
  journal = {The Journal of Supercomputing},
  keywords = {Cloud computing,Data-intensive workflow scheduling,Hybrid environment,QoS constrained scheduling,Robust scheduling,Workflow scheduling,Workflow-as-a-service},
  language = {en},
  number = {9}
}

@inproceedings{wu2017,
  title = {Improving {{SAT}}-Solving with {{Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGCSE Technical Symposium}} on {{Computer Science Education}}  - {{SIGCSE}} '17},
  author = {Wu, Haoze},
  year = {2017},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington, USA}},
  doi = {10.1145/3017680.3022464},
  abstract = {In this project, we aimed to improve the runtime of Minisat, a Conflict-Driven Clause Learning (CDCL) solver that solves the Propositional Boolean Satisfiability (SAT) problem. We first used a logistic regression model to predict the satisfiability of propositional boolean formulae after fixing the values of a certain fraction of the variables in each formula. We then applied the logistic model and added a preprocessing period to Minisat to determine the preferable initial value (either true or false) of each boolean variable using a Monte-Carlo approach. Concretely, for each MonteCarlo trial, we fixed the values of a certain ratio of randomly selected variables, and calculated the confidence that the resulting sub-formula is satisfiable with our logistic regression model. The initial value of each variable was set based on the mean confidence scores of the trials that started from the literals of that variable. We were particularly interested in setting the initial values of the backbone variables correctly, which are variables that have the same value in all solutions of a SAT formula. Our Monte-Carlo method was able to set 78\% of the backbones correctly. Excluding the preprocessing time, compared with the default setting of Minisat, the runtime of Minisat for satisfiable formulae decreased by 23\%. However, our method did not outperform vanilla Minisat in runtime, as the decrease in the conflicts was outweighed by the long runtime of the preprocessing period.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/wu_2017_improving_sat-solving_with_machine_learning.pdf},
  isbn = {978-1-4503-4698-6},
  language = {en}
}

@article{wu2017a,
  title = {{{DALiuGE}}: {{A}} Graph Execution Framework for Harnessing the Astronomical Data Deluge},
  shorttitle = {{{DALiuGE}}},
  author = {Wu, C. and Tobar, R. and Vinsen, K. and Wicenec, A. and Pallot, D. and Lao, B. and Wang, R. and An, T. and Boulton, M. and Cooper, I. and Dodson, R. and Dolensky, M. and Mei, Y. and Wang, F.},
  year = {2017},
  month = jul,
  volume = {20},
  pages = {1--15},
  issn = {2213-1337},
  doi = {10.1016/j.ascom.2017.03.007},
  abstract = {The Data Activated Liu 1 Graph Engine \textendash{} DALiuGE2\textendash{} is an execution framework for processing large astronomical datasets at a scale required by the Square Kilometre Array Phase 1 (SKA1). It includes an interface for expressing complex data reduction pipelines consisting of both datasets and algorithmic components and an implementation run-time to execute such pipelines on distributed resources. By mapping the logical view of a pipeline to its physical realisation, DALiuGE separates the concerns of multiple stakeholders, allowing them to collectively optimise large-scale data processing solutions in a coherent manner. The execution in DALiuGE is data-activated, where each individual data item autonomously triggers the processing on itself. Such decentralisation also makes the execution framework very scalable and flexible, supporting pipeline sizes ranging from less than ten tasks running on a laptop to tens of millions of concurrent tasks on the second fastest supercomputer in the world. DALiuGE has been used in production for reducing interferometry datasets from the Karl E. Jansky Very Large Array and the Mingantu Ultrawide Spectral Radioheliograph; and is being developed as the execution framework prototype for the Science Data Processor (SDP) consortium of the Square Kilometre Array (SKA) telescope. This paper presents a technical overview of DALiuGE and discusses case studies from the CHILES and MUSER projects that use DALiuGE to execute production pipelines. In a companion paper, we provide in-depth analysis of DALiuGE's scalability to very large numbers of tasks on two supercomputing facilities.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/wu_et_al_2017_daliuge.pdf;/Users/rwb/Zotero/storage/KHLYP7KJ/S2213133716301214.html},
  journal = {Astronomy and Computing},
  keywords = {Data driven,Dataflow,Graph execution engine,Many-task computing,Square kilometre array},
  language = {en}
}

@inproceedings{wu2018,
  title = {Performance {{Evaluation}} for a {{Registration Service}} with an {{Energy Efficient Cloud Architecture}}},
  booktitle = {Queueing {{Theory}} and {{Network Applications}}},
  author = {Wu, Haixing and Jin, Shunfu and Yue, Wuyi and Takahashi, Yutaka},
  editor = {Takahashi, Yutaka and {Phung-Duc}, Tuan and Wittevrongel, Sabine and Yue, Wuyi},
  year = {2018},
  pages = {133--141},
  publisher = {{Springer International Publishing}},
  abstract = {Cloud computing allows application providers to seamlessly scale services and enables users to adaptively scale usage. Cloud vendors always provide a free service to appeal to more anonymous users. In this paper, we propose a sleep-mode based cloud architecture, in which a free service and an optional registration service are provided on the same server. Regarding the free service as the first service, the registration service as the second optional service and the sleep state as the vacation, we establish an asynchronous multiple-vacation queueing model with a second optional service. We construct a three-dimensional Markov chain to derive the steady-state distribution of the queueing model, and estimate the average response time of anonymous users and the energy saving rate of system. Finally, we provide numerical results to investigate the trade-off between difference performance measures.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/wu_et_al_2018_performance_evaluation_for_a_registration_service_with_an_energy_efficient.pdf},
  isbn = {978-3-319-93736-6},
  keywords = {_tablet,Average response time,Cloud computing,Energy saving rate,Registration service,Second optional service queue},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{xu2014,
  title = {A Genetic Algorithm for Task Scheduling on Heterogeneous Computing Systems Using Multiple Priority Queues},
  author = {Xu, Yuming and Li, Kenli and Hu, Jingtong and Li, Keqin},
  year = {2014},
  month = jun,
  volume = {270},
  pages = {255--287},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2014.02.122},
  abstract = {On parallel and distributed heterogeneous computing systems, a heuristic-based task scheduling algorithm typically consists of two phases: task prioritization and processor selection. In a heuristic based task scheduling algorithm, different prioritization will produce different makespan on a heterogeneous computing system. Therefore, a good scheduling algorithm should be able to efficiently assign a priority to each subtask depending on the resources needed to minimize makespan. In this paper, a task scheduling scheme on heterogeneous computing systems using a multiple priority queues genetic algorithm (MPQGA) is proposed. The basic idea of our approach is to exploit the advantages of both evolutionary-based and heuristic-based algorithms while avoiding their drawbacks. The proposedalgorithm incorporates a genetic algorithm (GA) approach to assign a priority to each subtask while using a heuristic-based earliest finish time (EFT) approach to search for a solution for the task-to-processor mapping. The MPQGA method also designs crossover, mutation, and fitness function suitable for the scenario of directed acyclic graph (DAG) scheduling. The experimental results for large-sized problems from a large set of randomly generated graphs as well as graphs of real-world problems with various characteristics show that the proposed MPQGA algorithm outperforms two non-evolutionary heuristics and a random search method in terms of schedule quality.},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\Information Sciences\\2014\\xu_et_al_2014_a_genetic_algorithm_for_task_scheduling_on_heterogeneous_computing_systems.pdf;/Users/rwb/Zotero/storage/IRMHVVZD/S002002551400228X.html},
  journal = {Information Sciences},
  keywords = {Directed acyclic graph,Genetic algorithm,Heuristic algorithm,Makespan,Multiple priority queue,Task scheduling,Unread}
}

@article{xu2015,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  year = {2015},
  month = jun,
  volume = {2},
  pages = {165--193},
  issn = {2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2015/xu_tian_2015_a_comprehensive_survey_of_clustering_algorithms.pdf},
  journal = {Annals of Data Science},
  keywords = {_tablet,Clustering,Clustering algorithm,Clustering analysis,Survey,Unsupervised learning},
  language = {en},
  number = {2}
}

@article{xu2016,
  title = {Adaptive {{Task Scheduling Strategy Based}} on {{Dynamic Workload Adjustment}} for {{Heterogeneous Hadoop Clusters}}},
  author = {Xu, X. and Cao, L. and Wang, X.},
  year = {2016},
  month = jun,
  volume = {10},
  pages = {471--482},
  issn = {1932-8184},
  doi = {10.1109/JSYST.2014.2323112},
  abstract = {The original task scheduling algorithm of Hadoop cannot meet the performance requirements of heterogeneous clusters. According to the dynamic change of load of each task node and the difference of node performance of different tasks in the heterogeneous Hadoop cluster, a novel adaptive task scheduling strategy based on dynamic workload adjustment (ATSDWA) is presented. With ATSDWA, tasktrackers can adapt to the change of load at runtime, obtain tasks in accordance with the computing ability of their own, and realize the self-regulation, while avoiding the complexity of algorithm, which is the prime reason to make jobtracker the system performance bottleneck. Experimental results show that ATSDWA is a highly efficient and reliable algorithm, which can make heterogeneous Hadoop clusters stable, scalable, efficient, and load balancing. Furthermore, its performance is superior to the original and improved task scheduling strategy of Hadoop, from the aspects of the execution time of tasks, the resource utilization, and the speed-up ratio.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/xu_et_al_2016_adaptive_task_scheduling_strategy_based_on_dynamic_workload_adjustment_for.pdf;/Users/rwb/Zotero/storage/7JW2AL9K/6832443.html},
  journal = {IEEE Systems Journal},
  keywords = {_tablet,Adaptive scheduling,ATSDWA,cloud computing,Cloud computing,clustering methods,computational efficiency,data handling,distributed computing,dynamic scheduling,Dynamic scheduling,Heart beat,heterogeneous Hadoop clusters,load balancing,novel adaptive task scheduling strategy based on dynamic workload adjustment,parallel processing,Real-time systems,scheduling,Scheduling algorithms},
  number = {2}
}

@article{xu2017,
  title = {A Look-Ahead Algorithm for Online Multiple Workflow Scheduling Problem in Heterogeneous Systems},
  author = {Xu, Zhenzhen and Chen, Xin and Xu, Xiujuan and Zhao, Xiaowei and Dai, Jie and Jia, Mingfei},
  year = {2017},
  month = dec,
  volume = {25},
  pages = {331--342},
  issn = {1063-293X},
  doi = {10.1177/1063293X17728763},
  abstract = {A look-ahead algorithm is proposed to solve the online multiple workflow scheduling problem with two constraints in heterogeneous system. In this problem, workflows come to the system online when they are released, and each workflow is composed of multiple tasks which can be executed on heterogeneous processors, according to their types. Considered two constrains including the non-preemptive processor and the task order in a workflow, the proposed algorithm utilizes the information contained in the list of the submitted workflows and optimizes the scheduling of current task according to subsequent tasks. It can keep the subsequent tasks from waiting for a long time due to the occupation of the limited resource by the current task. The simulation results show that the proposed look-ahead algorithm outperforms four classical online scheduling algorithms, and the algorithm can get better performance when the look-ahead value L\,=\,1 than L\,{$\geq$}\,2.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/xu_et_al_2017_a_look-ahead_algorithm_for_online_multiple_workflow_scheduling_problem_in.pdf},
  journal = {Concurrent Engineering},
  language = {en},
  number = {4}
}

@inproceedings{yaghoobi2013,
  title = {A Non-Cooperative Game Theory Approach to Optimize Workflow Scheduling in Grid Computing},
  booktitle = {2013 {{IEEE Pacific Rim Conference}} on {{Communications}}, {{Computers}} and {{Signal Processing}} ({{PACRIM}})},
  author = {Yaghoobi, M. and Fanian, A. and Khajemohammadi, H. and Gulliver, T. A.},
  year = {2013},
  month = aug,
  pages = {108--113},
  doi = {10.1109/PACRIM.2013.6625458},
  abstract = {Grid computing employs resource sharing in heterogeneous computing networks to solve complex computing tasks. To provide suitable performance and response times, the available resources have to be scheduled and coordinated for workflow implementation in the grid environment. Therefore, task scheduling and resource allocation are very important to achieve high performance in grid computing. Game theory is one approach which can be used for scheduling. In this paper, a noncooperative game is proposed to minimize the time and cost of scheduling. Moreover, the aim of the proposed approach is to encourage resource brokers to use an optimal scheduling algorithm. In the proposed game, the broker profit is increased when a solution is proposed with lower time and cost for the users.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/yaghoobi_et_al_2013_a_non-cooperative_game_theory_approach_to_optimize_workflow_scheduling_in_grid.pdf;/Users/rwb/Zotero/storage/HXQ5JAVA/6625458.html},
  keywords = {broker profit,Computational modeling,game theory,Game theory,Games,Genetic algorithms,grid computing,Grid computing,heterogeneous computing networks,noncooperative game theory approach,optimal scheduling algorithm,Processor scheduling,resource allocation,response times,Scheduling,task scheduling,workflow scheduling,workflow scheduling optimization}
}

@article{yang2016,
  title = {Acquisition Planning and Scheduling of Computing Resources},
  author = {Yang, Chien-Nan and Lin, Bertrand M. T. and Hwang, F. J. and Wang, Meng-Chun},
  year = {2016},
  month = dec,
  volume = {76},
  pages = {167--182},
  issn = {0305-0548},
  doi = {10.1016/j.cor.2016.06.015},
  abstract = {Cloud computing has been attracting considerable attention since the last decade. This study considers a decision problem formulated from the use of computing services over the Internet. An agent receives orders of computing tasks from his/her clients and on the other hand he/she acquires computing resources from computing service providers to fulfill the requirements of the clients. The processors are bundled as packages according to their speeds and the business strategies of the providers. The packages are rated at a certain pricing scheme to provide flexible purchasing options to the agent. The decision of the agent is to select the packages which can be acquired from the service providers and then schedule the tasks of the clients onto the processors of the acquired packages such that the total cost, including acquisition cost and scheduling cost (total weighted tardiness), is minimized. In this study, we present an integer programming model to formulate the problem and propose several solution methods to produce acquisition and scheduling plans. Ten well-known heuristics of parallel-machine scheduling are adapted to fit into the studied problem so as to provide initial solutions. Tabu search and genetic algorithm are tailored to reflect the problem nature for improving upon the initial solutions. We conduct a series of computational experiments to evaluate the effectiveness and efficiency of all the proposed algorithms. The results of the numerical experiments reveal that the proposed tabu search and genetic algorithm can attain significant improvements.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/yang_et_al_2016_acquisition_planning_and_scheduling_of_computing_resources2.pdf;/Users/rwb/Zotero/storage/VVBQRJIJ/S0305054816301472.html},
  journal = {Computers \& Operations Research},
  keywords = {Acquisition planning,Computing service,Genetic algorithm,Heuristics,Scheduling,Tabu search}
}

@article{yang2017,
  title = {A Task Scheduling Algorithm Considering Game Theory Designed for Energy Management in Cloud Computing},
  author = {Yang, Jiachen and Jiang, Bin and Lv, Zhihan and Choo, Kim-Kwang Raymond},
  year = {2017},
  month = mar,
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.03.024},
  abstract = {With the increasing popularity of cloud computing products, task scheduling problem has become a hot research topic in this field. The task scheduling problem of cloud computing system is more complex than the traditional distributed system. Based on the analysis of cloud computing in related literature, we established a simplified model for task scheduling system in cloud computing.Different from the previous research of cloud computing task scheduling algorithm, the simplified model in this paper is based on game theory as a mathematical tool. Based on game theory, the task scheduling algorithm considering the reliability of the balanced task is proposed. Based on the balanced scheduling algorithm, the task scheduling model for computing nodes is proposed. In the cooperative game model, game strategy is used for the task in the calculation of rate allocation strategy on the node. Through analysis of experimental results, it is shown that the proposed algorithm has better optimization effect.},
  file = {/Users/rwb/Dropbox/PhD/zotero/017/yang_et_al_2017_a_task_scheduling_algorithm_considering_game_theory_designed_for_energy.pdf;/Users/rwb/Zotero/storage/WLRXICWJ/S0167739X17304673.html},
  journal = {Future Generation Computer Systems},
  keywords = {_tablet,Cloud computing,Game theory,Optimization,Task scheduling}
}

@article{yeo2006,
  title = {A Taxonomy of Market-Based Resource Management Systems for Utility-Driven Cluster Computing},
  author = {Yeo, Chee Shin and Buyya, Rajkumar},
  year = {2006},
  volume = {36},
  pages = {1381--1419},
  issn = {1097-024X},
  doi = {10.1002/spe.725},
  abstract = {In utility-driven cluster computing, cluster Resource Management Systems (RMSs) need to know the specific needs of different users in order to allocate resources according to their needs. This in turn is vital to achieve service-oriented Grid computing that harnesses resources distributed worldwide based on users' objectives. Recently, numerous market-based RMSs have been proposed to make use of real-world market concepts and behavior to assign resources to users for various computing platforms. The aim of this paper is to develop a taxonomy that characterizes and classifies how market-based RMSs can support utility-driven cluster computing in practice. The taxonomy is then mapped to existing market-based RMSs designed for both cluster and other computing platforms to survey current research developments and identify outstanding issues. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\Software Practice and Experience\\undefined\\yeo_buyya_a_taxonomy_of_market-based_resource_management_systems_for_utility-driven.pdf;/Users/rwb/Zotero/storage/E7N3FSP4/spe.html},
  journal = {Software: Practice and Experience},
  keywords = {cluster computing,market-based resource management systems,taxonomy,Unread,utility computing},
  language = {en},
  number = {13}
}

@article{yin2018,
  title = {Data-{{Aware Approximate Workflow Scheduling}}},
  author = {Yin, Dengpan and Kosar, Tevfik},
  year = {2018},
  month = may,
  abstract = {Optimization of data placement in complex scientific workflows has become very crucial since the large amounts of data generated by these workflows significantly increases the turnaround time of the end-to-end application. It is almost impossible to make an optimal scheduling for the end-to-end workflow without considering the intermediate data movement. In order to reduce the complexity of the workflow-scheduling problem, most of the existing work constrains the problem space by some unrealistic assumptions, which result in non-optimal scheduling in practice. In this study, we propose a genetic data-aware algorithm for the end-to-end workflow scheduling problem. Distinct from the past research, we develop a novel data-aware evaluation function for each chromosome, a common augmenting crossover operator and a simple but effective mutation operator. Our experiments on different workflow structures show that the proposed GA based approach gives a scheduling close to the optimal one.},
  archivePrefix = {arXiv},
  eprint = {1805.10499},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/yin_kosar_2018_data-aware_approximate_workflow_scheduling.pdf;/Users/rwb/Zotero/storage/6FAC8B7S/1805.html},
  journal = {arXiv:1805.10499 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryClass = {cs}
}

@article{yu,
  title = {Workflow {{Schdeduling Algorithms}} for {{Grid Computing}}},
  author = {Yu, Jia and Buyya, Rajkumar},
  pages = {33},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/yu_buyya_workflow_schdeduling_algorithms_for_grid_computing.pdf},
  language = {en}
}

@inproceedings{yu2005,
  title = {Cost-Based Scheduling of Scientific Workflow Applications on Utility Grids},
  booktitle = {First {{International Conference}} on E-{{Science}} and {{Grid Computing}} (e-{{Science}}'05)},
  author = {Yu, Jia and Buyya, R. and Tham, Chen Khong},
  year = {2005},
  month = jul,
  pages = {8 pp.-147},
  doi = {10.1109/E-SCIENCE.2005.26},
  abstract = {Over the last few years, grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models. Users consume these services based on their QoS (quality of service) requirements. In such "pay-per-use" grids, workflow execution cost must be considered during scheduling based on users' QoS constraints. In this paper, we propose a cost-based workflow scheduling algorithm that minimizes execution cost while meeting the deadline for delivering results. It can also adapt to the delays of service executions by rescheduling unexecuted tasks. We also attempt to optimally solve the task scheduling problem in branches with several sequential tasks by modeling the branch as a Markov decision process and using the value iteration method},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/yu_et_al_2005_cost-based_scheduling_of_scientific_workflow_applications_on_utility_grids.pdf;/Users/rwb/Zotero/storage/A3IR7I7A/1572219.html},
  keywords = {Computer networks,cost-based scheduling,Costs,decision theory,Delay,grid computing,Grid computing,grid technologies,Markov decision process,Markov processes,natural sciences computing,Optimal scheduling,pay-per-use grids,Processor scheduling,quality of service,Quality of service,scheduling,Scheduling algorithm,scientific workflow applications,service execution delays,service provisioning,task scheduling,Time factors,utility grids,value iteration method,Workflow management software}
}

@article{yu2005a,
  title = {A {{Taxonomy}} of {{Scientific Workflow Systems}} for {{Grid Computing}}},
  author = {Yu, Jia and Buyya, Rajkumar},
  year = {2005},
  month = sep,
  volume = {34},
  pages = {44--49},
  issn = {0163-5808},
  doi = {10.1145/1084805.1084814},
  abstract = {With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.},
  file = {C\:\\Users\\gerald\\Dropbox\\library\\SIGMOD Rec.\\2005\\yu_buyya_2005_a_taxonomy_of_scientific_workflow_systems_for_grid_computing.pdf;Dropbox/library/2005/Yu_Buyya/yu_buyya_2005_a_taxonomy_of_scientific_workflow_systems_for_grid_computing.pdf},
  journal = {SIGMOD Rec.},
  keywords = {grid computing,scientific workflows,taxonomy,Unread},
  number = {3}
}

@article{yu2005b,
  title = {A {{Taxonomy}} of {{Workflow Management Systems}} for {{Grid Computing}}},
  author = {Yu, Jia and Buyya, Rajkumar},
  year = {2005},
  month = sep,
  volume = {3},
  pages = {171--200},
  issn = {1572-9184},
  doi = {10.1007/s10723-005-9010-8},
  abstract = {With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. We also survey several representative Grid workflow systems developed by various projects world-wide to demonstrate the comprehensiveness of the taxonomy. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/yu_buyya_2005_a_taxonomy_of_workflow_management_systems_for_grid_computing.pdf},
  journal = {Journal of Grid Computing},
  keywords = {Grid computing,resource management,scheduling,taxonomy,workflow management},
  language = {en},
  number = {3}
}

@article{yu2005c,
  title = {Workflow Management and Resource Discovery for an Intelligent Grid},
  author = {Yu, Han and Bai, Xin and Marinescu, Dan C.},
  year = {2005},
  month = jul,
  volume = {31},
  pages = {797--811},
  issn = {0167-8191},
  doi = {10.1016/j.parco.2005.04.009},
  abstract = {A computational grid provides coordinated and transparent access to computing resources for grid users. Workflow management and resource discovery are two important functions of an intelligent grid. Workflow management refers to automatic workflow creation and coordinated workflow execution, and resource discovery facilitates resource allocation and claiming. In this paper we discuss workflow management and resource discovery in an intelligent grid environment.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2005/yu_et_al_2005_workflow_management_and_resource_discovery_for_an_intelligent_grid.pdf;/Users/rwb/Zotero/storage/DYDMHE9K/S0167819105000670.html},
  journal = {Parallel Computing},
  keywords = {Grid computing,Matchmaking,Planning,Process coordination,Resource discovery},
  number = {7},
  series = {Heterogeneous {{Computing}}}
}

@misc{yu2006,
  title = {Scheduling {{Scientific Workflow Applications}} with {{Deadline}} and {{Budget Constraints Using Genetic Algorithms}}},
  author = {Yu, Jia and Buyya, Rajkumar},
  year = {2006},
  doi = {10.1155/2006/271608},
  abstract = {Grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models, which are capable of supporting diverse computing services. It facilitates scientific applications to take advantage of computing resources distributed world wide to enhance the capability and performance. Many scientific applications in areas such as bioinformatics and astronomy require workflow processing in which tasks are executed based on their control or data dependencies. Scheduling such interdependent tasks on utility Grid environments need to consider users' QoS requirements. In this paper, we present a genetic algorithm approach to address scheduling optimization problems in workflow applications, based on two QoS constraints, deadline and budget.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/yu_buyya_2006_scheduling_scientific_workflow_applications_with_deadline_and_budget.pdf;/Users/rwb/Zotero/storage/FCIFHLPY/abs.html},
  howpublished = {https://www.hindawi.com/journals/sp/2006/271608/abs/},
  journal = {Scientific Programming},
  language = {en},
  type = {Research Article}
}

@inproceedings{yu2007,
  title = {Multi-Objective {{Planning}} for {{Workflow Execution}} on {{Grids}}},
  booktitle = {Proceedings of the 8th {{IEEE}}/{{ACM International Conference}} on {{Grid Computing}}},
  author = {Yu, Jia and Kirley, Michael and Buyya, Rajkumar},
  year = {2007},
  pages = {10--17},
  publisher = {{IEEE Computer Society}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/GRID.2007.4354110},
  abstract = {Utility Grids create an infrastructure for enabling users to consume services transparently over a global network. When optimizing workflow execution on utility Grids, we need to consider multiple Quality of Service (QoS) parameters including service prices and execution time. These optimization objectives may be in conflict. In this paper, we have proposed a workflow execution planning approach using multi-objective evolutionary algorithms (MOEAs). Our goal was to generate a set of trade-off scheduling solutions according to the users QoS requirements. The alternative trade-off solutions offer more flexibility to users when estimating their QoS requirements of workflow executions. Simulation results show that MOEAs are able to find a range of compromise solutions in a short computational time.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2007/yu_et_al_2007_multi-objective_planning_for_workflow_execution_on_grids.pdf},
  isbn = {978-1-4244-1559-5},
  series = {{{GRID}} '07}
}

@inproceedings{yu2008,
  ids = {yu2008},
  title = {A {{Planner}}-{{Guided Scheduling Strategy}} for {{Multiple Workflow Applications}}},
  booktitle = {2008 {{International Conference}} on {{Parallel Processing}} - {{Workshops}}},
  author = {Yu, Z. and Shi, W.},
  year = {2008},
  month = sep,
  pages = {1--8},
  doi = {10.1109/ICPP-W.2008.10},
  abstract = {Workflow applications are gaining popularity in recent years because of the prevalence of cluster environments. Many algorithms have been developed since, however most static algorithms are designed in the problem domain of scheduling single workflow applications, thus not applicable to a common cluster environment where multiple workflow applications and other independent jobs compete for resources. Dynamic scheduling approaches can handle the mixed workload practically by nature but their performance has yet to optimize as they do not have a global view of workflow applications. Recent research efforts suggest merging multiple workflows into one workflow before execution, but fail to address an important issue that multiple workflow applications may be submitted at different times by different users. In this paper, we propose a planner-guided dynamic scheduling strategy for multiple workflow applications, leveraging job dependence information and execution time estimation.Our approach schedules individual jobs dynamically without requiring merging the workflow applications a priori. The simulation results show that the proposed algorithm significantly outperforms two other algorithms by 43.6\% and 36.7\% with respect to workflow makespan and turnaround time respectively, and it performs even better when the number of concurrent workflow applications increases and the resources are scarce.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2008/yu_shi_2008_a_planner-guided_scheduling_strategy_for_multiple_workflow_applications.pdf;/Users/rwb/Dropbox/PhD/zotero/2008/yu_shi_2008_a_planner-guided_scheduling_strategy_for_multiple_workflow_applications.pdf;/Users/rwb/Zotero/storage/NKJFPEEK/citations.html;/Users/rwb/Zotero/storage/VTHWQP97/4626773.html},
  keywords = {_tablet,Algorithm design and analysis,cluster,Clustering algorithms,Computational modeling,Dynamic scheduling,dynamic scheduling strategy,execution time estimation,graph theory,Hafnium,Heuristic algorithms,leveraging job dependence information,multiple workflow applications,planner-guided scheduling strategy,scheduling,Scheduling,workflow}
}

@article{zaharia,
  title = {Resilient {{Distributed Datasets}}: {{A Fault}}-{{Tolerant Abstraction}} for {{In}}-{{Memory Cluster Computing}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  pages = {14},
  abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  file = {/Users/rwb/Zotero/storage/JL8GPBDZ/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf},
  language = {en}
}

@article{zhang2017,
  title = {A {{Survey}} on {{Multi}}-{{Task Learning}}},
  author = {Zhang, Yu and Yang, Qiang},
  year = {2017},
  month = jul,
  abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.},
  archivePrefix = {arXiv},
  eprint = {1707.08114},
  eprinttype = {arxiv},
  file = {/Users/rwb/Dropbox/PhD/zotero/2017/zhang_yang_2017_a_survey_on_multi-task_learning.pdf;/Users/rwb/Zotero/storage/9CYDDAZS/1707.html},
  journal = {arXiv:1707.08114 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{zhao2006,
  title = {Scheduling Multiple {{DAGs}} onto Heterogeneous Systems},
  booktitle = {Proceedings 20th {{IEEE International Parallel Distributed Processing Symposium}}},
  author = {Zhao, Henan and Sakellariou, R.},
  year = {2006},
  month = apr,
  pages = {14 pp.-},
  doi = {10.1109/IPDPS.2006.1639387},
  abstract = {The problem of scheduling a single DAG onto heterogeneous systems has been studied extensively. In this paper, we focus on the problem of scheduling more than one DAG at the same time onto a set of heterogeneous resources. The aim is not only to optimize the overall makespan, but also to achieve fairness, defined on the basis of the slowdown that each DAG would experience as a result of competing for resources with other DAGs. Two policies particularly focussing to deliver fairness are presented and evaluated along with another four policies that can be used to schedule multiple DAGs.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2006/zhao_sakellariou_2006_scheduling_multiple_dags_onto_heterogeneous_systems.pdf;/Users/rwb/Zotero/storage/NBS23PU4/1639387.html},
  keywords = {Computer science,Constraint optimization,Costs,directed acyclic graph,directed graphs,Grid computing,heterogeneous resources,heterogeneous systems,multiple DAG scheduling,Processor scheduling,Quality of service,resource allocation,scheduling,Tellurium}
}

@inproceedings{zhao2014,
  title = {Architecting {{Cloud Workflow}}: {{Theory}} and {{Practice}}},
  shorttitle = {Architecting {{Cloud Workflow}}},
  booktitle = {2014 {{IEEE International Conference}} on {{Computer}} and {{Information Technology}}},
  author = {Zhao, Yong and Li, Youfu and Raicu, Ioan and Lu, Shiyong and Zhang, Xuan},
  year = {2014},
  month = sep,
  pages = {466--473},
  publisher = {{IEEE}},
  address = {{Xi'an, China}},
  doi = {10.1109/CIT.2014.81},
  abstract = {The data scale, science analysis and processing complexity in scientific community are growing exponentially in the ``big data'' era. Cloud computing paradigm has been widely adopted to provide unprecedented scalability and resources on demand, while scientific workflow management systems (SWFMSs) have been proven essential to scientific computing and services computing. Uniting the advantages of both cloud computing and SWFMSs can bring a valuable solution to the scientific ``big data'' problem to researchers. Although a series of work have concentrated on integrating SWFMSs with Cloud platforms that provide much experience for future research and development, a study from an architectural perspective is still missing. The main contributions of this paper are: 1) based on a comprehensive survey of the available integration options, we propose a service framework for integrating SWFMSs with Cloud computing; 2) we implement the service framework based on various Cloud platforms to validate the feasibility of the proposed framework; and 3) we conduct a set of experiments to demonstrate the capability and use a NASA MODIS image processing workflow as a showcase of the implementation.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2014/zhao_et_al_2014_architecting_cloud_workflow.pdf},
  isbn = {978-1-4799-6239-6},
  keywords = {_tablet},
  language = {en}
}

@article{zheng2013,
  title = {Stochastic {{DAG}} Scheduling Using a {{Monte Carlo}} Approach},
  author = {Zheng, Wei and Sakellariou, Rizos},
  year = {2013},
  month = dec,
  volume = {73},
  pages = {1673--1689},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2013.07.019},
  abstract = {In heterogeneous computing systems, there is a need for solutions that can cope with the unavoidable uncertainty in individual task execution times, when scheduling DAGs. When such uncertainties occur, static DAG scheduling approaches may suffer, and some rescheduling may be necessary. Assuming that the uncertainty in task execution times is modelled in a stochastic manner, we may be able to use this information to improve static DAG scheduling considerably. In this paper, a novel DAG scheduling approach is proposed to solve this stochastic scheduling problem, based on a Monte Carlo method. The approach is built on the top of a classic static DAG scheduling heuristic and evaluated through extensive simulation. Empirical results show that a significant improvement of average application performance can be achieved by the proposed approach at a reasonable execution time cost.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2013/zheng_sakellariou_2013_stochastic_dag_scheduling_using_a_monte_carlo_approach.pdf;/Users/rwb/Zotero/storage/8NNZ8EHD/S0743731513001573.html},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {DAG scheduling,Directed acyclic graphs,Heterogeneous computing,Monte Carlo methods},
  number = {12},
  series = {Heterogeneity in {{Parallel}} and {{Distributed Computing}}}
}

@article{zhong2018,
  title = {A Bi-Objective Integer Programming Model for Partly-Restricted Flight Departure Scheduling},
  author = {Zhong, Han and Guan, Wei and Zhang, Wenyi and Jiang, Shixiong and Fan, Lingling},
  year = {2018},
  month = may,
  volume = {13},
  pages = {e0196146},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0196146},
  abstract = {The normal studies on air traffic departure scheduling problem (DSP) mainly deal with an independent airport in which the departure traffic is not affected by surrounded airports, which, however, is not a consistent case. In reality, there still exist cases where several commercial airports are closely located and one of them possesses a higher priority. During the peak hours, the departure activities of the lower-priority airports are usually required to give way to those of higher-priority airport. These giving-way requirements can inflict a set of changes on the modeling of departure scheduling problem with respect to the lower-priority airports. To the best of our knowledge, studies on DSP under this condition are scarce. Accordingly, this paper develops a bi-objective integer programming model to address the flight departure scheduling of the partly-restricted (e.g., lower-priority) one among several adjacent airports. An adapted tabu search algorithm is designed to solve the current problem. It is demonstrated from the case study of Tianjin Binhai International Airport in China that the proposed method can obviously improve the operation efficiency, while still realizing superior equity and regularity among restricted flows.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/zhong_et_al_2018_a_bi-objective_integer_programming_model_for_partly-restricted_flight_departure.pdf;/Users/rwb/Zotero/storage/JSVLL4UI/article.html},
  journal = {PLOS ONE},
  keywords = {Air flow,Aircraft,Airports,Aviation,Economic growth,Flow rate,Optimization,Turbulence},
  language = {en},
  number = {5}
}

@inproceedings{zhou2012,
  ids = {zhou2012},
  title = {A {{Comparison}} of {{CP}}, {{IP}}, and {{SAT Solvers}} through a {{Common Interface}}},
  booktitle = {2012 {{IEEE}} 24th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Zhou, N. and Tsuru, M. and Nobuyama, E.},
  year = {2012},
  month = nov,
  volume = {1},
  pages = {41--48},
  doi = {10.1109/ICTAI.2012.15},
  abstract = {This paper presents a common interface for Prolog to three different types of discrete solvers including Constraint Programming (CP), Integer Programming (IP), and SAT solvers. The interface comprises primitives for creating decision variables, specifying constraints, and invoking a solver, possibly with an objective function to be optimized. Before a solver is actually called, the accumulated variables and constraints are transformed into a form acceptable to the solver. For a SAT solver, in particular, variables are Booleanized and constraints are compiled into CNF. Implemented in B-Prolog, the interface allows the programmer to use the features of the host language such as recursion, pattern matching, arrays, and loops to describe problems. The interface provides an easy and uniform platform for exploring different solvers and models. This paper compares the performance of the CLP(FD) of B-Prolog, the CPLEX IP solver, and the Lingeling SAT solver on several problems through the same interface and for each problem it compares a model that uses Boolean variables and another model that uses general integer variables. Our experience tells that it is effortless to switch from one solver to another.},
  file = {/Users/rwb/Zotero/storage/SAKQVR42/6495027.html;/Users/rwb/Zotero/storage/VZ24MJFM/6495027.html},
  keywords = {array,B-Prolog,Boolean functions,Boolean variables,Booleanized variables,CLP(FD),Color,Combinatorial problems,common interface,computability,constraint handling,constraint programming,Constraint programming,constraint specification,CPLEX IP solver,decision variables,discrete solvers,Educational institutions,formal specification,general integer variables,integer programming,Integer programming,IP networks,Linear programming,Lingeling SAT solver,loops,Mathematical model,objective function optimization,pattern matching,Pattern matching,program control structures,Programming,Prolog,PROLOG,recursion,SAT solvers}
}

@article{zhou2018,
  ids = {zhou2018a},
  title = {Concurrent Workflow Budget- and Deadline-Constrained Scheduling in Heterogeneous Distributed Environments},
  author = {Zhou, Naqin and Li, FuFang and Xu, Kefu and Qi, Deyu},
  year = {2018},
  month = dec,
  volume = {22},
  pages = {7705--7718},
  issn = {1433-7479},
  doi = {10.1007/s00500-018-3229-3},
  abstract = {In heterogeneous distributed environment, it is a great challenge to schedule multiple workflows submitted at different times. Particularly, scheduling of concurrent workflows with deadline and budget constraints makes the problem become more complex. Recent studies have proposed dynamic scheduling strategies for concurrent workflows which have limitations in inconsistent environments. Therefore, this paper presents a new dynamic scheduling algorithm for concurrent workflows. This algorithm proposes a uniform ranking that considers the time and costs for both workflows and workgroups to assign priorities for tasks. In the resource selection phase, it controls the resource selection range for each task based on an optimistic budget for the current task and selects resources for the current task according to a defined bi-factor. The experimental results show that our algorithm outperforms the existing algorithms in both consistent and inconsistent environments.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2018/zhou_et_al_2018_concurrent_workflow_budget-_and_deadline-constrained_scheduling_in.pdf;/Users/rwb/Dropbox/PhD/zotero/2018/zhou_et_al_2018_concurrent_workflow_budget-_and_deadline-constrained_scheduling_in2.pdf},
  journal = {Soft Computing},
  keywords = {Budget,Concurrent workflows,Deadline,Online scheduling,Quality of service},
  language = {en},
  number = {23}
}

@article{zhu2016,
  ids = {zhu2016a},
  title = {Evolutionary {{Multi}}-{{Objective Workflow Scheduling}} in {{Cloud}}},
  author = {Zhu, Z. and Zhang, G. and Li, M. and Liu, X.},
  year = {2016},
  month = may,
  volume = {27},
  pages = {1344--1357},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2015.2446459},
  abstract = {Cloud computing provides promising platforms for executing large applications with enormous computational resources to offer on demand. In a Cloud model, users are charged based on their usage of resources and the required quality of service (QoS) specifications. Although there are many existing workflow scheduling algorithms in traditional distributed or heterogeneous computing environments, they have difficulties in being directly applied to the Cloud environments since Cloud differs from traditional heterogeneous environments by its service-based resource managing method and pay-per-use pricing strategies. In this paper, we highlight such difficulties, and model the workflow scheduling problem which optimizes both makespan and cost as a Multi-objective Optimization Problem (MOP) for the Cloud environments. We propose an evolutionary multi-objective optimization (EMO)-based algorithm to solve this workflow scheduling problem on an infrastructure as a service (IaaS) platform. Novel schemes for problem-specific encoding and population initialization, fitness evaluation and genetic operators are proposed in this algorithm. Extensive experiments on real world workflows and randomly generated workflows show that the schedules produced by our evolutionary algorithm present more stability on most of the workflows with the instance-based IaaS computing and pricing models. The results also show that our algorithm can achieve significantly better solutions than existing state-of-the-art QoS optimization scheduling algorithms in most cases. The conducted experiments are based on the on-demand instance types of Amazon EC2; however, the proposed algorithm are easy to be extended to the resources and pricing models of other IaaS services.},
  file = {/Users/rwb/Dropbox/PhD/zotero/2016/zhu_et_al_2016_evolutionary_multi-objective_workflow_scheduling_in_cloud.pdf;/Users/rwb/Dropbox/PhD/zotero/2016/zhu_et_al_2016_evolutionary_multi-objective_workflow_scheduling_in_cloud2.pdf;/Users/rwb/Zotero/storage/L2Z4N9G2/7127017.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Amazon EC2,cloud computing,Cloud computing,cloud environments,cloud model,Computational modeling,computational resources,distributed computing environment,EMO-based algorithm,Encoding,evolutionary algorithm,evolutionary computation,evolutionary multiobjective workflow scheduling,heterogeneous computing environment,IaaS platform,infrastructure as a service,Infrastructure as a Service,instance-based IaaS computing model,instance-based IaaS pricing model,MOP,multi-objective optimization,multiobjective optimization problem,pay-per-use pricing strategies,Pricing,Processor scheduling,QoS specification,quality of service,Quality of service,quality-of-service specification,resource allocation,Schedules,scheduling,Scheduling,service-based resource managing method,workflow scheduling,workflow scheduling algorithms},
  number = {5}
}

@misc{zotero-12,
  title = {Zotero | {{Your}} Personal Research Assistant},
  file = {/Users/rwb/Zotero/storage/B4C2YRAZ/start.html},
  howpublished = {https://www.zotero.org/start}
}

@misc{zotero-1363,
  title = {C++ {{Tutorial}}: {{Create QT}} Applications without {{QTCreator}}},
  shorttitle = {C++ {{Tutorial}}},
  abstract = {Introduction:
In this thread user @blackneos940 asked a couple of QT related questions.

I've put together a little tutorial, which hopefully answers his questions.
But I've decided to post it in it's own thread, as a piece of stand-alone content for the rest of the community.

The questions...},
  file = {/Users/rwb/Zotero/storage/BCRE8LB5/c-tutorial-create-qt-applications-without-qtcreator.html},
  howpublished = {https://www.linux.org/threads/c-tutorial-create-qt-applications-without-qtcreator.18409/},
  journal = {Linux.org},
  language = {en-US}
}

@misc{zotero-1791,
  title = {{{PaperCut MF}} : {{Web Print}}},
  file = {/Users/rwb/Zotero/storage/T6KHE97U/app.html},
  howpublished = {https://print.uwa.edu.au/app}
}

@misc{zotero-1815,
  title = {Fpga - {{Power}} Consumption Estimation from Number of {{FLOPS}} (Floating Point Operations)?},
  file = {/Users/rwb/Zotero/storage/M49P9RWT/power-consumption-estimation-from-number-of-flops-floating-point-operations.html},
  howpublished = {https://stackoverflow.com/questions/38901527/power-consumption-estimation-from-number-of-flops-floating-point-operations},
  journal = {Stack Overflow}
}

@misc{zotero-1818,
  title = {Core 2 {{Quad}} Flops - {{Google Search}}},
  file = {/Users/rwb/Zotero/storage/2VVGRSJ9/search.html},
  howpublished = {https://www.google.com/search?client=ubuntu\&hs=Ddn\&channel=fs\&ei=U0tTXu7-D\_bcz7sPnbmIwA0\&q=Core+2+Quad+flops\&oq=Core+2+Quad+flops\&gs\_l=psy-ab.3..33i160l2.9669.14133..14297...1.4..0.309.1688.2-6j1......0....2j1..gws-wiz.......0i71j0i273j0i67j0j0i22i30.a5m7eb3nzsw\&ved=0ahUKEwiu9PzAp-nnAhV27nMBHZ0cAtgQ4dUDCAo\&uact=5}
}

@misc{zotero-1931,
  title = {{{RobertLexis}}/{{CloudSimPy}}},
  abstract = {CloudSimPy: Datacenter job scheduling simulation framework - RobertLexis/CloudSimPy},
  file = {/Users/rwb/Zotero/storage/YTU6U7ED/main-makespan.html},
  howpublished = {https://github.com/RobertLexis/CloudSimPy},
  journal = {GitHub},
  language = {en}
}

@misc{zotero-1935,
  title = {How to Find Floating Point Operations per Clock Cycle Info?},
  file = {/Users/rwb/Zotero/storage/T8D4MEKG/how-to-find-floating-point-operations-per-clock-cycle-info.html},
  howpublished = {https://social.microsoft.com/Forums/en-US/0a0770e1-b3b0-4411-9584-7823eb749979/how-to-find-floating-point-operations-per-clock-cycle-info?forum=windowshpcitpros}
}

@misc{zotero-1936,
  title = {Sign in to {{Outlook}}},
  file = {/Users/rwb/Zotero/storage/PA49URH5/login.html},
  howpublished = {https://login.microsoftonline.com/common/login}
}

@misc{zotero-1937,
  title = {Begin: {{Quiz}} 1 \textendash{} {{GCRL1000}}\_{{SEM}}-1\_2020},
  file = {/Users/rwb/Zotero/storage/ID2STE5X/launchAssessment.html},
  howpublished = {https://lms.uwa.edu.au/webapps/assessment/take/launchAssessment.jsp?course\_id=\_50528\_1\&content\_id=\_1681610\_1\&mode=cpview}
}

@misc{zotero-2,
  title = {American Visa - {{Google Search}}},
  file = {/Users/rwb/Zotero/storage/ZJI7BK54/search.html},
  howpublished = {https://www.google.com/search?client=ubuntu\&channel=fs\&q=american+visa\&ie=utf-8\&oe=utf-8}
}

@misc{zotero-2171,
  title = {{{WhatsApp Web}}},
  abstract = {Quickly send and receive WhatsApp messages right from your computer.},
  file = {/Users/rwb/Zotero/storage/I9Y4QY3M/web.whatsapp.com.html},
  howpublished = {https://web.whatsapp.com/},
  language = {en}
}

@misc{zotero-2175,
  title = {Fibonacci {{Numbers}} - {{Competitive Programming Algorithms}}},
  file = {/Users/rwb/Zotero/storage/XEZHC45S/fibonacci-numbers.html},
  howpublished = {https://cp-algorithms.com/algebra/fibonacci-numbers.html}
}

@misc{zotero-2272,
  title = {A {{Chronology}} of {{Our Kickstarter}}},
  abstract = {A chronological plot of our first Kickstarter, from inception to the last planned stretch goal.},
  file = {/Users/rwb/Zotero/storage/KLDINSUU/news.html},
  howpublished = {https://mollyrocket.com/news\_0058},
  journal = {A Chronology of Our Kickstarter},
  language = {en}
}

@misc{zotero-2282,
  title = {{{IEEE Xplore Full}}-{{Text PDF}}:},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/ieee_xplore_full-text_pdf.pdf;/Users/rwb/Zotero/storage/69YGTTMA/stamp.html},
  howpublished = {https://ieeexplore-ieee-org.ezproxy.library.uwa.edu.au/stamp/stamp.jsp?tp=\&arnumber=7828435}
}

@misc{zotero-24,
  title = {Decision-{{Making Approaches}} for {{Performance QoS}} in {{Distributed Storage Systems}}: {{A Survey}} - {{IEEE Journals}} \& {{Magazine}}},
  file = {/Users/rwb/Zotero/storage/L92IJI8X/8618414.html},
  howpublished = {https://ieeexplore.ieee.org/document/8618414?source=tocalert\&dld=cmVzZWFyY2gudXdhLmVkdS5hdQ\%3D\%3D}
}

@misc{zotero-282,
  title = {Terence\_{{Yu}}\_{{Hang}}\_{{Leong}}-Chap04},
  file = {/Users/rwb/Zotero/storage/DD8L8YJQ/Terence_Yu_Hang_Leong-chap04.html},
  howpublished = {http://localhost:8888/notebooks/Terence\_Yu\_Hang\_Leong-chap04.ipynb}
}

@misc{zotero-284,
  title = {A {{Survey}} on {{Workflow Management}} and {{Scheduling}} in {{Cloud Computing}} - {{IEEE Conference Publication}}},
  file = {/Users/rwb/Zotero/storage/YNPJNTC4/6846537.html},
  howpublished = {https://ieeexplore.ieee.org/document/6846537}
}

@misc{zotero-3,
  title = {Data {{Processing}} with {{ASKAPSoft}}},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/data_processing_with_askapsoft.pdf}
}

@misc{zotero-334,
  title = {Pegasus {{FAQ}} - {{Pegasus}} - {{Pegasus Workflow Management System}}},
  file = {/Users/rwb/Zotero/storage/8T2CR23K/Pegasus+FAQ.html},
  howpublished = {https://confluence.pegasus.isi.edu/display/pegasus/Pegasus+FAQ}
}

@misc{zotero-347,
  title = {Integer Linear Programming-Based Cost Optimization for Scheduling Scientific Workflows in Multi-Cloud Environments | {{SpringerLink}}},
  file = {/Users/rwb/Zotero/storage/B2RBZSIA/s11227-018-2465-8.html},
  howpublished = {https://link-springer-com.ezproxy.library.uwa.edu.au/article/10.1007/s11227-018-2465-8}
}

@misc{zotero-447,
  title = {{{MOHEFT}}: {{A}} Multi-Objective List-Based Method for Workflow Scheduling - {{IEEE Conference Publication}}},
  file = {/Users/rwb/Zotero/storage/NL5PYLN4/6427573.html},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/6427573}
}

@article{zotero-501,
  title = {{{SKA}}-{{TEL}}-{{SKO}}-0000008-{{AG}}-{{REQ}}-{{SRS}}-{{Rev04}}-{{SKA1}}\_{{Level}}\_1\_{{System}}\_{{Requirement}}\_{{Specification}}},
  pages = {275},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/ska-tel-sko-0000008-ag-req-srs-rev04-ska1_level_1_system_requirement_specificati.pdf},
  language = {en}
}

@misc{zotero-513,
  title = {Multitask Optimization - {{Wikipedia}}},
  file = {/Users/rwb/Zotero/storage/84XZE2Z8/Multitask_optimization.html},
  howpublished = {https://en.wikipedia.org/wiki/Multitask\_optimization}
}

@misc{zotero-522,
  title = {Cost-Aware Challenges for Workflow Scheduling Approaches in Cloud Computing Environments: {{Taxonomy}} and Opportunities - {{ScienceDirect}}},
  file = {/Users/rwb/Zotero/storage/5GRJTVML/S0167739X15000242.html},
  howpublished = {https://www-sciencedirect-com.ezproxy.library.uwa.edu.au/science/article/pii/S0167739X15000242}
}

@article{zotero-546,
  title = {Understanding {{Peak Floating}}-{{Point Performance Claims}}},
  pages = {6},
  file = {/Users/rwb/Dropbox/PhD/zotero/undefined/understanding_peak_floating-point_performance_claims.pdf},
  language = {en}
}

@misc{zotero-563,
  title = {{{MOWS}} Board - {{Agile Board}} - {{ICRAR DIA JIRA}}},
  file = {/Users/rwb/Zotero/storage/JY9CF8LA/RapidBoard.html},
  howpublished = {https://jira.icrar.uwa.edu.au/secure/RapidBoard.jspa?rapidView=30\&projectKey=MOWS\&view=detail\&selectedIssue=MOWS-27\&sprint=36}
}

@misc{zotero-580,
  title = {Performance - {{What}} Is the Definition of {{Floating Point Operations}} ( {{FLOPs}} )},
  file = {/Users/rwb/Zotero/storage/WZ5BVXJS/what-is-the-definition-of-floating-point-operations-flops.html},
  howpublished = {https://stackoverflow.com/questions/52258370/what-is-the-definition-of-floating-point-operations-flops},
  journal = {Stack Overflow}
}

@misc{zotero-582,
  title = {Understanding Measures of Supercomputer Performance and Storage System Capacity},
  file = {/Users/rwb/Zotero/storage/Z9Z3P275/apeq.html},
  howpublished = {https://kb.iu.edu/d/apeq}
}

@misc{zotero-616,
  title = {{{IEEE Xplore Full}}-{{Text PDF}}:},
  file = {/Users/rwb/Zotero/storage/3LFPD3QZ/stamp.html},
  howpublished = {https://ieeexplore-ieee-org.ezproxy.library.uwa.edu.au/stamp/stamp.jsp?tp=\&arnumber=8476582\&tag=1}
}

@misc{zotero-635,
  title = {Extend {{Workflow Model}} to {{Manage}} the {{Process Branches}} in {{Planning}} and {{Scheduling}} - {{IEEE Conference Publication}}},
  file = {/Users/rwb/Zotero/storage/6N9GILHI/5421071.html},
  howpublished = {https://ieeexplore-ieee-org.ezproxy.library.uwa.edu.au/abstract/document/5421071}
}

@misc{zotero-651,
  title = {Google {{Maps}}},
  file = {/Users/rwb/Zotero/storage/V8JGLIL6/maps.html},
  howpublished = {https://www.google.com/maps}
}

@misc{zotero-661,
  title = {Scheduling under Uncertainty: {{Survey}} and Research Directions - {{IEEE Conference Publication}}},
  file = {/Users/rwb/Zotero/storage/BCIQGN4A/6866316.html},
  howpublished = {https://ieeexplore-ieee-org.ezproxy.library.uwa.edu.au/abstract/document/6866316}
}

@misc{zotero-662,
  title = {Machine {{Learning}} for {{Networking}}: {{Workflow}}, {{Advances}} and {{Opportunities}} - {{IEEE Journals}} \& {{Magazine}}},
  file = {/Users/rwb/Zotero/storage/75BE8IV7/8121867.html},
  howpublished = {https://ieeexplore-ieee-org.ezproxy.library.uwa.edu.au/abstract/document/8121867}
}

@misc{zotero-664,
  title = {{{WaaS}}: {{Workflow}}-as-a-{{Service}} for the {{Cloud}} with {{Scheduling}} of {{Continuous}} and {{Data}}-{{Intensive Workflows}} - {{OUP Journals}} \& {{Magazine}}},
  file = {/Users/rwb/Zotero/storage/343JL46A/8140848.html},
  howpublished = {https://ieeexplore-ieee-org.ezproxy.library.uwa.edu.au/abstract/document/8140848}
}

@misc{zotero-670,
  title = {Guide to {{Observing}} with the {{VLA}} \textemdash{} {{Science Website}}},
  file = {/Users/rwb/Zotero/storage/4AFEFST9/referencemanual-all-pages.html},
  howpublished = {https://science.nrao.edu/facilities/vla/docs/manuals/obsguide/referencemanual-all-pages}
}

@article{zotero-708,
  title = {Condor - a Hunter for Workstation},
  file = {/Users/rwb/Zotero/storage/NQXQTJYT/condor_-_a_hunter_for_workstation.pdf},
  keywords = {_tablet}
}

@misc{zotero-93,
  title = {Adaptive Dual-Criteria Task Group Allocation for Clustering-Based Multi-Workflow Scheduling on Parallel Computing Platform | {{SpringerLink}}},
  file = {/Users/rwb/Zotero/storage/D8QFSH6W/s11227-015-1469-x.html},
  howpublished = {https://link.springer.com/article/10.1007/s11227-015-1469-x}
}


