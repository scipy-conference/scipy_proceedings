
@misc{Alam_Kedro_2022,
  title = {Kedro},
  author = {Alam, Sajid and B{\u a}lan, Lorena and Comym, Gabriel and Dada, Yetunde and Danov, Ivan and Hoang, Lim and Kanchwala, Rashida and Klein, Jiri and Milne, Antony and Schwarzmann, Joel and Theisen, Merel and Wong, Susanna},
  year = {2022},
  howpublished = "\url{https://kedro.org/}",
  month = mar
}

@inproceedings{barrakCoevolutionMLPipelines2021,
  title = {On the {{Co-evolution}} of {{ML Pipelines}} and {{Source Code}} - {{Empirical Study}} of {{DVC Projects}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Barrak, Amine and Eghan, Ellis E. and Adams, Bram},
  year = {2021},
  month = mar,
  pages = {422--433},
  issn = {1534-5351},
  doi = {10.1109/SANER50967.2021.00046},
  abstract = {The growing popularity of machine learning (ML) applications has led to the introduction of software engineering tools such as Data Versioning Control (DVC), MLFlow and Pachyderm that enable versioning ML data, models, pipelines and model evaluation metrics. Since these versioned ML artifacts need to be synchronized not only with each other, but also with the source and test code of the software applications into which the models are integrated, prior findings on co-evolution and coupling between software artifacts might need to be revisited. Hence, in order to understand the degree of coupling between ML-related and other software artifacts, as well as the adoption of ML versioning features, this paper empirically studies the usage of DVC in 391 Github projects, 25 of which in detail. Our results show that more than half of the DVC files in a project are changed at least once every one-tenth of the project's lifetime. Furthermore, we observe a tight coupling between DVC files and other artifacts, with 1/4 pull requests changing source code and 1/2 pull requests changing tests requiring a change to DVC files. As additional evidence of the observed complexity associated with adopting ML-related software engineering tools like DVC, an average of 78\% of the studied projects showed a non-constant trend in pipeline complexity.},
  keywords = {Co-evolution,Complexity theory,Couplings,Data models,DVC,Measurement,ML Pipeline,ML versioning,Pipelines,Software,Tools,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\N4HB2YN7\\Barrak et al. - 2021 - On the Co-evolution of ML Pipelines and Source Cod.pdf;C\:\\Users\\81n\\Zotero\\storage\\RH5A9D9W\\9425888.html}
}

@article{belloumCollaborativeEScienceExperiments2011,
  title = {Collaborative E-{{Science Experiments}} and {{Scientific Workflows}}},
  author = {Belloum, Adam and Inda, Marcia A. and Vasunin, Dmitry and Korkhov, Vladimir and Zhao, Zhiming and Rauwerda, Han and Breit, Timo M. and Bubak, Marian and Hertzberger, Luis O.},
  year = {2011},
  month = jul,
  journal = {IEEE Internet Computing},
  volume = {15},
  number = {4},
  pages = {39--47},
  issn = {1941-0131},
  doi = {10.1109/MIC.2011.87},
  abstract = {Recent advances in Internet and grid technologies have greatly enhanced scientific experiments' life cycle. In addition to compute- and data-intensive tasks, large-scale collaborations involving geographically distributed scientists and e-infrastructure are now possible. Scientific workflows, which encode the logic of experiments, are becoming valuable resources. Sharing these resources and letting scientists worldwide work together on one experiment is essential for promoting knowledge transfer and speeding up the development of scientific experiments. Here, the authors discuss the challenges involved in supporting collaborative e-Science experiments and propose support for different phases of the scientific experimentation life cycle.},
  keywords = {Collaborative tools,Computational modeling,Design for experiments,e-Science,experiment life cycle,Internet,Monitoring,Scientific computing,Web services,Workflow management software,workflow management systems,WSRF},
  file = {C\:\\Users\\81n\\Zotero\\storage\\VTMLD94X\\Belloum et al. - 2011 - Collaborative e-Science Experiments and Scientific.pdf;C\:\\Users\\81n\\Zotero\\storage\\IBS4NBS7\\5934851.html}
}

@inproceedings{bergstraMakingScienceModel2013,
  title = {Making a {{Science}} of {{Model Search}}: {{Hyperparameter Optimization}} in {{Hundreds}} of {{Dimensions}} for {{Vision Architectures}}},
  shorttitle = {Making a {{Science}} of {{Model Search}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Bergstra, James and Yamins, Daniel and Cox, David},
  year = {2013},
  month = feb,
  pages = {115--123},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.     In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included.  A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric.  Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.},
  langid = {english},
  file = {C\:\\Users\\81n\\Zotero\\storage\\6D3BMBET\\Bergstra et al. - 2013 - Making a Science of Model Search Hyperparameter O.pdf}
}

@article{deelmanWorkflowsEScienceOverview2009,
  title = {Workflows and E-{{Science}}: {{An}} Overview of Workflow System Features and Capabilities},
  shorttitle = {Workflows and E-{{Science}}},
  author = {Deelman, Ewa and Gannon, Dennis and Shields, Matthew and Taylor, Ian},
  year = {2009},
  month = may,
  journal = {Future Generation Computer Systems},
  volume = {25},
  pages = {524--540},
  doi = {10.1016/j.future.2008.06.012},
  abstract = {Scientific workflow systems have become a necessary tool for many applications, enabling the composition and execution of complex analysis on distributed resources. Today there are many workflow systems, often with overlapping functionality. A key issue for potential users of work- flow systems is the need to be able to compare the capabilities of the various available tools. There can be confusion about system functionality and the tools are often selected without a proper functional analysis. In this paper we extract a taxonomy of features from the way sci- entists make use of existing workflow systems and we illustrate this feature set by providing some examples taken from existing workflow systems. The taxonomy provides end users with a mechanism by which they can assess the suitability of workflow in general and how they might use these features to make an informed choice about which workflow system would be a good choice for their particular application.},
  keywords = {analysis,claim:parameterization,claim:reproducibility,survey,taxonomy},
  file = {C\:\\Users\\81n\\Zotero\\storage\\46L8W4MT\\Deelman et al. - 2009 - Workflows and e-Science An overview of workflow s.pdf}
}

@article{domenechMlexperimentPythonFramework2020,
  title = {Ml-Experiment: {{A Python}} Framework for Reproducible Data Science},
  shorttitle = {Ml-Experiment},
  author = {Domenech, Antonio Molner and Guill{\'e}n, Alberto},
  year = {2020},
  month = sep,
  journal = {Journal of Physics: Conference Series},
  volume = {1603},
  number = {1},
  pages = {012025},
  publisher = {{IOP Publishing}},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1603/1/012025},
  abstract = {Nowadays, data science projects are usually developed in an unstructured way, which makes it difficult to reproduce. It is also hard to move from an experimental environment to production. Operational workflows such as containerization, continuous deployment, and cloud orchestration allow data science researchers to move a pipeline from a local environment to the cloud. Being aware of the difficulties of setting those workflows up, this paper presents a framework to ease experiment tracking and operationalizing machine learning by combining existent and well-supported technologies. These technologies include Docker, Mlflow, Ray, among others. The framework provides an opinionated workflow to design and execute experiments either on a local environment or the cloud. ml-experiment includes: an automatic tracking system for the most famous machine learning libraries: Tensorflow, Keras, Fastai, Xgboost and Lightgdm, first-class support for distributed training and hyperparameter optimization, and a Command Line Interface (CLI) for packaging and running projects inside containers.},
  langid = {english},
  keywords = {tool,tool:mlexperiment},
  file = {C\:\\Users\\81n\\Zotero\\storage\\G6HG2665\\Domenech and Guill√©n - 2020 - ml-experiment A Python framework for reproducible.pdf}
}

@article{donohoReproducibleResearchComputational2009,
  title = {Reproducible {{Research}} in {{Computational Harmonic Analysis}}},
  author = {Donoho, David L. and Maleki, Arian and Rahman, Inam Ur and Shahram, Morteza and Stodden, Victoria},
  year = {2009},
  month = jan,
  journal = {Computing in Science Engineering},
  volume = {11},
  number = {1},
  pages = {8--18},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2009.15},
  abstract = {Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immature\textemdash traditional scientific publication is incapable of finding and rooting out errors in scientific computation\textemdash which must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. The authors have practiced reproducible computational research for 15 years and have integrated it with their scientific research and with doctoral and postdoctoral education. In this article, they review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.},
  keywords = {claim:reproducibility,Computational modeling,computational science,Computer errors,Data mining,Harmonic analysis,Image recognition,Information management,Laboratories,Large-scale systems,Logic testing,reproducible research,scientific computing,Scientific computing,scientific publication},
  file = {C\:\\Users\\81n\\Zotero\\storage\\73TQKP2V\\Donoho et al. - 2009 - Reproducible Research in Computational Harmonic An.pdf;C\:\\Users\\81n\\Zotero\\storage\\XMSEXMIT\\4720218.html}
}

@article{duboisMaintainingCorrectnessScientific2005,
  title = {Maintaining Correctness in Scientific Programs},
  author = {Dubois, P.F.},
  year = {2005},
  month = may,
  journal = {Computing in Science Engineering},
  volume = {7},
  number = {3},
  pages = {80--85},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2005.54},
  abstract = {Combine a high rate of change (which makes correctness hard to maintain) with an increased sensitivity to failure to maintain correctness and you have a big problem. Solving this problem must be the focus of our methodology. In this paper, the author describes the layered approach that he found to be the most successful in maintaining correctness in the face of rapid change.},
  keywords = {Best practices,claim:setoml,correctness,Environmental management,History,Maintenance engineering,Open source software,Programming profession,Robustness,scientific programming,software development,Target tracking,Testing,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\9ZF546KJ\\Dubois - 2005 - Maintaining correctness in scientific programs.pdf;C\:\\Users\\81n\\Zotero\\storage\\4NJ3Q32S\\1425400.html}
}

@article{gobleFAIRComputationalWorkflows2020,
  title = {{{FAIR Computational Workflows}}},
  author = {Goble, Carole and {Cohen-Boulakia}, Sarah and {Soiland-Reyes}, Stian and Garijo, Daniel and Gil, Yolanda and Crusoe, Michael R. and Peters, Kristian and Schober, Daniel},
  year = {2020},
  month = jan,
  journal = {Data Intelligence},
  volume = {2},
  number = {1-2},
  pages = {108--121},
  issn = {2641-435X},
  doi = {10.1162/dint_a_00033},
  abstract = {Computational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.},
  keywords = {toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\4UBJRGU7\\Goble et al. - 2020 - FAIR Computational Workflows.pdf;C\:\\Users\\81n\\Zotero\\storage\\HDVVHJ9J\\FAIR-Computational-Workflows.html}
}

@misc{goyalMachineLearningOperations2020,
  title = {Machine Learning Operations},
  author = {Goyal, A.},
  year = {2020},
  journal = {International Journal of Information Technology Insights \& Transformations [ISSN: 2581-5172 (Online)]},
  volume = {4},
  publisher = {{Eureka Journals Pune, India}},
  file = {C\:\\Users\\81n\\Zotero\\storage\\Z3GG26NQ\\Goyal - 2020 - Machine learning operations.pdf}
}

@inproceedings{greffSacredInfrastructureComputational2017,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J{\"u}rgen},
  year = {2017},
  pages = {49--56},
  publisher = {{SciPy}},
  address = {{Austin, Texas}},
  doi = {10.25080/shinma-7f4c6e7-008},
  langid = {english},
  keywords = {tool,tool:sacred,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\DKYUVYWX\\Greff et al. - 2017 - The Sacred Infrastructure for Computational Resear.pdf}
}

@article{greffSacredInfrastructureComputational2017a,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J{\"u}rgen},
  year = {2017},
  journal = {Proceedings of the 16th Python in Science Conference},
  pages = {49--56},
  doi = {10.25080/shinma-7f4c6e7-008},
  file = {C\:\\Users\\81n\\Zotero\\storage\\IKH8DFKY\\Greff et al. - 2017 - The Sacred Infrastructure for Computational Resear.pdf;C\:\\Users\\81n\\Zotero\\storage\\XYHI7ZNQ\\klaus_greff.html}
}

@article{gundersenReproducibleAIReproducible2018,
  title = {On {{Reproducible AI}}: {{Towards Reproducible Research}}, {{Open Science}}, and {{Digital Scholarship}} in {{AI Publications}}},
  shorttitle = {On {{Reproducible AI}}},
  author = {Gundersen, Odd Erik and Gil, Yolanda and Aha, David W.},
  year = {2018},
  month = sep,
  journal = {AI Magazine},
  volume = {39},
  number = {3},
  pages = {56--68},
  issn = {2371-9621},
  doi = {10.1609/aimag.v39i3.2816},
  abstract = {Background: Science is experiencing a reproducibility crisis. Artificial intelligence research is not an exception. Objective: To give practical and pragmatic recommendations for how to document AI research so that the results are reproducible. Method: Our analysis of the literature shows that AI publications fall short of providing enough documentation to facilitate reproducibility. Our suggested best practices are based on a framework for reproducibility and recommendations given for other disciplines. Results: We have made an author checklist based on our investigation and provided examples for how every item in the checklist can be documented. Conclusion: We encourage reviewers to use the suggested best practices and author checklist when reviewing submissions for AAAI publications and future AAAI conferences.},
  copyright = {Copyright (c)  AI Magazine},
  langid = {english},
  keywords = {claim:reproducibility,important},
  file = {C\:\\Users\\81n\\Zotero\\storage\\P64DP6FJ\\Gundersen et al. - 2018 - On Reproducible AI Towards Reproducible Research,.pdf}
}

@article{gundersenStateArtReproducibility2018,
  title = {State of the {{Art}}: {{Reproducibility}} in {{Artificial Intelligence}}},
  shorttitle = {State of the {{Art}}},
  author = {Gundersen, Odd Erik and Kjensmo, Sigbj{\o}rn},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
  copyright = {Copyright (c)},
  doi = {10.1609/aaai.v32i1.11503},
  langid = {english},
  keywords = {claim:reproducibility,documentation,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\RJEBWNZY\\Gundersen and Kjensmo - 2018 - State of the Art Reproducibility in Artificial In.pdf}
}

@article{halchenkoDataLadDistributedSystem2021,
  title = {{{DataLad}}: Distributed System for Joint Management of Code, Data, and Their Relationship},
  shorttitle = {{{DataLad}}},
  author = {Halchenko, Yaroslav O. and Meyer, Kyle and Poldrack, Benjamin and Solanky, Debanjum Singh and Wagner, Adina S. and Gors, Jason and MacFarlane, Dave and Pustina, Dorian and Sochat, Vanessa and Ghosh, Satrajit S. and M{\"o}nch, Christian and Markiewicz, Christopher J. and Waite, Laura and Shlyakhter, Ilya and de la Vega, Alejandro and Hayashi, Soichi and H{\"a}usler, Christian Olaf and Poline, Jean-Baptiste and Kadelka, Tobias and Skyt{\'e}n, Kusti and Jarecka, Dorota and Kennedy, David and Strauss, Ted and Cieslak, Matt and Vavra, Peter and Ioanas, Horea-Ioan and Schneider, Robin and Pfl{\"u}ger, Mika and Haxby, James V. and Eickhoff, Simon B. and Hanke, Michael},
  year = {2021},
  month = jul,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {63},
  pages = {3262},
  issn = {2475-9066},
  doi = {10.21105/joss.03262},
  abstract = {Halchenko et al., (2021). DataLad: distributed system for joint management of code, data, and their relationship. Journal of Open Source Software, 6(63), 3262, https://doi.org/10.21105/joss.03262},
  langid = {english},
  keywords = {tool,tool:datalad},
  file = {C\:\\Users\\81n\\Zotero\\storage\\T2RJ5HLV\\Halchenko et al. - 2021 - DataLad distributed system for joint management o.pdf;C\:\\Users\\81n\\Zotero\\storage\\UMQ8B9RZ\\joss.html}
}

@article{halchenkoDataLadDistributedSystem2021a,
  title = {{{DataLad}}: Distributed System for Joint Management of Code, Data, and Their Relationship},
  shorttitle = {{{DataLad}}},
  author = {Halchenko, Yaroslav O. and Meyer, Kyle and Poldrack, Benjamin and Solanky, Debanjum Singh and Wagner, Adina S. and Gors, Jason and MacFarlane, Dave and Pustina, Dorian and Sochat, Vanessa and Ghosh, Satrajit S. and M{\"o}nch, Christian and Markiewicz, Christopher J. and Waite, Laura and Shlyakhter, Ilya and de la Vega, Alejandro and Hayashi, Soichi and H{\"a}usler, Christian Olaf and Poline, Jean-Baptiste and Kadelka, Tobias and Skyt{\'e}n, Kusti and Jarecka, Dorota and Kennedy, David and Strauss, Ted and Cieslak, Matt and Vavra, Peter and Ioanas, Horea-Ioan and Schneider, Robin and Pfl{\"u}ger, Mika and Haxby, James V. and Eickhoff, Simon B. and Hanke, Michael},
  year = {2021},
  month = jul,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {63},
  pages = {3262},
  issn = {2475-9066},
  doi = {10.21105/joss.03262},
  abstract = {Halchenko et al., (2021). DataLad: distributed system for joint management of code, data, and their relationship. Journal of Open Source Software, 6(63), 3262, https://doi.org/10.21105/joss.03262},
  langid = {english},
  keywords = {tool,tool:datalad,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\QHPWHVB2\\Halchenko et al. - 2021 - DataLad distributed system for joint management o.pdf;C\:\\Users\\81n\\Zotero\\storage\\AY3TB2MG\\joss.html}
}

@article{hutsonArtificialIntelligenceFaces2018,
  title = {Artificial Intelligence Faces Reproducibility Crisis},
  author = {Hutson, Matthew},
  year = {2018},
  month = feb,
  journal = {Science},
  volume = {359},
  number = {6377},
  pages = {725--726},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.359.6377.725},
  file = {C\:\\Users\\81n\\Zotero\\storage\\XJGF43W8\\Hutson - 2018 - Artificial intelligence faces reproducibility cris.pdf}
}

@inproceedings{isdahlOutoftheBoxReproducibilitySurvey2019,
  title = {Out-of-the-{{Box Reproducibility}}: {{A Survey}} of {{Machine Learning Platforms}}},
  shorttitle = {Out-of-the-{{Box Reproducibility}}},
  booktitle = {2019 15th {{International Conference}} on {{eScience}} ({{eScience}})},
  author = {Isdahl, Richard and Gundersen, Odd Erik},
  year = {2019},
  month = sep,
  pages = {86--95},
  doi = {10.1109/eScience.2019.00017},
  abstract = {Even machine learning experiments that are fully conducted on computers are not necessarily reproducible. An increasing number of open source and commercial, closed source machine learning platforms are being developed that help address this problem. However, there is no standard for assessing and comparing which features are required to fully support reproducibility. We propose a quantitative method that alleviates this problem. Based on the proposed method we assess and compare the current state of the art machine learning platforms for how well they support making empirical results reproducible. Our results show that BEAT and Floydhub have the best support for reproducibility with Codalab and Kaggle as close contenders. The most commonly used machine learning platforms provided by the big tech companies have poor support for reproducibility.},
  keywords = {analysis,Computer science,Documentation,Hardware,Licenses,Machine learning,machine-learning-platforms,Measurement,Reproducibility,reproducible-AI,survey,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\S8RELX6S\\Isdahl and Gundersen - 2019 - Out-of-the-Box Reproducibility A Survey of Machin.pdf;C\:\\Users\\81n\\Zotero\\storage\\KUNS5FHP\\9041744.html}
}

@article{kellyFiveRecommendedPractices2009a,
  title = {Five {{Recommended Practices}} for {{Computational Scientists Who Write Software}}},
  author = {Kelly, Diane and Hook, Daniel and Sanders, Rebecca},
  year = {2009},
  month = sep,
  journal = {Computing in Science Engineering},
  volume = {11},
  number = {5},
  pages = {48--53},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2009.139},
  abstract = {Few software engineering techniques and approaches are specifically useful for computational scientists, and despite recent efforts, it could be many years before a consolidated handbook is available. Meanwhile, computational scientists can look to the practices of other scientists who write successful software.},
  keywords = {Best practices,Computer architecture,Computer interfaces,Data structures,Guidelines,Lifting equipment,Military computing,scientific software,Software testing,toread,User interfaces,Writing},
  file = {C\:\\Users\\81n\\Zotero\\storage\\QCJN7Z2E\\Kelly et al. - 2009 - Five Recommended Practices for Computational Scien.pdf;C\:\\Users\\81n\\Zotero\\storage\\K7VK3AQC\\5228715.html}
}

@article{khritankovMLDevDataScience2021,
  title = {{{MLDev}}: {{Data Science Experiment Automation}} and {{Reproducibility Software}}},
  shorttitle = {{{MLDev}}},
  author = {Khritankov, Anton and Pershin, Nikita and Ukhov, Nikita and Ukhov, Artem},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.12322 [cs]},
  eprint = {2107.12322},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we explore the challenges of automating experiments in data science. We propose an extensible experiment model as a foundation for integration of different open source tools for running research experiments. We implement our approach in a prototype open source MLDev software package and evaluate it in a series of experiments yielding promising results. Comparison with other state-of-the-art tools signifies novelty of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Computer Science - Software Engineering,I.2.m,survey,tool,tool:dvc,tool:mldev},
  file = {C\:\\Users\\81n\\Zotero\\storage\\UY325QGM\\Khritankov et al. - 2021 - MLDev Data Science Experiment Automation and Repr.pdf;C\:\\Users\\81n\\Zotero\\storage\\H642VX67\\2107.html}
}

@misc{kuprieievDVCDataVersion2022,
  title = {{{DVC}}: {{Data Version Control}} - {{Git}} for {{Data}} \& {{Models}}},
  shorttitle = {{{DVC}}},
  author = {Kuprieiev, Ruslan and Pachhai, Saugat and Petrov, Dmitry and Redzy{\'n}ski, Pawe{\l} and da {Costa-Luis}, Casper and Rowlands, Peter and Schepanovski, Alexander and Shcheklein, Ivan and Taskaya, Batuhan and Orpinel, Jorge and Gao and Santos, F{\'a}bio and Castro, David de la Iglesia and Sharma, Aman and Zhanibek and Hodovic, Dani and Kodenko, Nikita and Grigorev, Andrew and Earl and Dash, Nabanita and Vyshnya, George and {maykulkarni} and Hora, Max and Vera and Mangal, Sanidhya and Baranowski, Wojciech and Wolff, Clemens and Benoy, Kurian},
  year = {2022},
  month = apr,
  doi = {10.5281/zenodo.6417224},
  abstract = {Refer to https://dvc.org/doc/install for installation instructions. Changes üöÄ New Features and Enhancements exp show: pcp: Allow to --drop Experiment (\#7536) @daavoo üêõ Bug Fixes build: workaround pyinstaller inspect.getmodule issue (\#7545) @pmrowla plots: image: fix image to\_json structure (\#7533) @pared üî® Maintenance build: workaround pyinstaller inspect.getmodule issue (\#7545) @pmrowla dvcfs: base on fsspec (\#7521) @efiop Thanks again to @daavoo, @efiop, @jorgeorpinel, @pared, @pmrowla and @skshetry for the contributions! üéâ},
  howpublished = {Zenodo},
  keywords = {ai,collaboration,data-science,data-version-control,developer-tools,git,machine-learning,python,reproducibility},
  file = {C\:\\Users\\81n\\Zotero\\storage\\3NJHSI2U\\6417224.html}
}

@article{lamprechtFAIRPrinciplesResearch2020,
  title = {Towards {{FAIR}} Principles For Research Software},
  author = {Lamprecht, Anna-Lena and Garcia, Leyla and Kuzak, Mateusz and Martinez, Carlos and Arcila, Ricardo and Martin Del Pico, Eva and Dominguez Del Angel, Victoria and {van de Sandt}, Stephanie and Ison, Jon and Martinez, Paula Andrea and McQuilton, Peter and Valencia, Alfonso and Harrow, Jennifer and Psomopoulos, Fotis and Gelpi, Josep Ll and Chue Hong, Neil and Goble, Carole and {Capella-Gutierrez}, Salvador},
  year = {2020},
  month = jan,
  journal = {Data Science},
  volume = {3},
  number = {1},
  pages = {37--59},
  publisher = {{IOS Press}},
  issn = {2451-8484},
  doi = {10.3233/DS-190026},
  abstract = {The FAIR Guiding Principles, published in 2016, aim to improve the findability, accessibility, interoperability and reusability of digital research objects for both humans and machines. Until now the FAIR principles have been mostly applied to resear},
  langid = {english},
  keywords = {toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\LZWZ43HQ\\Lamprecht et al. - 2020 - Towards FAIR principles for&nbsp\;research&nbsp\;sof.pdf;C\:\\Users\\81n\\Zotero\\storage\\B9BP89QU\\ds190026.html}
}

@article{madduriReproducibleBigData2019,
  title = {Reproducible Big Data Science: {{A}} Case Study in Continuous {{FAIRness}}},
  shorttitle = {Reproducible Big Data Science},
  author = {Madduri, Ravi and Chard, Kyle and D'Arcy, Mike and Jung, Segun C. and Rodriguez, Alexis and Sulakhe, Dinanath and Deutsch, Eric and Funk, Cory and Heavner, Ben and Richards, Matthew and Shannon, Paul and Glusman, Gustavo and Price, Nathan and Kesselman, Carl and Foster, Ian},
  year = {2019},
  month = apr,
  journal = {PLOS ONE},
  volume = {14},
  number = {4},
  pages = {e0213013},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0213013},
  abstract = {Big biomedical data create exciting opportunities for discovery, but make it difficult to capture analyses and outputs in forms that are findable, accessible, interoperable, and reusable (FAIR). In response, we describe tools that make it easy to capture, and assign identifiers to, data and code throughout the data lifecycle. We illustrate the use of these tools via a case study involving a multi-step analysis that creates an atlas of putative transcription factor binding sites from terabytes of ENCODE DNase I hypersensitive sites sequencing data. We show how the tools automate routine but complex tasks, capture analysis algorithms in understandable and reusable forms, and harness fast networks and powerful cloud computers to process data rapidly, all without sacrificing usability or reproducibility\textemdash thus ensuring that big data are not hard-to-(re)use data. We evaluate our approach via a user study, and show that 91\% of participants were able to replicate a complex analysis involving considerable data volumes.},
  langid = {english},
  keywords = {Cloud computing,Computer software,DNA footprinting,DNase Seq,Genomics,Metadata,Reproducibility,Sequence alignment,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\BPM93367\\Madduri et al. - 2019 - Reproducible big data science A case study in con.pdf;C\:\\Users\\81n\\Zotero\\storage\\35NK9Q6U\\article.html}
}

@misc{martindaleCurifactory2022,
  title = {Curifactory},
  author = {Martindale, Nathan and Hite, Jason and Stewart, Scott L. and Adams, Mark},
  year = {2022},
  month = mar,
  abstract = {An experiment workflow and organization tool.},
  copyright = {BSD-3-Clause},
  howpublished = "\url{https://github.com/ORNL/curifactory}"
}

@article{merelo-guervosAgileDataScience2021,
  title = {Agile (Data) Science: A (Draft) Manifesto},
  shorttitle = {Agile (Data) Science},
  author = {{Merelo-Guerv{\'o}s}, Juan Juli{\'a}n and {Garc{\'i}a-Valdez}, Mario},
  year = {2021},
  month = may,
  journal = {arXiv:2104.12545 [cs]},
  eprint = {2104.12545},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Science has a data management as well as a project management problem. While industrial grade data science teams have embraced the *agile* mindset, and adopted or created all kind of tools to manage reproducible workflows, academia-based science is still (mostly) mired in a mindset that's focused on a single final product (a paper), without focusing on incremental improvement and, over all, reproducibility. In this report we argue towards the adoption of the agile mindset and agile data science tools in academia, to make a more responsible, sustainable, and above all, reproducible science.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Software Engineering,D.1.m,J.m},
  file = {C\:\\Users\\81n\\Zotero\\storage\\S9TL7L9D\\Merelo-Guerv√≥s and Garc√≠a-Valdez - 2021 - Agile (data) science a (draft) manifesto.pdf;C\:\\Users\\81n\\Zotero\\storage\\6V9CX6U4\\2104.html}
}

@article{mitchellFAIRDataPipeline2021,
  title = {{{FAIR Data Pipeline}}: Provenance-Driven Data Management for Traceable Scientific Workflows},
  shorttitle = {{{FAIR Data Pipeline}}},
  author = {Mitchell, Sonia Natalie and Lahiff, Andrew and Cummings, Nathan and Hollocombe, Jonathan and Boskamp, Bram and Reddyhoff, Dennis and Field, Ryan and Zarebski, Kristian and Wilson, Antony and Burke, Martin and Archibald, Blair and Bessell, Paul and Blackwell, Richard and Boden, Lisa A. and Brett, Alys and Brett, Sam and Dundas, Ruth and Enright, Jessica and {Gonzalez-Beltran}, Alejandra N. and Harris, Claire and Hinder, Ian and Hughes, Christopher David and Knight, Martin and Mano, Vino and McMonagle, Ciaran and Mellor, Dominic and Mohr, Sibylle and Marion, Glenn and Matthews, Louise and McKendrick, Iain J. and Pooley, Christopher Mark and Porphyre, Thibaud and Reeves, Aaron and Townsend, Edward and Turner, Robert and Walton, Jeremy and Reeve, Richard},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07117 [cs, q-bio]},
  eprint = {2110.07117},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Modern epidemiological analyses to understand and combat the spread of disease depend critically on access to, and use of, data. Rapidly evolving data, such as data streams changing during a disease outbreak, are particularly challenging. Data management is further complicated by data being imprecisely identified when used. Public trust in policy decisions resulting from such analyses is easily damaged and is often low, with cynicism arising where claims of "following the science" are made without accompanying evidence. Tracing the provenance of such decisions back through open software to primary data would clarify this evidence, enhancing the transparency of the decision-making process. Here, we demonstrate a Findable, Accessible, Interoperable and Reusable (FAIR) data pipeline developed during the COVID-19 pandemic that allows easy annotation of data as they are consumed by analyses, while tracing the provenance of scientific outputs back through the analytical source code to data sources. Such a tool provides a mechanism for the public, and fellow scientists, to better assess the trust that should be placed in scientific evidence, while allowing scientists to support policy-makers in openly justifying their decisions. We believe that tools such as this should be promoted for use across all areas of policy-facing research.},
  archiveprefix = {arXiv},
  keywords = {claim:reproducibility,Computer Science - Digital Libraries,Quantitative Biology - Quantitative Methods,tool,tool:datalad,tool:dvc},
  file = {C\:\\Users\\81n\\Zotero\\storage\\TDCTQ3JX\\Mitchell et al. - 2021 - FAIR Data Pipeline provenance-driven data managem.pdf;C\:\\Users\\81n\\Zotero\\storage\\V284V543\\2110.html}
}

@misc{MLflowMachineLearning2022,
  title = {{{MLflow}}: {{A Machine Learning Lifecycle Platform}}},
  shorttitle = {{{MLflow}}},
  year = {2022},
  month = apr,
  abstract = {Open source platform for the machine learning lifecycle},
  copyright = {Apache-2.0},
  howpublished = "\url{https://mlflow.org/}",
  keywords = {ai,apache-spark,machine-learning,ml,mlflow,model-management}
}

@inproceedings{namakiVamsaAutomatedProvenance2020,
  title = {Vamsa: {{Automated Provenance Tracking}} in {{Data Science Scripts}}},
  shorttitle = {Vamsa},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Namaki, Mohammad Hossein and Floratou, Avrilia and Psallidas, Fotis and Krishnan, Subru and Agrawal, Ashvin and Wu, Yinghui and Zhu, Yiwen and Weimer, Markus},
  year = {2020},
  month = aug,
  series = {{{KDD}} '20},
  pages = {1542--1551},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3394486.3403205},
  abstract = {There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4\% to 99.1\% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.},
  isbn = {978-1-4503-7998-4},
  keywords = {automated,data science,machine learning,provenance},
  file = {C\:\\Users\\81n\\Zotero\\storage\\D95KKA88\\Namaki et al. - 2020 - Vamsa Automated Provenance Tracking in Data Scien.pdf}
}

@inproceedings{olorisadeReproducibilityMachineLearningBased2017,
  title = {Reproducibility in {{Machine Learning-Based Studies}}: {{An Example}} of {{Text Mining}}},
  shorttitle = {Reproducibility in {{Machine Learning-Based Studies}}},
  booktitle = {Reproducibility in ML Workshop, 34th International Conference on Machine Learning},
  author = {Olorisade, Babatunde K. and Brereton, Pearl and Andras, Peter},
  year = {2017},
  series = {ICML 2017},
  month = jun,
  abstract = {An exploration of text mining experiment factors affecting reproducibility},
  langid = {english},
  keywords = {claim:reproducibility},
  file = {C\:\\Users\\81n\\Zotero\\storage\\EUF86HEN\\Olorisade et al. - 2017 - Reproducibility in Machine Learning-Based Studies.pdf;C\:\\Users\\81n\\Zotero\\storage\\32XNLWC8\\forum.html}
}

@misc{PachydermLeaderData2022,
  title = {Pachyderm \textendash{} {{The Leader}} in {{Data Versioning}} and {{Pipelines}} for {{MLOps}}},
  year = {2022},
  month = apr,
  abstract = {The Complete MLOps Stack},
  howpublished = {Pachyderm},
  keywords = {analytics,big-data,containers,data-analysis,data-science,distributed-systems,docker,go,kubernetes,pachyderm}
}

@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  journal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  langid = {english},
  keywords = {claim:reproducibility},
  file = {C\:\\Users\\81n\\Zotero\\storage\\8F963THZ\\Peng - 2011 - Reproducible Research in Computational Science.pdf}
}

@inproceedings{quarantaTaxonomyToolsReproducible,
  title = {A {{Taxonomy}} of {{Tools}} for {{Reproducible Machine Learning Experiments}}},
  author = {Quaranta, Luigi and Calefato, Fabio and Lanubile, Filippo},
  booktitle = {AIxIA 2021 Discussion Papers, 20th International Conference of the Italian Association for Artificial Intelligence},
  pages = {65--76},
  year = {2021},
  abstract = {The broad availability of machine learning (ML) libraries and frameworks makes the rapid prototyping of ML models a relatively easy task to achieve. However, the quality of prototypes is challenged by their reproducibility. Reproducing an ML experiment typically entails repeating the whole process, from data collection to model building, other than multiple optimization steps that must be carefully tracked. In this paper, we define a comprehensive taxonomy to characterize tools for ML experiment tracking and review some of the most popular solutions under the lens of the taxonomy. The taxonomy and related recommendations may help data scientists to more easily orient themselves and make an informed choice when selecting appropriate tools to shape the workflow of their ML experiments.},
  langid = {english},
  keywords = {analysis,claim:reproducibility,claim:workflowmanagerfeatures,mlops,survey,taxonomy,tool:dvc,tool:mlflow},
  file = {C\:\\Users\\81n\\Zotero\\storage\\NMZG3I74\\Quaranta et al. - A Taxonomy of Tools for Reproducible Machine Learn.pdf}
}

@inproceedings{redyukAutomatedDocumentationEndtoEnd2019,
  title = {Automated {{Documentation}} of {{End-to-End Experiments}} in {{Data Science}}},
  booktitle = {2019 {{IEEE}} 35th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Redyuk, Sergey},
  year = {2019},
  month = apr,
  pages = {2076--2080},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2019.00243},
  abstract = {Reproducibility plays a crucial role in experimentation. However, the modern research ecosystem and the underlying frameworks are constantly evolving and thereby making it extremely difficult to reliably reproduce scientific artifacts such as data, algorithms, trained models and visualizations. We, therefore, aim to design a novel system for assisting data scientists with rigorous end-to-end documentation of data-oriented experiments. Capturing data lineage, metadata, and other artifacts helps reproducing and sharing experimental results. We summarize this challenge as automated documentation of data science experiments. We aim at reducing manual overhead for experimenting researchers, and intend to create a novel approach in dataflow and metadata tracking based on the analysis of the experiment source code. The envisioned system will accelerate the research process in general, and enable capturing fine-grained meta information by deriving a declarative representation of data science experiments.},
  keywords = {automated,automatic tracking of metadata,Biological system modeling,Data science,Documentation,experimentation in data science,Load modeling,Metadata,reproducibility,responsible data management,Task analysis},
  file = {C\:\\Users\\81n\\Zotero\\storage\\3PQXWVRE\\Redyuk - 2019 - Automated Documentation of End-to-End Experiments .pdf;C\:\\Users\\81n\\Zotero\\storage\\FCDS8LR7\\8731587.html}
}

@article{rezigDataCivilizerHolistic2019,
  title = {Data {{Civilizer}} 2.0: A Holistic Framework for Data Preparation and Analytics},
  shorttitle = {Data {{Civilizer}} 2.0},
  author = {Rezig, El Kindi and Cao, Lei and Stonebraker, Michael and Simonini, Giovanni and Tao, Wenbo and Madden, Samuel and Ouzzani, Mourad and Tang, Nan and Elmagarmid, Ahmed K.},
  year = {2019},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {12},
  pages = {1954--1957},
  issn = {2150-8097},
  doi = {10.14778/3352063.3352108},
  abstract = {Data scientists spend over 80\% of their time (1) parameter-tuning machine learning models and (2) iterating between data cleaning and machine learning model execution. While there are existing efforts to support the first requirement, there is currently no integrated workflow system that couples data cleaning and machine learning development. The previous version of Data Civilizer was geared towards data cleaning and discovery using a set of pre-defined tools. In this paper, we introduce Data Civilizer 2.0, an end-to-end workflow system satisfying both requirements. In addition, this system also supports a sophisticated data debugger and a workflow visualization system. In this demo, we will show how we used Data Civilizer 2.0 to help scientists at the Massachusetts General Hospital build their cleaning and machine learning pipeline on their 30TB brain activity dataset.},
  langid = {english},
  keywords = {toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\K6QWICAA\\Rezig et al. - 2019 - Data Civilizer 2.0 a holistic framework for data .pdf}
}

@article{rufDemystifyingMLOpsPresenting2021,
  title = {Demystifying {{MLOps}} and {{Presenting}} a {{Recipe}} for the {{Selection}} of {{Open-Source Tools}}},
  author = {Ruf, Philipp and Madan, Manav and Reich, Christoph and {Ould-Abdeslam}, Djaffar},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {19},
  pages = {8861},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app11198861},
  abstract = {Nowadays, machine learning projects have become more and more relevant to various real-world use cases. The success of complex Neural Network models depends upon many factors, as the requirement for structured and machine learning-centric project development management arises. Due to the multitude of tools available for different operational phases, responsibilities and requirements become more and more unclear. In this work, Machine Learning Operations (MLOps) technologies and tools for every part of the overall project pipeline, as well as involved roles, are examined and clearly defined. With the focus on the inter-connectivity of specific tools and comparison by well-selected requirements of MLOps, model performance, input data, and system quality metrics are briefly discussed. By identifying aspects of machine learning, which can be reused from project to project, open-source tools which help in specific parts of the pipeline, and possible combinations, an overview of support in MLOps is given. Deep learning has revolutionized the field of Image processing, and building an automated machine learning workflow for object detection is of great interest for many organizations. For this, a simple MLOps workflow for object detection with images is portrayed.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {analysis,MlOps,quality metrics,survey,tool,tool comparison,tool:dvc,tool:mlflow,workflow automation},
  file = {C\:\\Users\\81n\\Zotero\\storage\\XSGPY64D\\Ruf et al. - 2021 - Demystifying MLOps and Presenting a Recipe for the.pdf;C\:\\Users\\81n\\Zotero\\storage\\A7XIKDVW\\8861.html}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  year = {2013},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {10},
  pages = {e1003285},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  langid = {english},
  keywords = {Archives,claim:parameterization,claim:workflowmanagerfeatures,Computer and information sciences,Computer applications,Genome analysis,Habits,Replication studies,Reproducibility,Source code,toread},
  file = {C\:\\Users\\81n\\Zotero\\storage\\34ASB74J\\Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf;C\:\\Users\\81n\\Zotero\\storage\\VZZ42RMW\\article.html}
}

@misc{SpotifyLuigi2022,
  title = {Spotify/Luigi},
  year = {2022},
  month = apr,
  abstract = {Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.},
  copyright = {Apache-2.0},
  howpublished = {Spotify},
  keywords = {hadoop,luigi,orchestration-framework,python,scheduling}
}

@misc{stoddenPublishingStandardsComputational2013,
  title = {Publishing {{Standards}} for {{Computational Science}}: ``{{Setting}} the {{Default}} to {{Reproducible}}''},
  shorttitle = {Publishing {{Standards}} for {{Computational Science}}},
  author = {Stodden, Victoria and Borwein, Jonathan and Bailey, David H.},
  year = {2013},
  howpublished = {Pennsylvania State University},
  abstract = {A group of computational scientists has developed a set of standards to guide the dissemination of reproducible research. Computation is now central to the scientific enterprise, and the emergence of powerful computational hardware combined with a vast array of computational software, presents novel opportunities for researchers. Unfortunately the scientific culture surrounding computational work has evolved in ways that make it difficult to verify findings, efficiently build on past research, or even to apply the basic tenets of the scientific method to computational procedures. As a result computational science is facing a credibility crisis [1-4]. The enormous scale of state-of-the-art scientific computations, using tens or hundreds of thousands of processors, presents unprecedented challenges. Numerical reproducibility is a major issue, as is hardware reliability. For some applications, even rare interactions of circuitry with stray subatomic particles matter. In December of 2012, more than 70 computational scientists and stakeholders such as journal editors and funding agency officials gathered at Brown University for the ICERM Workshop on Reproducibility in Computational and Experimental Mathematics. This},
  keywords = {claim:reproducibility},
  file = {C\:\\Users\\81n\\Zotero\\storage\\QMLHUFQA\\Stodden et al. - 2013 - Publishing Standards for Computational Science ‚ÄúS.pdf;C\:\\Users\\81n\\Zotero\\storage\\HJ9KNZQV\\summary.html}
}

@article{storerBridgingChasmSurvey2018,
  title = {Bridging the {{Chasm}}: {{A Survey}} of {{Software Engineering Practice}} in {{Scientific Programming}}},
  shorttitle = {Bridging the {{Chasm}}},
  author = {Storer, Tim},
  year = {2018},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {4},
  pages = {1--32},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3084225},
  abstract = {The use of software is pervasive in all fields of science. Associated software development efforts may be very large, long lived, and complex, requiring the commitment of significant resources. However, several authors have argued that the ``gap'' or ``chasm'' between software engineering and scientific programming is a serious risk to the production of reliable scientific results, as demonstrated in a number of case studies. This article reviews the research that addresses the gap, exploring how both software engineering and research practice may need to evolve to accommodate the use of software in science.},
  langid = {english},
  keywords = {claim:researchgeared,claim:setoml,important,survey},
  file = {C\:\\Users\\81n\\Zotero\\storage\\4MEFQC79\\Storer - 2018 - Bridging the Chasm A Survey of Software Engineeri.pdf}
}

@article{sugimuraBuildingReproducibleMachine2018,
  title = {Building a {{Reproducible Machine Learning Pipeline}}},
  author = {Sugimura, Peter and Hartl, Florian},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.04570 [cs, stat]},
  eprint = {1810.04570},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Reproducibility of modeling is a problem that exists for any machine learning practitioner, whether in industry or academia. The consequences of an irreproducible model can include significant financial costs, lost time, and even loss of personal reputation (if results prove unable to be replicated). This paper will first discuss the problems we have encountered while building a variety of machine learning models, and subsequently describe the framework we built to tackle the problem of model reproducibility. The framework is comprised of four main components (data, feature, scoring, and evaluation layers), which are themselves comprised of well defined transformations. This enables us to not only exactly replicate a model, but also to reuse the transformations across different models. As a result, the platform has dramatically increased the speed of both offline and online experimentation while also ensuring model reproducibility.},
  archiveprefix = {arXiv},
  keywords = {claim:reproducibility,claim:workflowmanagerfeatures,Computer Science - Machine Learning,Computer Science - Software Engineering,Statistics - Machine Learning},
  file = {C\:\\Users\\81n\\Zotero\\storage\\2T3CKSVJ\\Sugimura and Hartl - 2018 - Building a Reproducible Machine Learning Pipeline.pdf;C\:\\Users\\81n\\Zotero\\storage\\4TES92SF\\1810.html}
}

@article{whiteDataDepsJlRepeatable2019a,
  title = {{{DataDeps}}.Jl: {{Repeatable Data Setup}} for {{Reproducible Data Science}}},
  shorttitle = {{{DataDeps}}.Jl},
  author = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
  year = {2019},
  month = oct,
  journal = {Journal of Open Research Software},
  volume = {7},
  number = {1},
  pages = {33},
  publisher = {{Ubiquity Press}},
  issn = {2049-9647},
  doi = {10.5334/jors.244},
  abstract = {Article: DataDeps.jl: Repeatable Data Setup for Reproducible Data Science},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {tool},
  file = {C\:\\Users\\81n\\Zotero\\storage\\6GF854TR\\White et al. - 2019 - DataDeps.jl Repeatable Data Setup for Reproducibl.pdf}
}

@article{wilkinsonFAIRGuidingPrinciples2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = mar,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\textemdash representing academia, industry, funding agencies, and scholarly publishers\textemdash have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {foundational,Publication characteristics,Research data},
  file = {C\:\\Users\\81n\\Zotero\\storage\\LDVGCBIQ\\Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf;C\:\\Users\\81n\\Zotero\\storage\\2WW2XPRR\\sdata201618.html}
}

@article{wrattenReproducibleScalableShareable2021,
  title = {Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers},
  author = {Wratten, Laura and Wilm, Andreas and G{\"o}ke, Jonathan},
  year = {2021},
  month = oct,
  journal = {Nature Methods},
  volume = {18},
  number = {10},
  pages = {1161--1168},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01254-9},
  abstract = {The rapid growth of high-throughput technologies has transformed biomedical research. With the increasing amount and complexity of data, scalability and reproducibility have become essential not just for experiments, but also for computational analysis. However, transforming data into information involves running a large number of tools, optimizing parameters, and integrating dynamically changing reference data. Workflow managers were developed in response to such challenges. They simplify pipeline development, optimize resource usage, handle software installation and versions, and run on different compute platforms, enabling workflow portability and sharing. In this Perspective, we highlight key features of workflow managers, compare commonly used approaches for bioinformatics workflows, and provide a guide for computational and noncomputational users. We outline community-curated pipeline initiatives that enable novice and experienced users to perform complex, best-practice analyses without having to manually assemble workflows. In sum, we illustrate how workflow managers contribute to making computational analysis in biomedical research shareable, scalable, and reproducible.},
  copyright = {2021 Springer Nature America, Inc.},
  langid = {english},
  keywords = {analysis,claim:workflowmanagerfeatures,Computational platforms and environments,important,Programming language,Software,taxonomy,tool:luigi,tool:pachyderm},
  file = {C\:\\Users\\81n\\Zotero\\storage\\AD6774BK\\Wratten et al. - 2021 - Reproducible, scalable, and shareable analysis pip.pdf;C\:\\Users\\81n\\Zotero\\storage\\JSYW49FW\\s41592-021-01254-9.html}
}
