# Feel free to delete these first few references, which are specific to the template:

@article{Hansen2021-fd,
  title   = {{CiliaQ}: a simple, open-source software for automated
             quantification of ciliary morphology and fluorescence in 2D, 3D,
             and {4D} images},
  author  = {Hansen, Jan Niklas and Rassmann, Sebastian and St{\"u}ven, Birthe
             and Jurisch-Yaksi, Nathalie and Wachten, Dagmar},
  journal = {The European Physical Journal E},
  volume  = 44,
  number  = 2,
  pages   = {18},
  month   = mar,
  year    = 2021,
  doi     = {https://doi.org/10.1140/epje/s10189-021-00031-y}
}

@article{lee2011muco,
  title     = {Muco-ciliary transport: effect of mucus viscosity, cilia beat frequency and cilia density},
  author    = {Lee, WL and Jayathilake, PG and Tan, Zhijun and Le, DV and Lee, HP and Khoo, BC},
  journal   = {Computers \& Fluids},
  volume    = {49},
  number    = {1},
  pages     = {214--221},
  year      = {2011},
  publisher = {Elsevier},
  doi       = {https://doi.org/10.1016/j.compfluid.2011.05.016}
}


@article{doretto2003dynamic,
  title     = {Dynamic textures},
  author    = {Doretto, Gianfranco and Chiuso, Alessandro and Wu, Ying Nian and Soatto, Stefano},
  journal   = {International journal of computer vision},
  volume    = {51},
  pages     = {91--109},
  year      = {2003},
  publisher = {Springer},
  doi       = {https://doi.org/10.1023/A:1021669406132}
}


@inproceedings{Hyndman2007HigherorderAM,
  title     = {Higher-order Autoregressive Models for Dynamic Textures},
  author    = {Midori Hyndman and Allan D. Jepson and David J. Fleet},
  booktitle = {British Machine Vision Conference},
  year      = {2007},
  url       = {https://api.semanticscholar.org/CorpusID:15060494},
  doi       = {https://doi.org/10.5244/C.21.76}
}

@misc{Iakubovskii:2019,
  author       = {Pavel Iakubovskii},
  title        = {Segmentation Models Pytorch},
  year         = {2019},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/qubvel/segmentation_models.pytorch}}
}


@inproceedings{kirillov2017unified,
  title        = {A unified architecture for instance and semantic segmentation},
  author       = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle    = {Computer Vision and Pattern Recognition Conference},
  year         = {2017},
  organization = {CVPR},
  doi          = {https://doi.org/10.48550/arXiv.2112.04603}
}
@inproceedings{vaezi2022novel,
  title     = {A Novel Pipeline for Cell Instance Segmentation, Tracking and Motility Classification of Toxoplasma Gondii in 3D Space.},
  author    = {Vaezi, Seyed Alireza and Orlando, Gianni and Fazli, Mojtaba Sedigh and Ward, Gary E and Moreno, Silvia NJ and Quinn, Shannon},
  booktitle = {SciPy},
  pages     = {60--63},
  year      = {2022},
  doi       = {https://doi.org/10.25080/majora-212e5952-009}
}


@inproceedings{zain2022low,
  title     = {Low Level Feature Extraction for Cilia Segmentation},
  author    = {Zain, Meekail and Miller, Eric and Quinn, Shannon and Lo, Cecilia},
  booktitle = {Proceedings of the Python in Science Conference},
  year      = {2022},
  doi       = {http://dx.doi.org/10.25080/majora-212e5952-026}
}

@inproceedings{zain2020towards,
  title     = {Towards an unsupervised spatiotemporal representation of cilia video using a modular generative pipeline},
  author    = {Zain, Meekail and Rao, Sonia and Safir, Nathan and Wyner, Quinn and Humphrey, Isabella and Eldridge, Alex and Li, Chenxiao and AlAila, BahaaEddin and Quinn, Shannon},
  booktitle = {Proceedings of the Python in Science Conference},
  year      = {2020},
  doi       = {https://doi.org/10.25080/majora-342d178e-017}
}

@article{quinn2015automated,
  title     = {Automated identification of abnormal respiratory ciliary motion in nasal biopsies},
  author    = {Quinn, Shannon P and Zahid, Maliha J and Durkin, John R and Francis, Richard J and Lo, Cecilia W and Chennubhotla, S Chakra},
  journal   = {Science translational medicine},
  volume    = {7},
  number    = {299},
  pages     = {299ra124--299ra124},
  year      = {2015},
  publisher = {American Association for the Advancement of Science},
  doi       = {https://doi.org/10.1126/scitranslmed.aaa1233}
}

@article{YAKIMOVICH2021100383,
  title    = {Labels in a haystack: Approaches beyond supervised learning in biomedical applications},
  journal  = {Patterns},
  volume   = {2},
  number   = {12},
  pages    = {100383},
  year     = {2021},
  issn     = {2666-3899},
  doi      = {https://doi.org/10.1016/j.patter.2021.100383},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666389921002506},
  author   = {Artur Yakimovich and Anaël Beaugnon and Yi Huang and Elif Ozkirimli},
  keywords = {machine learning, data labeling, data value, active learning, self-supervised learning, semi-supervised learning, data annotation, zero-shot learning},
  abstract = {Summary
              Recent advances in biomedical machine learning demonstrate great potential for data-driven techniques in health care and biomedical research. However, this potential has thus far been hampered by both the scarcity of annotated data in the biomedical domain and the diversity of the domain's subfields. While unsupervised learning is capable of finding unknown patterns in the data by design, supervised learning requires human annotation to achieve the desired performance through training. With the latter performing vastly better than the former, the need for annotated datasets is high, but they are costly and laborious to obtain. This review explores a family of approaches existing between the supervised and the unsupervised problem setting. The goal of these algorithms is to make more efficient use of the available labeled data. The advantages and limitations of each approach are addressed and perspectives are provided.}
}

@article{van2020survey,
  title     = {A survey on semi-supervised learning},
  author    = {Van Engelen, Jesper E and Hoos, Holger H},
  journal   = {Machine learning},
  volume    = {109},
  number    = {2},
  pages     = {373--440},
  year      = {2020},
  publisher = {Springer},
  doi       = {https://doi.org/10.1007/s10994-019-05855-6}
}
@article{settles2009active,
  title     = {Active learning literature survey},
  author    = {Settles, Burr},
  year      = {2009},
  publisher = {University of Wisconsin-Madison Department of Computer Sciences}
}
@article{10.3389/fcvm.2020.00105,
  author   = {Chen, Chen and Bai, Wenjia and Davies, Rhodri H. and Bhuva, Anish N. and Manisty, Charlotte H. and Augusto, Joao B. and Moon, James C and Aung, Nay and Lee, Aaron M. and Sanghvi, Mihir M. and Fung, Kenneth and Paiva, Jose Miguel and Petersen, Steffen E. and Lukaschuk, Elena and Piechnik, Stefan K. and Neubauer, Stefan and Rueckert, Daniel},
  title    = {Improving the Generalizability of Convolutional Neural Network-Based Segmentation on CMR Images},
  journal  = {Frontiers in Cardiovascular Medicine},
  volume   = {7},
  year     = {2020},
  url      = {https://www.frontiersin.org/articles/10.3389/fcvm.2020.00105},
  doi      = {10.3389/fcvm.2020.00105},
  issn     = {2297-055X},
  abstract = {Background: Convolutional neural network (CNN) based segmentation methods provide an efficient and automated way for clinicians to assess the structure and function of the heart in cardiac MR images. While CNNs can generally perform the segmentation tasks with high accuracy when training and test images come from the same domain (e.g., same scanner or site), their performance often degrades dramatically on images from different scanners or clinical sites.Methods: We propose a simple yet effective way for improving the network generalization ability by carefully designing data normalization and augmentation strategies to accommodate common scenarios in multi-site, multi-scanner clinical imaging data sets. We demonstrate that a neural network trained on a single-site single-scanner dataset from the UK Biobank can be successfully applied to segmenting cardiac MR images across different sites and different scanners without substantial loss of accuracy. Specifically, the method was trained on a large set of 3,975 subjects from the UK Biobank. It was then directly tested on 600 different subjects from the UK Biobank for intra-domain testing and two other sets for cross-domain testing: the ACDC dataset (100 subjects, 1 site, 2 scanners) and the BSCMR-AS dataset (599 subjects, 6 sites, 9 scanners).Results: The proposed method produces promising segmentation results on the UK Biobank test set which are comparable to previously reported values in the literature, while also performing well on cross-domain test sets, achieving a mean Dice metric of 0.90 for the left ventricle, 0.81 for the myocardium, and 0.82 for the right ventricle on the ACDC dataset; and 0.89 for the left ventricle, 0.83 for the myocardium on the BSCMR-AS dataset.Conclusions: The proposed method offers a potential solution to improve CNN-based model generalizability for the cross-scanner and cross-site cardiac MR image segmentation task.}
}
@article{Krois2021,
  author   = {Krois, Joachim
              and Garcia Cantu, Anselmo
              and Chaurasia, Akhilanand
              and Patil, Ranjitkumar
              and Chaudhari, Prabhat Kumar
              and Gaudin, Robert
              and Gehrung, Sascha
              and Schwendicke, Falk},
  title    = {Generalizability of deep learning models for dental image analysis},
  journal  = {Scientific Reports},
  year     = {2021},
  month    = {Mar},
  day      = {17},
  volume   = {11},
  number   = {1},
  pages    = {6102},
  abstract = {We assessed the generalizability of deep learning models and how to improve it. Our exemplary use-case was the detection of apical lesions on panoramic radiographs. We employed two datasets of panoramic radiographs from two centers, one in Germany (Charit{\'e}, Berlin, n{\thinspace}={\thinspace}650) and one in India (KGMU, Lucknow, n{\thinspace}={\thinspace}650): First, U-Net type models were trained on images from Charit{\'e} (n{\thinspace}={\thinspace}500) and assessed on test sets from Charit{\'e} and KGMU (each n{\thinspace}={\thinspace}150). Second, the relevance of image characteristics was explored using pixel-value transformations, aligning the image characteristics in the datasets. Third, cross-center training effects on generalizability were evaluated by stepwise replacing Charite with KGMU images. Last, we assessed the impact of the dental status (presence of root-canal fillings or restorations). Models trained only on Charit{\'e} images showed a (mean{\thinspace}{\textpm}{\thinspace}SD) F1-score of 54.1{\thinspace}{\textpm}{\thinspace}0.8{\%} on Charit{\'e} and 32.7{\thinspace}{\textpm}{\thinspace}0.8{\%} on KGMU data (p{\thinspace}<{\thinspace}0.001/t-test). Alignment of image data characteristics between the centers did not improve generalizability. However, by gradually increasing the fraction of KGMU images in the training set (from 0 to 100{\%}) the F1-score on KGMU images improved (46.1{\thinspace}{\textpm}{\thinspace}0.9{\%}) at a moderate decrease on Charit{\'e} images (50.9{\thinspace}{\textpm}{\thinspace}0.9{\%}, p{\thinspace}<{\thinspace}0.01). Model performance was good on KGMU images showing root-canal fillings and/or restorations, but much lower on KGMU images without root-canal fillings and/or restorations. Our deep learning models were not generalizable across centers. Cross-center training improved generalizability. Noteworthy, the dental status, but not image characteristics were relevant. Understanding the reasons behind limits in generalizability helps to mitigate generalizability problems.},
  issn     = {2045-2322},
  doi      = {10.1038/s41598-021-85454-5},
  url      = {https://doi.org/10.1038/s41598-021-85454-5}
}
@article{doi:10.1148/ryai.2020190195,
  author  = {Yan, Wenjun and Huang, Lu and Xia, Liming and Gu, Shengjia and Yan, Fuhua and Wang, Yuanyuan and Tao, Qian},
  title   = {MRI Manufacturer Shift and Adaptation: Increasing the                     Generalizability of Deep Learning Segmentation for MR Images Acquired with                     Different Scanners},
  journal = {Radiology: Artificial Intelligence},
  volume  = {2},
  number  = {4},
  pages   = {e190195},
  year    = {2020},
  doi     = {10.1148/ryai.2020190195},
  note    = {PMID: 33937833},
  url     = { 
             
             https://doi.org/10.1148/ryai.2020190195
             
             
             
             },
  eprint  = { 
             
             https://doi.org/10.1148/ryai.2020190195
             
             
             
             }
}
@article{Sandfort2019,
  author   = {Sandfort, Veit
              and Yan, Ke
              and Pickhardt, Perry J.
              and Summers, Ronald M.},
  title    = {Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks},
  journal  = {Scientific Reports},
  year     = {2019},
  month    = {Nov},
  day      = {15},
  volume   = {9},
  number   = {1},
  pages    = {16884},
  abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p{\thinspace}<{\thinspace}0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p{\thinspace}<{\thinspace}0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
  issn     = {2045-2322},
  doi      = {10.1038/s41598-019-52737-x},
  url      = {https://doi.org/10.1038/s41598-019-52737-x}
}
@article{YAKIMOVICH2021100383,
  title    = {Labels in a haystack: Approaches beyond supervised learning in biomedical applications},
  journal  = {Patterns},
  volume   = {2},
  number   = {12},
  pages    = {100383},
  year     = {2021},
  issn     = {2666-3899},
  doi      = {https://doi.org/10.1016/j.patter.2021.100383},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666389921002506},
  author   = {Artur Yakimovich and Anaël Beaugnon and Yi Huang and Elif Ozkirimli},
  keywords = {machine learning, data labeling, data value, active learning, self-supervised learning, semi-supervised learning, data annotation, zero-shot learning},
  abstract = {Summary
              Recent advances in biomedical machine learning demonstrate great potential for data-driven techniques in health care and biomedical research. However, this potential has thus far been hampered by both the scarcity of annotated data in the biomedical domain and the diversity of the domain's subfields. While unsupervised learning is capable of finding unknown patterns in the data by design, supervised learning requires human annotation to achieve the desired performance through training. With the latter performing vastly better than the former, the need for annotated datasets is high, but they are costly and laborious to obtain. This review explores a family of approaches existing between the supervised and the unsupervised problem setting. The goal of these algorithms is to make more efficient use of the available labeled data. The advantages and limitations of each approach are addressed and perspectives are provided.}
}

@article{van2001art,
  title     = {The art of data augmentation},
  author    = {Van Dyk, David A and Meng, Xiao-Li},
  journal   = {Journal of Computational and Graphical Statistics},
  volume    = {10},
  number    = {1},
  pages     = {1--50},
  year      = {2001},
  publisher = {Taylor \& Francis},
  doi       = {https://doi.org/10.1198/10618600152418584}
}

@article{krizhevsky2012imagenet,
  title   = {Imagenet classification with deep convolutional neural networks},
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal = {Advances in neural information processing systems},
  volume  = {25},
  year    = {2012},
  doi     = {https://doi.org/10.1145/3065386}
}
@inproceedings{ronneberger2015u,
  title        = {U-net: Convolutional networks for biomedical image segmentation},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle    = {Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  pages        = {234--241},
  year         = {2015},
  organization = {Springer},
  doi          = {https://doi.org/10.48550/arXiv.1505.04597}
}
@article{goodfellow2014generative,
  title   = {Generative adversarial nets},
  author  = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal = {Advances in neural information processing systems},
  volume  = {27},
  year    = {2014},
  doi     = {https://doi.org/10.48550/arXiv.1406.2661}
}
@article{yi2019generative,
  title     = {Generative adversarial network in medical imaging: A review},
  author    = {Yi, Xin and Walia, Ekta and Babyn, Paul},
  journal   = {Medical image analysis},
  volume    = {58},
  pages     = {101552},
  year      = {2019},
  publisher = {Elsevier},
  doi       = {https://doi.org/10.48550/arXiv.1809.07294}
}
@article{Sanford2020-yg,
  title    = {Data Augmentation and Transfer Learning to Improve
              Generalizability of an Automated Prostate Segmentation Model},
  author   = {Sanford, Thomas H and Zhang, Ling and Harmon, Stephanie A and
              Sackett, Jonathan and Yang, Dong and Roth, Holger and Xu, Ziyue
              and Kesani, Deepak and Mehralivand, Sherif and Baroni, Ronaldo H
              and Barrett, Tristan and Girometti, Rossano and Oto, Aytekin and
              Purysko, Andrei S and Xu, Sheng and Pinto, Peter A and Xu,
              Daguang and Wood, Bradford J and Choyke, Peter L and Turkbey,
              Baris},
  journal  = {AJR Am J Roentgenol},
  volume   = 215,
  number   = 6,
  pages    = {1403--1410},
  month    = oct,
  year     = 2020,
  address  = {United States},
  keywords = {artificial intelligence; prostate MRI; segmentation},
  language = {en},
  doi      = {https://doi.org/10.2214/ajr.19.22347}
}

@inproceedings{NEURIPS2019_eb1e7832,
  author    = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Transfusion: Understanding Transfer Learning for Medical Imaging},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
  volume    = {32},
  year      = {2019},
  doi       = {https://doi.org/10.48550/arXiv.1902.07208}
}

@misc{hutchinson2017overcoming,
  title         = {Overcoming data scarcity with transfer learning},
  author        = {Maxwell L. Hutchinson and Erin Antono and Brenna M. Gibbons and Sean Paradiso and Julia Ling and Bryce Meredig},
  year          = {2017},
  eprint        = {1711.05099},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  doi           = {https://doi.org/10.48550/arXiv.1711.05099}
}
@inproceedings{kim2019self,
  title     = {Self-supervised video representation learning with space-time cubic puzzles},
  author    = {Kim, Dahun and Cho, Donghyeon and Kweon, In So},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {33},
  number    = {01},
  pages     = {8545--8552},
  year      = {2019},
  doi       = {https://doi.org/10.48550/arXiv.1811.09795}
}

@inproceedings{kolesnikov2019revisiting,
  title     = {Revisiting self-supervised visual representation learning},
  author    = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {1920--1929},
  year      = {2019},
  doi       = {https://doi.org/10.48550/arXiv.1901.09005}
}

@inproceedings{mahendran2019cross,
  title        = {Cross pixel optical-flow similarity for self-supervised learning},
  author       = {Mahendran, Aravindh and Thewlis, James and Vedaldi, Andrea},
  booktitle    = {Computer Vision--ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected Papers, Part V 14},
  pages        = {99--116},
  year         = {2019},
  organization = {Springer},
  doi          = {https://doi.org/10.48550/arXiv.1807.05636}
}
@article{li2006one,
  title   = {One-shot learning of object categories},
  author  = {Li, Fei-Fei and Fergus, Rob and Perona, Pietro and others},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell},
  volume  = {28},
  number  = {4},
  pages   = {594--611},
  year    = {2006},
  doi     = {https://doi.org/10.1109/TPAMI.2006.79}
}
@inproceedings{miller2000learning,
  title        = {Learning from one example through shared densities on transforms},
  author       = {Miller, Erik G and Matsakis, Nicholas E and Viola, Paul A},
  booktitle    = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)},
  volume       = {1},
  pages        = {464--471},
  year         = {2000},
  organization = {IEEE},
  doi          = {https://doi.org/10.1109/CVPR.2000.855856}
}
@article{khatibi2021proposing,
  title     = {Proposing a novel unsupervised stack ensemble of deep and conventional image segmentation (SEDCIS) method for localizing vitiligo lesions in skin images},
  author    = {Khatibi, Toktam and Rezaei, Niloofar and Ataei Fashtami, Leila and Totonchi, Mehdi},
  journal   = {Skin Research and Technology},
  volume    = {27},
  number    = {2},
  pages     = {126--137},
  year      = {2021},
  publisher = {Wiley Online Library},
  doi       = {https://doi.org/10.1111/srt.12920}
}