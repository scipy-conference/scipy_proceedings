@phdthesis{jit4opencl,
abstract = {Heterogeneous computing platforms that use GPUs and CPUs in tandem for computation have become an important choice to build low-cost high-performance computing platforms. The computing ability of modern GPUs surpasses that of CPUs can offer for certain classes of applications. GPUs can deliver several Tera-Flops in peak performance. However, programmers must adopt a more complicated and more difficult new programming paradigm. To alleviate the burden of programming for heterogeneous systems, Garg and Amaral developed a Python compiling framework that combines an ahead-of-time compiler called unPython with a just-in-time compiler called jit4GPU. This compilation framework generates code for systems with AMD GPUs. We extend the framework to retarget it to generate OpenCL code, an industry standard that is implemented for most GPUs. Therefore, by generating OpenCL code, this new compiler, called jit4OpenCL, enables the execution of the same program in a wider selection of heterogeneous platforms. To further improve the target-code performance on nVidia GPUs, we developed an array-access analysis tool that helps to exploit the data reusability by utilizing the shared (local) memory space hierarchy in OpenCL. The thesis presents an experimental performance evaluation indicating that, in comparison with jit4GPU, jit4OpenCL has performance degradation because of the current performance of implementations of OpenCL, and also because of the extra time needed for the additional just-in-time compilation. However, the portable code generated by jit4OpenCL still have performance gains in some applications compared to highly optimized CPU code.},
author = {Aharon-Shalom, E and Heller, A},
booktitle = {Computer Engineering},
pages = {2865},
title = {{Jit4OpenCL: a compiler from Python to OpenCL}},
url = {http://repository.library.ualberta.ca/dspace/handle/10048/1551},
volume = {129},
year = {2010}
}
@inproceedings{sciby,
author = {Andersen, Rasmus and Vinter, Brian},
booktitle = {Proceedings of the 2008 International Conference on Grid Computing \& Applications, GCA 2008 : Las Vegas, Nevada, USA, July 14-17, 2008. CSREA Press.},
file = {:home/safl/Documents/Mendeley/Andersen, Vinter - 2008 - The Scientific Byte Code Virtual Machine.pdf:pdf},
pages = {175--181},
title = {{The Scientific Byte Code Virtual Machine}},
url = {http://dk.migrid.org/public/doc/published\_papers/sbc.pdf},
year = {2008}
}
@article{numpy_2001,
abstract = {"Python is a small and easy-to-learn language with surprising capabilities. It is an interpreted object-oriented scripting language and has a full range of sophisticated features such as first-class functions, garbage collection, and exception handling. Python has properties that make it especially appealing for scientific programming: Python is quite simple and easy to learn, but it is a full and complete language. It is simple to extend Python with your own compiled objects and functions. Python is portable, from Unix to Windows 95 to Linux to Macintosh. Python is free, with no license required even if you make a commercial product out of it. Python has a large user-contributed library of ``modules''. These modules cover a wide variety of needs, such as audio and image processing, World Wide Web programming, and graphical user interfaces. In particular, there is an interface to the popular Tk package for building windowing applications. And now, Python has a high-performance array module similar to the facilities in specialized array languages such as Matlab, IDL, Basis, or Yorick. This extension also adds complex numbers to the language. Array operations in Python lead to the execution of loops in C, so that most of the work is done at full compiled speed."},
author = {Ascher, David and Dubois, Paul F},
doi = {10.2307/2007860},
institution = {Lawrence Livermore National Laboratory, Livermore, CA},
issn = {08941866},
journal = {Computers in Physics},
number = {3},
pages = {262--267},
publisher = {Citeseer},
title = {{Numerical Python}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.5683\&amp;rep=rep1\&amp;type=pdf},
volume = {10},
year = {2001}
}
@article{Backus78,
author = {Backus, J},
journal = {Communications of the ACM},
number = {8},
pages = {613--641},
title = {{Can Programming be Liberated from the von Neumann Style?: A Functional Style and its Algebra of Programs}},
volume = {16},
year = {1978}
}
@misc{swater,
author = {Burkardt, John},
title = {{Shallow Water Equations}},
url = {http://people.sc.fsu.edu/~jburkardt/m\_src/shallow\_water\_2d/},
year = {2010}
}
@inproceedings{sejits,
abstract = {Todays high productivity programming languages such as Python lack the performance of harder-to-program efficiency languages (CUDA, Cilk, C with OpenMP) that can exploit extensive programmer knowledge of parallel hardware architectures. We combine efficiency-language performance with productivity-language programmability using selective embedded just-in-time specialization (SEJITS). At runtime, we specialize (generate, compile, and execute efficiency-language source code for) an application-specific and platform-specific subset of a productivity language, largely invisibly to the application programmer. Because the specialization machinery is implemented in the productivity language itself, it is easy for efficiency programmers to incrementally add specializers for new domain abstractions, new hardware, or both. SEJITS has the potential to bridge productivity-layer research and efficiency-layer research, allowing domain experts to exploit different parallel hardware architectures with a fraction of the programmer time and effort usually required.},
author = {Catanzaro, Bryan and Kamil, Shoaib and Lee, Yunsup and Asanovi\'{c}, Krste and Demmel, James and Keutzer, Kurt and Shalf, John and Yelick, Kathy and Fox, O},
booktitle = {Proc of 1st Workshop Programmable Models for Emerging Architecture PMEA},
number = {UCB/EECS-2010-23},
organization = {EECS Department, University of California, Berkeley},
pmid = {8401072916273492945},
publisher = {Citeseer},
title = {{SEJITS: Getting Productivity and Performance With Selective Embedded JIT Specialization}},
url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-23.html},
year = {2009}
}
@article{ms:accelerator,
author = {David, Tarditi and Sidd, Puri and Jose, Oglesby},
file = {:home/safl/Documents/Mendeley/tr-2005-184 (2).pdf:pdf},
journal = {October},
keywords = {data parallelism,graphics processing units,just-in-},
title = {{Accelerator : Using Data Parallelism to Program GPUs for General-Purpose Uses}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=70250}
}
@article{lang:octave,
abstract = {The electrical conductivity of nanoporous gold was measured in situ during the charging and decharging of the surface of the metal. The nanoporous gold samples were prepared by the process of selective dealloying of Ag from Au-Ag alloy. Charge was induced on the surface by making the sample a working electrode in an electrochemical cell. The conductivity was observed to vary reversibly with the induced surface charge.(C) 2008 American Institute of Physics.},
annote = {
        From Duplicate 2 ( 
        
          GNU Octave
        
         - Eaton, John W )

        
        

        

        

      },
author = {Eaton, John W},
file = {:home/safl/Documents/Mendeley/Eaton - 1997 - GNU Octave.pdf:pdf},
issn = {00218979},
journal = {History},
number = {February},
pages = {1--356},
publisher = {Network Theory Ltd.},
title = {{GNU Octave}},
url = {http://www.octave.org},
volume = {103},
year = {1997}
}
@article{simd,
author = {Flynn, Michael .J.},
file = {:home/safl/Documents/Mendeley/5\_flynn.pdf:pdf},
journal = {Proceedings of the IEEE},
number = {12},
pages = {1901--1909},
publisher = {IEEE},
title = {{Very high-speed computing systems}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1447203},
volume = {54},
year = {1966}
}
@article{unpython,
abstract = {A new compilation framework enables the execution of numerical-intensive applications, written in Python, on a hybrid execution environment formed by a CPU and a GPU. This compiler automatically computes the set of memory locations that need to be transferred to the GPU, and produces the correct mapping between the CPU and the GPU address spaces. Thus, the programming model implements a virtual shared address space. This framework is implemented as a combination of unPython, an ahead-of-time compiler from Python/NumPy to the C programming language, and jit4GPU, a just-in-time compiler from C to the AMD CAL interface. Experimental evaluation demonstrates that for some benchmarks the generated GPU code is 50 times faster than generated OpenMP code. The GPU performance also compares favorably with optimized CPU BLAS code for single-precision computations in most cases.},
annote = {
        From Duplicate 1 ( 
        
        
          Compiling Python to a hybrid execution environment
        
        
         - Garg, Rahul; Amaral, Jos\'{e} Nelson )

        
        

        

        

      },
author = {Garg, Rahul and Amaral, Jos\'{e} Nelson},
doi = {10.1145/1735688.1735695},
file = {:home/safl/Documents/Mendeley/Garg, Amaral - 2010 - Compiling Python to a hybrid execution environment.pdf:pdf},
isbn = {9781605589350},
journal = {Computing},
pages = {19--30},
publisher = {ACM Press},
title = {{Compiling Python to a hybrid execution environment}},
url = {http://portal.acm.org/citation.cfm?id=1735688.1735695},
year = {2010}
}
@misc{intel:mkl,
author = {Intel},
booktitle = {Transform},
pages = {2--4},
title = {{Intel Math Kernel Library (MKL)}},
url = {http://software.intel.com/en-us/articles/intel-mkl/},
year = {2008}
}
@article{py:cuda:opencl,
abstract = {High-performance computing has recently seen a surge of interest in heterogeneous systems, with an emphasis on modern Graphics Processing Units (GPUs). These devices offer tremendous potential for performance and efficiency in important large-scale applications of computational science. However, exploiting this potential can be challenging, as one must adapt to the specialized and rapidly evolving computing environment currently exhibited by GPUs. One way of addressing this challenge is to embrace better techniques and develop tools tailored to their needs. This article presents one simple technique, GPU run-time code generation (RTCG), along with PyCUDA and PyOpenCL, two open-source toolkits that support this technique. In introducing PyCUDA and PyOpenCL, this article proposes the combination of a dynamic, high-level scripting language with the massive performance of a GPU as a compelling two-tiered computing platform, potentially offering significant performance and productivity advantages over conventional single-tier, static systems. The concept of RTCG is simple and easily implemented using existing, robust infrastructure. Nonetheless it is powerful enough to support (and encourage) the creation of custom application-specific tools by its users. The premise of the paper is illustrated by a wide range of examples where the technique has been applied with considerable success.},
author = {Kl\"{o}ckner, Andreas and Pinto, Nicolas and Lee, Yunsup and Catanzaro, Bryan and Ivanov, Paul and Fasih, Ahmed},
file = {:home/safl/Documents/Mendeley/Kl\"{o}ckner et al. - 2009 - PyCUDA and PyOpenCL A Scripting-Based Approach to GPU Run-Time Code Generation.pdf:pdf},
journal = {Brain},
keywords = {automated tuning,code generation,gpu,high level,languages,many core,massive parallelism,single instruction multiple data,software engineering},
number = {4},
pages = {1--24},
publisher = {Elsevier B.V.},
title = {{PyCUDA and PyOpenCL: A Scripting-Based Approach to GPU Run-Time Code Generation}},
url = {http://arxiv.org/abs/0911.3456},
volume = {911},
year = {2009}
}
@inproceedings{kristensen10_dnumpy,
author = {Kristensen, Mads R B and Vinter, Brian},
booktitle = {Fourth Conference on Partitioned Global Address Space Programming Model, PGAS\{'\}10},
file = {:home/safl/Documents/Mendeley/Unknown - 2010 - Numerical Python for scalable architectures.pdf:pdf},
isbn = {978-1-4503-0461-0},
publisher = {ACM},
title = {{Numerical Python for Scalable Architectures}},
url = {http://distnumpy.googlecode.com/files/kristensen10.pdf},
year = {2010}
}
@article{Kristensen09,
address = {Los Alamitos, CA, USA},
author = {Kristensen, Mads Ruben Burgdorff and Happe, Hans Henrik and Vinter, Brian},
doi = {10.1109/IPDPS.2009.5160936},
isbn = {978-1-4244-3751-1},
journal = {Parallel and Distributed Processing Symposium, International},
publisher = {IEEE Computer Society},
title = {{GPAW optimized for Blue Gene/P using hybrid programming}},
year = {2009}
}
@article{kristensen2011gpaw,
author = {Kristensen, M and Happe, H and Vinter, B},
issn = {1895-1767},
journal = {Scalable Computing: Practice and Experience},
number = {2},
pages = {265--274},
title = {{Hybrid Parallel Programming for Blue Gene/P}},
volume = {12},
year = {2011}
}
@misc{Latt06_lbm2d,
author = {Latt, Jonas},
title = {{Channel flow past a cylinderical obstacle, using a LB method}},
url = {http://www.lbmethod.org/openlb/downloads/cylinder.m},
year = {2006}
}
@book{lang:matlab,
address = {Natick, Massachusetts},
author = {MATLAB},
publisher = {The MathWorks Inc.},
title = {{version 7.10.0 (R2010a)}},
year = {2010}
}
@article{intel:arraybb,
author = {Newburn, Chris J and So, Byoungro and Liu, Zhenying and Mccool, Michael and Ghuloum, Anwar and Toit, Stefanus Du and Wang, Zhi Gang and Du, Zhao Hui and Chen, Yongjian and Wu, Gansha and Guo, Peng and Liu, Zhanglin and Zhang, Dan},
journal = {Symposium A Quarterly Journal In Modern Foreign Literatures},
pages = {1--12},
publisher = {IEEE},
title = {{Intel â€™ s Array Building Blocks : A Retargetable , Dynamic Compiler and Embedded Language}},
url = {http://software.intel.com/en-us/blogs/wordpress/wp-content/uploads/2011/03/ArBB-CGO2011-distr.pdf},
year = {2011}
}
@misc{cuda,
author = {Nvidia, Null},
booktitle = {NVIDIA Corporation},
file = {:home/safl/Documents/Mendeley/Nvidia - 2010 - NVIDIA CUDA Programming Guide 2.0.pdf:pdf},
institution = {Nvidia},
number = {2.3.1},
pages = {1--111},
publisher = {cuda},
title = {{NVIDIA CUDA Programming Guide 2.0}},
url = {http://developer.download.nvidia.com/compute/cuda/3\_2\_prod/toolkit/docs/CUDA\_C\_Programming\_Guide.pdf},
volume = {Version 3.},
year = {2010}
}
@article{numpy,
abstract = {By itself, Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
author = {Oliphant, Travis E},
doi = {10.1109/MCSE.2007.58},
institution = {Enthought},
issn = {15219615},
journal = {Computing in Science Engineering},
number = {3},
pages = {10--20},
publisher = {IEEE Computer Society},
title = {{Python for Scientific Computing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4160250},
volume = {9},
year = {2007}
}
@article{opencl,
author = {Opencl, Khronos and Group, Working and Munshi, Aaftab},
editor = {Munshi, Aaftab},
journal = {ReVision},
pages = {1--377},
publisher = {Khronos OpenCL Working Group},
title = {{OpenCL Specification}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:OpenCL+Specification\#2},
year = {2010}
}
@article{openmp,
author = {Pas, Ruud Van Der},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
number = {5},
pages = {1--82},
publisher = {ACM},
title = {{An Introduction Into OpenMP}},
url = {http://portal.acm.org/citation.cfm?id=1168898},
volume = {34},
year = {2005}
}
@article{mpi,
abstract = {Process naming to allow libraries to describe their communication in terms suitable to their own data structures and algorithms, ffl The ability to "adorn" a set of communicating processes with additional user-defined attributes, such as extra collective operations. This mechanism should provide a means for the user or library writer effectively to extend a message-passing notation. In addition, a unified mechanism or object is needed for conveniently denoting communication context, the group of communicating processes, to house abstract process naming, and to store adornments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 5.1. INTRODUCTION 133 5.1.2 MPI's Support for Libraries The corresponding concepts that MPI provides, specifically to support robust libraries, are as follows: ffl Contexts of communication, ffl Groups of processes, ffl Virtual topologies, ffl Attribute caching, ffl Commun...},
author = {Passing, Message and Forum, Interface},
file = {:home/safl/Documents/Mendeley/Passing, Forum - 2009 - MPI A Message-Passing Interface Standard.pdf:pdf},
institution = {Message Passing Interface Forum},
journal = {Forum American Bar Association},
number = {UT-CS-94-230},
pages = {647},
publisher = {University of Tennesse},
title = {{MPI : A Message-Passing Interface Standard}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:MPI+:+A+Message-Passing+Interface+Standard\#5},
volume = {8},
year = {2009}
}
@misc{SciMark,
annote = {
        From Duplicate 1 ( 
        
          SciMark 2.0
        
         - Pozo, R; Miller, B )

        
        

        

        

      },
author = {Pozo, R and Miller, B},
title = {{SciMark 2.0}},
url = {http://math.nist.gov/scimark2/},
year = {2002}
}
@manual{lang:r,
address = {Vienna, Austria},
annote = {\{ISBN\} 3-900051-07-0},
author = {{R Development Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {http://www.r-project.org},
year = {2011}
}
@article{lang:python,
abstract = {If you do muchwork on computers, eventually you find that theres some task youd like to automate. For example, you may wish to perform a search-and-replace over a large number of text files, or rename and rearrange a bunch of photo files in a complicated way. Perhaps youd like to write a small custom database, or a specialized GUI application, or a simple game. If youre a professional software developer, you may have to work with several C/C++/Java libraries but find the usual write/compile/test/re-compile cycle is too slow. Perhaps youre writing a test suite for such a library and find writing the testing code a tedious task. Or maybe youve written a program that could use an extension language, and you dont want to design and implement a whole new language for your application.},
author = {Rossum, Guido Van and Drake, Fred L},
doi = {10.1111/j.1094-348X.2008.00203\_7.x},
editor = {{Fred L Drake}, Jr},
institution = {Python Software Foundation},
journal = {History},
number = {4},
pages = {1--122},
publisher = {Python Software Foundation},
title = {{Python Tutorial}},
url = {http://docs.python.org/tutorial/},
volume = {42},
year = {2010}
}
@article{Kristensen2012arXiv,
author = {{Ruben Burgdorff Kristensen}, M and Vinter, B},
journal = {Arxiv Preprint arXiv:1201.3804v1 },
keywords = { Parallel, and Cluster Computing,Computer Science - Distributed},
month = jan,
title = {{Managing Communication Latency-Hiding at Runtime for Parallel Programming Languages and Libraries}},
year = {2012}
}
@inproceedings{lang:idl,
author = {Stern, B A},
organization = {ASCE},
title = {{Interactive Data Language}},
year = {2000}
}
@misc{apl,
file = {:home/safl/Documents/Mendeley/Unknown - Unknown - why apl.html:html},
keywords = {Unknown},
title = {why apl?},
url = {http://www.sigapl.org/whyapl.htm}
}
