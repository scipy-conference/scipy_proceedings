
Performance Study
-----------------

..
  It would also be great to see experiments that compare the performance of hand parallelized codes with the automatic parallelization based on the new concept. The experiments only compare the speedup of the new concept to the singel core performance.

..
  But the main important aspect is the lack of comparison with reference codes written in Fortran, C or C++. We have no reference to evaluate the performance. From a comparison against Fortran, C and C++, one would learn what are the main limiting factors and this could be a source of inspiration to improve cphVB. As it stands, the evaluation section is not providing enough insight to be really useful for a programmer who may be interested to switch to Python and cphVB.

.. table:: Lenovo ThinkStation D20. :label:`tab:specs`

   +------------------------------+-----------------+
   | Processor                    | Two Intel Xeon  |
   +------------------------------+-----------------+
   | Clock                        | 2.67 GHz        |	
   +------------------------------+-----------------+
   | Private L1 Data Cache        | 64 KB           |
   +------------------------------+-----------------+
   | Private L2 Data Cache        | 512 KB          |
   +------------------------------+-----------------+
   | Shared L3 Cache per Socket   | 12MB            |
   +------------------------------+-----------------+
   | Memory Bandwidth             | 25.6 GB/s       |
   +------------------------------+-----------------+
   | Memory per Node              | 96GB DDR3-1066  |
   +------------------------------+-----------------+
   | Compiler                     | GCC 4.4.5       |
   +------------------------------+-----------------+

In order to demonstrate the performance of our initial cphVB implementation and thereby the potential of the cphVB design, we will conduct some performance benchmarks using NumPy [*]_. We execute the benchmark applications on one Lenove ThinkStation D20 with two Intel Xenon processers (Table :ref:`tab:specs`). 

We execute the benchmark applications on one Lenovo ThinkStation D20 with two Intel Xeon processers (Table :ref:`tab:specs`). The experiments use up to all eight CPU-cores on the machine and for each executaion we calculate the speedup of cphVB compared to NumPy. We perform strong scaling experiments, in which the problem size is constant though all the executions. For each experiment, we find the block size that results in best performance and we calculate the result of each experiment using the average of three executions.

The benchmark consists of the following Python/NumPy applications. All are pure Python applications that make use of NumPy and none uses any external libraries.

 - **Monte Carlo Pi** Approximating Pi using Monte Carlo simulation.  The implementation is a translation and vectorization of the Monte Carlo simulation included in the benchmark suite SciMark 2.0 [Sci02]_, which is written in Java (Fig. :ref:`benchmark:MCpi`). 

 - **kNN** A naive implementation of a k Nearest Neighbor search (Fig. :ref:`benchmark:kNN`). 

 - **N-body** A Newtonian N-body simulation that uses a :math:`O(n^2)` algorithm that computes all body-body interactions. (Fig. :ref:`benchmark:nbody`).

 - **Shallow Water** A simulation that simulates a system governed by the shallow water equations. It is a translation of a MATLAB application by Burkardt [Bur10]_ (Fig. :ref:`benchmark:swater`). 

Discussion
~~~~~~~~~~

The Monte Carlo Pi simulation is an embarrassingly parallel problem because thread coordination is only relevant at the end of the program. Thus, cphVB provides good performance speedup compared to NumPy – at eight CPU-cores cphVB is more than six times faster than NumPy (:math:`44.6 sec` to :math:`7.0 sec`).

On the other hand, our naive implementation of the k Nearest Neighbor search is not an embarrassedly parallel problem. However, it has a time complexity of :math:`O(n^2)` when the number of elements and the size of the query set is :math:`n`, thus the problem should be scalable. The result of our experiment is also promising – with a performance speedup of more than five when running on eight CPU-cores (:math:`17.7 sec` to :math:`3.2 sec`). Because of better cache utilization through data blocking, the performance of cphVB is more than twice as good as NumPy when using one CPU-core. Still, when using more than four CPU-cores the memory bandwidth becomes the limiting factor.

The N-body simulation also has a time complexity of :math:`O(n^2)` but it exhibits less cache locality. At one CPU-core NumPy outperforms cphVB by quite a margin – the execution time for NumPy is :math:`5.9 sec` whereas the execution time for cphVB is :math:`7.6 sec`. This is because the N-body implementation uses matrix transpose as part of the computation loop, which NumPy controls more efficiently than cphVB. 

Finally, the Shallow Water simulation only has a time complexity of :math:`O(n)` thus it is the most memory intensive application in our benchmark. Still, cphVB manages to achieve a performance speedup of almost three compared to NumPy – the execution time goes from :math:`25.7 sec` to :math:`8.8 sec`.

.. figure:: ../../../benchmarks/MonteCarlo_104857600-10.pdf

   Runtime of the Monte Carlo Pi Approximation. The job consists of a vector with 100M elements using 10 iterations. :label:`benchmark:MCpi`

.. figure:: ../../../benchmarks/kNN_1024-1024.pdf

   Runtime of the  k Nearest Neighbor search. The job consists of 1K elements and the query set also consists of 1K elements. :label:`benchmark:kNN`

.. figure:: ../../../benchmarks/nbody_8192-2.pdf

   Runtime of the N-body simulation. The job consists of 8K bodies that simulate 10 time steps. :label:`benchmark:nbody`

.. figure:: ../../../benchmarks/swater_2048-10.pdf

   Runtime of the Shallow Water Equation. The job consists of 4M grid points that simulate 10 time steps. :label:`benchmark:swater`

.. [*] NumPy version 1.6.1.
